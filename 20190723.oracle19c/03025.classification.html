<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xml:lang="en-us" lang="en-us">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <meta name="abstract" content="Learn how to predict a categorical target through Classification - the supervised mining function.">
      <meta name="description" content="Learn how to predict a categorical target through Classification - the supervised mining function.">
      <title>Classification</title>
      <meta property="og:site_name" content="Oracle Help Center">
      <meta property="og:title" content="API Guide">
      <meta property="og:description" content="Learn how to predict a categorical target through Classification - the supervised mining function.">
      <link rel="stylesheet" href="/sp_common/book-template/ohc-book-template/css/book.css">
      <link rel="shortcut icon" href="/sp_common/book-template/ohc-common/img/favicon.ico">
      <meta name="application-name" content="API Guide">
      <meta name="generator" content="DITA Open Toolkit version 1.8.5 (Mode = doc)">
      <meta name="plugin" content="SP_docbuilder HTML plugin release 18.2.2">
      <link rel="alternate" href="data-mining-api-guide.pdf" title="PDF File" type="application/pdf">
      <meta name="robots" content="all">
      <link rel="schema.dcterms" href="http://purl.org/dc/terms/">
      <meta name="dcterms.created" content="2019-03-12T22:34:55-07:00">
      
      <meta name="dcterms.dateCopyrighted" content="2005, 2019">
      <meta name="dcterms.category" content="database">
      <meta name="dcterms.identifier" content="E97869-02">
      
      <meta name="dcterms.product" content="en/database/oracle/oracle-database/19">
      
      <link rel="prev" href="regression.html" title="Previous" type="text/html">
      <link rel="next" href="anomaly-detection.html" title="Next" type="text/html">
      <script>
        document.write('<style type="text/css">');
        document.write('body > .noscript, body > .noscript ~ * { visibility: hidden; }');
        document.write('</style>');
     </script>
      <script data-main="/sp_common/book-template/ohc-book-template/js/book-config" src="/sp_common/book-template/requirejs/require.js"></script>
      <script>
            if (window.require === undefined) {
                document.write('<script data-main="sp_common/book-template/ohc-book-template/js/book-config" src="sp_common/book-template/requirejs/require.js"><\/script>');
                document.write('<link href="sp_common/book-template/ohc-book-template/css/book.css" rel="stylesheet"/>');
            }
        </script>
      <script type="application/json" id="ssot-metadata">{"primary":{"category":{"short_name":"database","element_name":"Database","display_in_url":true},"suite":{"short_name":"oracle","element_name":"Oracle","display_in_url":true},"product_group":{"short_name":"not-applicable","element_name":"Not applicable","display_in_url":false},"product":{"short_name":"oracle-database","element_name":"Oracle Database","display_in_url":true},"release":{"short_name":"19","element_name":"Release 19","display_in_url":true}}}</script>
      
    <meta name="dcterms.title" content="Data Mining API Guide">
    <meta name="dcterms.isVersionOf" content="DMAPI">
    <meta name="dcterms.release" content="Release 19">
  </head>
   <body>
      <div class="noscript alert alert-danger text-center" role="alert">
         <a href="regression.html" class="pull-left"><span class="glyphicon glyphicon-chevron-left" aria-hidden="true"></span>Previous</a>
         <a href="anomaly-detection.html" class="pull-right">Next<span class="glyphicon glyphicon-chevron-right" aria-hidden="true"></span></a>
         <span class="fa fa-exclamation-triangle" aria-hidden="true"></span> JavaScript must be enabled to correctly display this content
        
      </div>
      <article>
         <header>
            <ol class="breadcrumb" vocab="http://schema.org/" typeof="BreadcrumbList">
               <li property="itemListElement" typeof="ListItem"><a href="index.html" property="item" typeof="WebPage"><span property="name">API Guide</span></a></li>
               <li property="itemListElement" typeof="ListItem"><a href="mining-functions.html" property="item" typeof="WebPage"><span property="name"> Mining Functions</span></a></li>
               <li class="active" property="itemListElement" typeof="ListItem"> Classification </li>
            </ol>
            <a id="GUID-3D51EC47-E686-4468-8F49-A27B5F8E8FE4" name="GUID-3D51EC47-E686-4468-8F49-A27B5F8E8FE4"></a><a id="DMCON004"></a>
            
            <h2 id="DMAPI-GUID-3D51EC47-E686-4468-8F49-A27B5F8E8FE4" class="sect2"><span class="enumeration_chapter">4 </span> Classification 
            </h2>
         </header>
         <div class="ind">
            <div>
               <p>Learn how to predict a categorical target through Classification - the supervised mining function.</p>
               <p><a id="d10452e19" class="indexterm-anchor"></a><a id="d10452e21" class="indexterm-anchor"></a><a id="d10452e25" class="indexterm-anchor"></a></p>
               <ul style="list-style-type: disc;">
                  <li>
                     <p><a href="classification.html#GUID-9F922514-0F8D-42F5-BEB1-F59A09FA1CD2">About Classification</a></p>
                  </li>
                  <li>
                     <p><a href="classification.html#GUID-539BBA6A-7AEA-4F59-B963-040220D1B8B7" title="Lift measures the degree to which the predictions of a classification model are better than randomly-generated predictions.Learn the different Lift statistics that Oracle Data Mining can compute.ROC is a metric for comparing predicted and actual target values in a classification model.The ROC curve for a model represents all the possible combinations of values in its confusion matrix.">Testing a Classification Model</a></p>
                  </li>
                  <li>
                     <p><a href="classification.html#GUID-90CBA874-4713-4257-8D0A-2B3C20CA2D29" title="Compares Cost matrix and Confusion matrix for costs and accuracy to evaluate model quality.Discusses the importance of positive and negative classes in a confusion matrix.Learn about Priors and Class Weights in a Classification model to produce a useful result.">Biasing a Classification Model</a></p>
                  </li>
                  <li>
                     <p><a href="classification.html#GUID-79357E30-8018-4EEE-851E-25E10400BB65" title="Learn different Classification algorithms used in Oracle Data Mining.">Classification Algorithms</a></p>
                  </li>
               </ul>
            </div>
            <div>
               <div class="relinfo">
                  <p><strong>Related Topics</strong></p>
                  <ul>
                     <li><a href="data-mining-basics.html#GUID-2116E665-721E-4EBA-AFE1-A30D6E8078C6" title="Understand the basic concepts of Oracle Data Mining.">Oracle Data Mining Basics</a></li>
                  </ul>
               </div>
            </div><a id="DMCON197"></a><div class="props_rev_3"><a id="GUID-9F922514-0F8D-42F5-BEB1-F59A09FA1CD2" name="GUID-9F922514-0F8D-42F5-BEB1-F59A09FA1CD2"></a><h3 id="DMAPI-GUID-9F922514-0F8D-42F5-BEB1-F59A09FA1CD2" class="sect3"><span class="enumeration_section">4.1 </span>About Classification
               </h3>
               <div>
                  <p>Classification is a data mining function that assigns items in a collection to target categories or classes. The goal of classification is to accurately predict the target class for each case in the data. For example, a classification model can be used to identify loan applicants as low, medium, or high credit risks.</p>
                  <p>A classification task begins with a data set in which the class assignments are known. For example, a classification model that predicts credit risk can be developed based on observed data for many loan applicants over a period of time. In addition to the historical credit rating, the data might track employment history, home ownership or rental, years of residence, number and type of investments, and so on. Credit rating is the target, the other attributes are the predictors, and the data for each customer constitutes a case.</p>
                  <p>Classifications are discrete and do not imply order. Continuous, floating-point values indicate a <a id="d10452e85" class="indexterm-anchor"></a>numerical, rather than a categorical, target. A predictive model with a numerical target uses a regression algorithm, not a classification algorithm.
                  </p>
                  <p>The simplest type of <a id="d10452e90" class="indexterm-anchor"></a>classification problem is binary classification. In binary classification, the target attribute has only two possible values: for example, high credit rating or low credit rating. <a id="d10452e95" class="indexterm-anchor"></a>Multiclass targets have more than two values: for example, low, medium, high, or unknown credit rating.
                  </p>
                  <p>In the model build (training) process, a classification algorithm finds relationships between the values of the predictors and the values of the target. Different classification algorithms use different techniques for finding relationships. These relationships are summarized in a model, which can then be applied to a different data set in which the class assignments are unknown. </p>
                  <p>Classification models are tested by comparing the predicted values to known target values in a set of test data. The historical data for a classification project is typically divided into two data sets: one for building the model; the other for testing the model.</p>
                  <p>Applying a classification model results in class assignments and probabilities for each case. For example, a model that classifies customers as low, medium, or high value also predicts the probability of each classification for each customer. </p>
                  <p>Classification has many applications in customer segmentation, business modeling, marketing, credit analysis, and biomedical and drug response modeling.</p>
               </div>
            </div><a id="DMCON053"></a><div class="props_rev_3"><a id="GUID-539BBA6A-7AEA-4F59-B963-040220D1B8B7" name="GUID-539BBA6A-7AEA-4F59-B963-040220D1B8B7"></a><h3 id="DMAPI-GUID-539BBA6A-7AEA-4F59-B963-040220D1B8B7" class="sect3"><span class="enumeration_section">4.2 </span>Testing a Classification Model
               </h3>
               <div>
                  <p>A classification model is tested by applying it to test data with known target values and comparing the predicted values with the known values. </p>
                  <p>The test data must be compatible with the data used to build the model and must be prepared in the same way that the build data was prepared. Typically the build data and test data come from the same historical data set. A percentage of the records is used to build the model; the remaining records are used to test the model.</p>
                  <p>Test metrics are used to assess how accurately the model predicts the known values. If the model performs well and meets the business requirements, it can then be applied to new data to predict the future.</p>
               </div><a id="DMCON204"></a><a id="DMCON034"></a><div class="props_rev_3"><a id="GUID-2FE2596F-D345-4688-8C29-22A4511B1E81" name="GUID-2FE2596F-D345-4688-8C29-22A4511B1E81"></a><h4 id="DMAPI-GUID-2FE2596F-D345-4688-8C29-22A4511B1E81" class="sect4"><span class="enumeration_section">4.2.1 </span>Confusion Matrix
                  </h4>
                  <div>
                     <p>A <a id="d10452e154" class="indexterm-anchor"></a>confusion matrix displays the number of correct and incorrect predictions made by the model compared with the actual classifications in the test data. The matrix is <span class="italic">n</span>-by-<span class="italic">n</span>, where <span class="italic">n</span> is the number of classes.
                     </p>
                     <p>The following figure shows a confusion matrix for a binary classification model. The rows present the number of actual classifications in the test data. The columns present the number of predicted classifications made by the model.</p>
                     <div class="figure" id="GUID-2FE2596F-D345-4688-8C29-22A4511B1E81__I1009125">
                        <p class="titleinfigure">Figure 4-1 Confusion Matrix for a Binary Classification Model</p><img src="img/confusion_matrix.gif" alt="Description of Figure 4-1 follows" title="Description of Figure 4-1 follows" longdesc="img_text/confusion_matrix.html"><br><a href="img_text/confusion_matrix.html">Description of "Figure 4-1 Confusion Matrix for a Binary Classification Model"</a></div>
                     <!-- class="figure" -->
                     <p>In this example, the model correctly predicted the positive class for <code class="codeph">affinity_card</code> 516 times and incorrectly predicted it 25 times. The model correctly predicted the negative class for <code class="codeph">affinity_card</code> 725 times and incorrectly predicted it 10 times. The following can be computed from this confusion matrix:
                     </p>
                     <ul style="list-style-type: disc;">
                        <li>
                           <p>The model made 1241 correct predictions (516 + 725).</p>
                        </li>
                        <li>
                           <p>The model made 35 incorrect predictions (25 + 10).</p>
                        </li>
                        <li>
                           <p>There are 1276 total scored cases (516 + 25 + 10 + 725).</p>
                        </li>
                        <li>
                           <p>The error rate is 35/1276 = 0.0274.</p>
                        </li>
                        <li>
                           <p>The overall accuracy rate is 1241/1276 = 0.9725.</p>
                        </li>
                     </ul>
                  </div>
               </div><a id="DMCON035"></a><div class="props_rev_3"><a id="GUID-C1821096-E396-4A56-8404-735946489D6E" name="GUID-C1821096-E396-4A56-8404-735946489D6E"></a><h4 id="DMAPI-GUID-C1821096-E396-4A56-8404-735946489D6E" class="sect4"><span class="enumeration_section">4.2.2 </span>Lift
                  </h4>
                  <div>
                     <p>Lift measures the degree to which the predictions of a classification model are better than randomly-generated predictions.</p>
                     <p><a id="d10452e218" class="indexterm-anchor"></a>Lift applies to binary classification only, and it requires the designation of a positive class. If the model itself does not have a binary target, you can compute lift by designating one class as positive and combining all the other classes together as one negative class.
                     </p>
                     <p>Numerous statistics can be calculated to support the notion of lift. Basically, lift can be understood as a ratio of two percentages: the percentage of correct positive classifications made by the model to the percentage of actual positive classifications in the test data. For example, if 40% of the customers in a marketing survey have responded favorably (the positive classification) to a promotional campaign in the past and the model accurately predicts 75% of them, the lift is obtained by dividing .75 by .40. The resulting lift is 1.875.</p>
                     <p>Lift is computed against quantiles that each contain the same number of cases. The data is divided into quantiles after it is scored. It is ranked by probability of the positive class from highest to lowest, so that the highest concentration of positive predictions is in the top quantiles. A typical number of quantiles is 10.</p>
                     <p>Lift is commonly used to measure the performance of response models in marketing applications. The purpose of a response model is to identify segments of the population with potentially high concentrations of positive responders to a marketing campaign. Lift reveals how much of the population must be solicited to obtain the highest percentage of potential responders. </p>
                  </div>
                  <div>
                     <div class="relinfo">
                        <p><strong>Related Topics</strong></p>
                        <ul>
                           <li><a href="classification.html#GUID-D327D68B-59C3-48D1-B644-96CE8AE6D74E" title="Discusses the importance of positive and negative classes in a confusion matrix.">Positive and Negative Classes</a></li>
                        </ul>
                     </div>
                  </div><a id="DMCON206"></a><div class="props_rev_3"><a id="GUID-8AEC30B7-156F-483A-9869-AAB815618EBA" name="GUID-8AEC30B7-156F-483A-9869-AAB815618EBA"></a><h5 id="DMAPI-GUID-8AEC30B7-156F-483A-9869-AAB815618EBA" class="sect5"><span class="enumeration_section">4.2.2.1 </span>Lift Statistics
                     </h5>
                     <div>
                        <p>Learn the different Lift statistics that Oracle Data Mining can compute.</p>
                        <p>Oracle Data Mining computes the following lift statistics:</p>
                        <ul style="list-style-type: disc;">
                           <li>
                              <p><strong class="term">Probability threshold</strong> for a quantile <span class="italic">n</span> is the minimum probability for the positive target to be included in this quantile or any preceding quantiles (quantiles <span class="italic">n</span>-1, <span class="italic">n</span>-2,..., 1). If a cost matrix is used, a cost threshold is reported instead. The cost threshold is the maximum cost for the positive target to be included in this quantile or any of the preceding quantiles.
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">Cumulative gain</strong> is the ratio of the cumulative number of positive targets to the total number of positive targets.
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">Target density</strong> of a quantile is the number of true positive instances in that quantile divided by the total number of instances in the quantile. 
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">Cumulative target density</strong> for quantile <span class="italic">n</span> is the target density computed over the first <span class="italic">n</span> quantiles.
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">Quantile lift</strong> is the ratio of the target density for the quantile to the target density over all the test data. 
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">Cumulative percentage of records</strong> for a quantile is the percentage of all cases represented by the first <span class="italic">n</span> quantiles, starting at the end that is most confidently positive, up to and including the given quantile. 
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">Cumulative number of targets</strong> for quantile <span class="italic">n</span> is the number of true positive instances in the first <span class="italic">n</span> quantiles. 
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">Cumulative number of nontargets</strong> is the number of actually negative instances in the first <span class="italic">n</span> quantiles. 
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">Cumulative lift</strong> for a quantile is the ratio of the cumulative target density to the target density over all the test data. 
                              </p>
                           </li>
                        </ul>
                     </div>
                     <div>
                        <div class="relinfo">
                           <p><strong>Related Topics</strong></p>
                           <ul>
                              <li><a href="classification.html#GUID-F28D1480-ED10-4634-A566-F4D3CD17BDD7" title="Compares Cost matrix and Confusion matrix for costs and accuracy to evaluate model quality.Discusses the importance of positive and negative classes in a confusion matrix.">Costs</a></li>
                           </ul>
                        </div>
                     </div>
                  </div>
               </div><a id="DMCON054"></a><div class="props_rev_3"><a id="GUID-31006922-A679-4E29-B6C7-26E9F72E2845" name="GUID-31006922-A679-4E29-B6C7-26E9F72E2845"></a><h4 id="DMAPI-GUID-31006922-A679-4E29-B6C7-26E9F72E2845" class="sect4"><span class="enumeration_section">4.2.3 </span>Receiver Operating Characteristic (ROC)
                  </h4>
                  <div>
                     <p>ROC is a metric for comparing predicted and actual target values in a classification model.</p>
                     <p>ROC, like Lift, applies to Binary Classification and requires the designation of a positive class. </p>
                     <p>You can use ROC to gain insight into the decision-making ability of the model. How likely is the model to accurately predict the negative or the positive class? </p>
                     <p>ROC measures the impact of changes in the <strong class="term">probability threshold</strong>. The probability threshold is the decision point used by the model for classification. The default probability threshold for binary classification is 0.5. When the probability of a prediction is 50% or more, the model predicts that class. When the probability is less than 50%, the other class is predicted. (In multiclass classification, the predicted class is the one predicted with the highest probability.)
                     </p>
                  </div>
                  <div>
                     <div class="relinfo">
                        <p><strong>Related Topics</strong></p>
                        <ul>
                           <li><a href="classification.html#GUID-D327D68B-59C3-48D1-B644-96CE8AE6D74E" title="Discusses the importance of positive and negative classes in a confusion matrix.">Positive and Negative Classes</a></li>
                        </ul>
                     </div>
                  </div><a id="DMCON207"></a><div class="props_rev_3"><a id="GUID-E96FD68E-C714-477B-B67A-8725D70E0A60" name="GUID-E96FD68E-C714-477B-B67A-8725D70E0A60"></a><h5 id="DMAPI-GUID-E96FD68E-C714-477B-B67A-8725D70E0A60" class="sect5"><span class="enumeration_section">4.2.3.1 </span>The ROC Curve
                     </h5>
                     <div>
                        <p>ROC can be plotted as a curve on an X-Y axis. The <strong class="term">false positive rate</strong> is placed on the X axis. The <strong class="term">true positive rate</strong> is placed on the Y axis.
                        </p>
                        <p>The top left corner is the optimal location on an ROC graph, indicating a high true positive rate and a low false positive rate. </p>
                     </div>
                  </div><a id="DMCON209"></a><div class="props_rev_3"><a id="GUID-01C2E054-7160-4AA9-B469-A9D538CCB25D" name="GUID-01C2E054-7160-4AA9-B469-A9D538CCB25D"></a><h5 id="DMAPI-GUID-01C2E054-7160-4AA9-B469-A9D538CCB25D" class="sect5"><span class="enumeration_section">4.2.3.2 </span>Area Under the Curve
                     </h5>
                     <div>
                        <p>The area under the ROC curve (AUC) measures the discriminating ability of a binary classification model. The larger the AUC, the higher the likelihood that an actual positive case is assigned, and a higher probability of being positive than an actual negative case. The AUC measure is especially useful for data sets with unbalanced target distribution (one target class dominates the other). </p>
                     </div>
                  </div><a id="DMCON210"></a><div class="props_rev_3"><a id="GUID-EFE904AB-0A3D-4E19-BA43-2A7F43C49D86" name="GUID-EFE904AB-0A3D-4E19-BA43-2A7F43C49D86"></a><h5 id="DMAPI-GUID-EFE904AB-0A3D-4E19-BA43-2A7F43C49D86" class="sect5"><span class="enumeration_section">4.2.3.3 </span>ROC and Model Bias
                     </h5>
                     <div>
                        <p>The ROC curve for a model represents all the possible combinations of values in its confusion matrix.</p>
                        <p>Changes in the probability threshold affect the predictions made by the model. For instance, if the threshold for predicting the positive class is changed from 0.5 to 0.6, then fewer positive predictions are made. This affects the distribution of values in the confusion matrix: the number of true and false positives and true and false negatives differ. </p>
                        <p>You can use ROC to find the probability thresholds that yield the highest overall accuracy or the highest per-class accuracy. For example, if it is important to you to accurately predict the positive class, but you don't care about prediction errors for the negative class, then you can lower the threshold for the positive class. This can bias the model in favor of the positive class.</p>
                        <p>A cost matrix is a convenient mechanism for changing the probability thresholds for model scoring. </p>
                     </div>
                     <div>
                        <div class="relinfo">
                           <p><strong>Related Topics</strong></p>
                           <ul>
                              <li><a href="classification.html#GUID-F28D1480-ED10-4634-A566-F4D3CD17BDD7" title="Compares Cost matrix and Confusion matrix for costs and accuracy to evaluate model quality.Discusses the importance of positive and negative classes in a confusion matrix.">Costs</a></li>
                           </ul>
                        </div>
                     </div>
                  </div><a id="DMCON211"></a><div class="props_rev_3"><a id="GUID-1940425D-A464-46CD-9C8D-E536E62C1EB5" name="GUID-1940425D-A464-46CD-9C8D-E536E62C1EB5"></a><h5 id="DMAPI-GUID-1940425D-A464-46CD-9C8D-E536E62C1EB5" class="sect5"><span class="enumeration_section">4.2.3.4 </span>ROC Statistics
                     </h5>
                     <div>
                        <p>Oracle Data Mining computes the following ROC statistics:</p>
                        <ul style="list-style-type: disc;">
                           <li>
                              <p><strong class="term">Probability threshold:</strong> The minimum predicted positive class probability resulting in a positive class prediction. Different threshold values result in different hit rates and different false alarm rates.
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">True negatives:</strong> Negative cases in the test data with predicted probabilities strictly less than the probability threshold (correctly predicted).
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">True positives:</strong> Positive cases in the test data with predicted probabilities greater than or equal to the probability threshold (correctly predicted).
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">False negatives:</strong> Positive cases in the test data with predicted probabilities strictly less than the probability threshold (incorrectly predicted).
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">False positives:</strong> Negative cases in the test data with predicted probabilities greater than or equal to the probability threshold (incorrectly predicted).
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">True positive fraction</strong>: Hit rate. (true positives/(true positives + false negatives)) 
                              </p>
                           </li>
                           <li>
                              <p><strong class="term">False positive fraction:</strong> False alarm rate. (false positives<span class="bold">/</span>(false positives + true negatives))
                              </p>
                           </li>
                        </ul>
                     </div>
                  </div>
               </div>
            </div><a id="DMCON050"></a><div class="props_rev_3"><a id="GUID-90CBA874-4713-4257-8D0A-2B3C20CA2D29" name="GUID-90CBA874-4713-4257-8D0A-2B3C20CA2D29"></a><h3 id="DMAPI-GUID-90CBA874-4713-4257-8D0A-2B3C20CA2D29" class="sect3"><span class="enumeration_section">4.3 </span>Biasing a Classification Model
               </h3>
               <div>
                  <p>Costs, prior probabilities, and class weights are methods for biasing <a id="d10452e548" class="indexterm-anchor"></a>classification models.
                  </p>
               </div><a id="DMCON032"></a><div class="props_rev_3"><a id="GUID-F28D1480-ED10-4634-A566-F4D3CD17BDD7" name="GUID-F28D1480-ED10-4634-A566-F4D3CD17BDD7"></a><h4 id="DMAPI-GUID-F28D1480-ED10-4634-A566-F4D3CD17BDD7" class="sect4"><span class="enumeration_section">4.3.1 </span>Costs
                  </h4>
                  <div>
                     <p>A <a id="d10452e573" class="indexterm-anchor"></a>cost matrix is a mechanism for influencing the decision making of a model. A <a id="d10452e576" class="indexterm-anchor"></a>cost matrix can cause the model to minimize costly misclassifications. It can also cause the model to maximize beneficial accurate classifications.
                     </p>
                     <p>For example, if a model classifies a customer with poor credit as low risk, this error is costly. A cost matrix can bias the model to avoid this type of error. The cost matrix can also be used to bias the model in favor of the correct classification of customers who have the worst credit history. </p>
                     <p>ROC is a useful metric for evaluating how a model behaves with different probability thresholds. You can use ROC to help you find optimal costs for a given classifier given different usage scenarios. You can use this information to create cost matrices to influence the deployment of the model.</p>
                  </div><a id="DMCON212"></a><div class="props_rev_3"><a id="GUID-E96BDCE8-7143-4F1B-8203-1C40714EA765" name="GUID-E96BDCE8-7143-4F1B-8203-1C40714EA765"></a><h5 id="DMAPI-GUID-E96BDCE8-7143-4F1B-8203-1C40714EA765" class="sect5"><span class="enumeration_section">4.3.1.1 </span>Costs Versus Accuracy
                     </h5>
                     <div>
                        <p>Compares Cost matrix and Confusion matrix for costs and accuracy to evaluate model quality.</p>
                        <p>Like a <a id="d10452e605" class="indexterm-anchor"></a>confusion matrix, a cost matrix is an <span class="italic">n</span>-by-<span class="italic">n</span> matrix, where <span class="italic">n</span> is the number of classes. Both confusion matrices and cost matrices include each possible combination of actual and predicted results based on a given set of test data. 
                        </p>
                        <p>A confusion matrix is used to measure <a id="d10452e619" class="indexterm-anchor"></a>accuracy, the ratio of correct predictions to the total number of predictions. A cost matrix is used to specify the relative importance of accuracy for different predictions. In most business applications, it is important to consider costs in addition to accuracy when evaluating model quality.
                        </p>
                     </div>
                     <div>
                        <div class="relinfo">
                           <p><strong>Related Topics</strong></p>
                           <ul>
                              <li><a href="classification.html#GUID-2FE2596F-D345-4688-8C29-22A4511B1E81">Confusion Matrix</a></li>
                           </ul>
                        </div>
                     </div>
                  </div><a id="DMCON214"></a><a id="DMCON213"></a><div class="props_rev_3"><a id="GUID-D327D68B-59C3-48D1-B644-96CE8AE6D74E" name="GUID-D327D68B-59C3-48D1-B644-96CE8AE6D74E"></a><h5 id="DMAPI-GUID-D327D68B-59C3-48D1-B644-96CE8AE6D74E" class="sect5"><span class="enumeration_section">4.3.1.2 </span>Positive and Negative Classes
                     </h5>
                     <div>
                        <p>Discusses the importance of positive and negative classes in a confusion matrix.</p>
                        <p>The positive class is the class that you care the most about. Designation of a positive class is required for computing Lift and ROC. </p>
                        <p>In the confusion matrix, in the following figure, the value <code class="codeph">1</code> is designated as the positive class. This means that the creator of the model has determined that it is more important to accurately predict customers who increase spending with an affinity card (<code class="codeph">affinity_card</code>=1) than to accurately predict non-responders (<code class="codeph">affinity_card</code>=0). If you give affinity cards to some customers who are not likely to use them, there is little loss to the company since the cost of the cards is low. However, if you overlook the customers who are likely to respond, you miss the opportunity to increase your revenue.
                        </p>
                        <div class="figure" id="GUID-D327D68B-59C3-48D1-B644-96CE8AE6D74E__BGBICGCG">
                           <p class="titleinfigure">Figure 4-2 Positive and Negative Predictions</p><img src="img/false_pos_neg.gif" alt="Description of Figure 4-2 follows" title="Description of Figure 4-2 follows" longdesc="img_text/false_pos_neg.html"><br><a href="img_text/false_pos_neg.html">Description of "Figure 4-2 Positive and Negative Predictions"</a></div>
                        <!-- class="figure" -->
                        <p>The true and false positive rates in this confusion matrix are:</p>
                        <ul style="list-style-type: disc;">
                           <li>
                              <p>False positive rate — 10/(10 + 725) =.01</p>
                           </li>
                           <li>
                              <p>True positive rate — 516/(516 + 25) =.95</p>
                           </li>
                        </ul>
                     </div>
                     <div>
                        <div class="relinfo">
                           <p><strong>Related Topics</strong></p>
                           <ul>
                              <li><a href="classification.html#GUID-C1821096-E396-4A56-8404-735946489D6E" title="Lift measures the degree to which the predictions of a classification model are better than randomly-generated predictions.">Lift</a></li>
                              <li><a href="classification.html#GUID-31006922-A679-4E29-B6C7-26E9F72E2845" title="ROC is a metric for comparing predicted and actual target values in a classification model.">Receiver Operating Characteristic (ROC)</a></li>
                           </ul>
                        </div>
                     </div>
                  </div><a id="DMCON216"></a><a id="DMCON215"></a><div class="props_rev_3"><a id="GUID-2ABAC6F7-3071-463A-B112-34C2F4D6A0CD" name="GUID-2ABAC6F7-3071-463A-B112-34C2F4D6A0CD"></a><h5 id="DMAPI-GUID-2ABAC6F7-3071-463A-B112-34C2F4D6A0CD" class="sect5"><span class="enumeration_section">4.3.1.3 </span>Assigning Costs and Benefits
                     </h5>
                     <div>
                        <p>In a cost matrix, positive numbers (costs) can be used to influence negative outcomes. Since negative costs are interpreted as benefits, negative numbers (benefits) can be used to influence positive outcomes.</p>
                        <p>Suppose you have calculated that it costs your business $1500 when you do not give an affinity card to a customer who can increase spending. Using the model with the confusion matrix shown in <a href="classification.html#GUID-D327D68B-59C3-48D1-B644-96CE8AE6D74E__BGBICGCG">Figure 4-2</a>, each false negative (misclassification of a responder) costs $1500. Misclassifying a non-responder is less expensive to your business. You estimate that each false positive (misclassification of a non-responder) only costs $300.
                        </p>
                        <p>You want to keep these costs in mind when you design a promotion campaign. You estimate that it costs $10 to include a customer in the promotion. For this reason, you associate a benefit of $10 with each true negative prediction, because you can simply eliminate those customers from your promotion. Each customer that you eliminate represents a savings of $10. In your cost matrix, you specify this benefit as -10, a negative cost.</p>
                        <p>The following figure shows how you would represent these costs and benefits in a cost matrix:</p>
                        <div class="figure" id="GUID-2ABAC6F7-3071-463A-B112-34C2F4D6A0CD__BGBDJGAA">
                           <p class="titleinfigure">Figure 4-3 Cost Matrix Representing Costs and Benefits</p><img src="img/cost_matrix.gif" alt="Description of Figure 4-3 follows" title="Description of Figure 4-3 follows" longdesc="img_text/cost_matrix.html"><br><a href="img_text/cost_matrix.html">Description of "Figure 4-3 Cost Matrix Representing Costs and Benefits"</a></div>
                        <!-- class="figure" -->
                        <p>With Oracle Data Mining you can specify costs to influence the scoring of any classification model. Decision Tree models can also use a cost matrix to influence the model build.</p>
                     </div>
                  </div>
               </div><a id="DMCON033"></a><div class="props_rev_3"><a id="GUID-590DD2C5-1BA5-40A3-9E3E-92AA2AE1D0EC" name="GUID-590DD2C5-1BA5-40A3-9E3E-92AA2AE1D0EC"></a><h4 id="DMAPI-GUID-590DD2C5-1BA5-40A3-9E3E-92AA2AE1D0EC" class="sect4"><span class="enumeration_section">4.3.2 </span>Priors and Class Weights
                  </h4>
                  <div>
                     <p>Learn about Priors and Class Weights in a Classification model to produce a useful result.</p>
                     <p>With <a id="d10452e749" class="indexterm-anchor"></a><a id="d10452e751" class="indexterm-anchor"></a>Bayesian models, you can specify <strong class="term">Prior</strong> probabilities to offset differences in distribution between the build data and the real population (scoring data). With other forms of Classification, you are able to specify <strong class="term">Class Weights</strong>, which have the same biasing effect as priors.
                     </p>
                     <p>In many problems, one target value dominates in frequency. For example, the positive responses for a telephone marketing campaign is 2% or less, and the occurrence of fraud in credit card transactions is less than 1%. A classification model built on historic data of this type cannot observe enough of the rare class to be able to distinguish the characteristics of the two classes; the result can be a model that when applied to new data predicts the frequent class for every case. While such a model can be highly accurate, it is not be very useful. This illustrates that it is not a good idea to rely solely on <a id="d10452e762" class="indexterm-anchor"></a>accuracy when judging the quality of a Classification model.
                     </p>
                     <p>To correct for unrealistic distributions in the training data, you can specify priors for the model build process. Other approaches to compensating for data distribution issues include <a id="d10452e767" class="indexterm-anchor"></a>stratified sampling and <a id="d10452e770" class="indexterm-anchor"></a>anomaly detection. 
                     </p>
                  </div>
                  <div>
                     <div class="relinfo">
                        <p><strong>Related Topics</strong></p>
                        <ul>
                           <li><a href="anomaly-detection.html#GUID-D9D23B6C-5215-4E37-80E3-F2460D82A533" title="Learn how to detect rare cases in the data through Anomaly Detection - an unsupervised function.">Anomaly Detection</a></li>
                        </ul>
                     </div>
                  </div>
               </div>
            </div><a id="DMCON219"></a><div class="props_rev_3"><a id="GUID-79357E30-8018-4EEE-851E-25E10400BB65" name="GUID-79357E30-8018-4EEE-851E-25E10400BB65"></a><h3 id="DMAPI-GUID-79357E30-8018-4EEE-851E-25E10400BB65" class="sect3"><span class="enumeration_section">4.4 </span>Classification Algorithms
               </h3>
               <div>
                  <p>Learn different Classification algorithms used in Oracle Data Mining.</p>
                  <p>Oracle <a id="d10452e813" class="indexterm-anchor"></a>Data Mining provides the following algorithms for classification: 
                  </p>
                  <ul style="list-style-type: disc;">
                     <li>
                        <p><strong class="term">Decision Tree</strong></p>
                        <p>Decision trees automatically generate rules, which are conditional statements that reveal the logic used to build the tree.</p>
                     </li>
                     <li>
                        <p><span>Explicit Semantic Analysis</span></p>
                        <p>Explicit Semantic Analysis (ESA) is designed to make predictions for text data. This algorithm can address use cases with hundreds of thousands of classes.</p>
                     </li>
                     <li>
                        <p><strong class="term">Naive Bayes</strong></p>
                        <p><a id="d10452e836" class="indexterm-anchor"></a>Naive Bayes uses Bayes' Theorem, a formula that calculates a probability by counting the frequency of values and combinations of values in the historical data.
                        </p>
                     </li>
                     <li>
                        <p><strong class="term">Generalized Linear Models (GLM)</strong></p>
                        <p>GLM is a popular statistical technique for linear modeling. Oracle Data Mining implements GLM for binary classification and for regression. GLM provides extensive coefficient statistics and model statistics, as well as row diagnostics. GLM also supports confidence bounds.</p>
                     </li>
                     <li>
                        <p><strong class="term">Random Forest</strong></p>
                        <p>Random Forest is a powerful and popular machine learning algorithm that brings significant performance and scalability benefits.</p>
                     </li>
                     <li>
                        <p><strong class="term">Support Vector Machines (SVM)</strong></p>
                        <p>SVM is a powerful, state-of-the-art algorithm based on linear and nonlinear regression. Oracle Data Mining implements SVM for binary and multiclass classification.</p>
                     </li>
                  </ul>
                  <div class="infoboxnote" id="GUID-79357E30-8018-4EEE-851E-25E10400BB65__GUID-FE32656D-34B2-4AB6-86FA-A424D3042A4B">
                     <p class="notep1">Note:</p>
                     <p>Oracle Data Mining uses Naive Bayes as the default <a id="d10452e862" class="indexterm-anchor"></a>classification algorithm.
                     </p>
                  </div>
               </div>
               <div>
                  <div class="relinfo">
                     <p><strong>Related Topics</strong></p>
                     <ul>
                        <li><a href="decision-tree.html#GUID-14DE1A88-220F-44F0-9AC8-77CA844D4A63" title="Learn how to use Decision Tree algorithm. Decision Tree is one of the Classification algorithms that the Oracle Data Mining supports.">Decision Tree</a></li>
                        <li><a href="explicit-semantic-analysis.html#GUID-7DC30272-E234-4C7C-B7D2-29D0E5448BA6" title="Learn how to use Explicit Semantic Analysis (ESA) as an unsupervised algorithm for Feature Extraction function and as a supervised algorithm for Classification.">Explicit Semantic Analysis</a></li>
                        <li><a href="naive-bayes.html#GUID-BB77D68D-3E07-4522-ACB6-FD6723BDA92A" title="Learn how to use Naive Bayes Classification algorithm that the Oracle Data Mining supports.">Naive Bayes</a></li>
                        <li><a href="generalized-linear-models.html#GUID-5E59530F-EBD9-414E-8C8B-63F8079772CE" title="Learn how to use Generalized Linear Models (GLM) statistical technique for Linear modeling.">Generalized Linear Models</a></li>
                        <li><a href="random-forest.html#GUID-B6506C33-8555-4181-993F-CD7D48B4DA3C" title="Learn how to use Random Forest as a classification algorithm.">Random Forest</a></li>
                        <li><a href="support-vector-machines.html#GUID-FD5DF1FB-AAAA-4D4E-84A2-8F645F87C344" title="Learn how to use Support Vector Machines, a powerful algorithm based on statistical learning theory.">Support Vector Machines</a></li>
                     </ul>
                  </div>
               </div>
            </div>
         </div>
      </article>
   </body>
</html>
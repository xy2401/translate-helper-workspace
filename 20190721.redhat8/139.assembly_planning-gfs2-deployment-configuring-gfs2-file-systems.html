<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 1. Planning a GFS2 file system deployment</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_planning-gfs2-deployment-configuring-gfs2-file-systems"/>Chapter 1. Planning a GFS2 file system deployment</h1></div></div></div><p>
			The Red Hat GFS2 file system is a 64-bit symmetric cluster file system which provides a shared namespace and manages coherency between multiple nodes sharing a common block device. A GFS2 file system is intended to provide a feature set which is as close as possible to a local file system, while at the same time enforcing full cluster coherency between nodes. In a few cases, the Linux file system API does not allow the clustered nature of GFS2 to be totally transparent; for example, programs using Posix locks in GFS2 should avoid using the <code class="literal">GETLK</code> function since, in a clustered environment, the process ID may be for a different node in the cluster. In most cases however, the functionality of a GFS2 file system is identical to that of a local file system.
		</p><p>
			The Red Hat Enterprise Linux (RHEL) Resilient Storage Add-On provides GFS2, and it depends on the RHEL High Availability Add-On to provide the cluster management required by GFS2.
		</p><p>
			The <code class="literal">gfs2.ko</code> kernel module implements the GFS2 file system and is loaded on GFS2 cluster nodes.
		</p><p>
			To get the best performance from GFS2, it is important to take into account the performance considerations which stem from the underlying design. Just like a local file system, GFS2 relies on the page cache in order to improve performance by local caching of frequently used data. In order to maintain coherency across the nodes in the cluster, cache control is provided by the <span class="emphasis"><em>glock</em></span> state machine.
		</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>
				Make sure that your deployment of the Red Hat High Availability Add-On meets your needs and can be supported. Consult with an authorized Red Hat representative to verify your configuration prior to deployment.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_basic-gfs2-parameters-planning-gfs2-deployment"/>Key GFS2 parameters to determine</h1></div></div></div><p>
				Before you install and set up GFS2, note the following key characteristics of your GFS2 file systems:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">GFS2 nodes</span></dt><dd>
							Determine which nodes in the cluster will mount the GFS2 file systems.
						</dd><dt><span class="term">Number of file systems</span></dt><dd>
							Determine how many GFS2 file systems to create initially. (More file systems can be added later.)
						</dd><dt><span class="term">File system name</span></dt><dd>
							Determine a unique name for each file system. The name must be unique for all <code class="literal">lock_dlm</code> file systems over the cluster. Each file system name is required in the form of a parameter variable. For example, this book uses file system names <code class="literal">mydata1</code> and <code class="literal">mydata2</code> in some example procedures.
						</dd><dt><span class="term">Journals</span></dt><dd>
							Determine the number of journals for your GFS2 file systems. GFS2 requires one journal for each node in the cluster that needs to mount the file system. For example, if you have a 16-node cluster but need to mount only the file system from two nodes, you need only two journals. GFS2 allows you to add journals dynamically at a later point with the <code class="literal">gfs2_jadd</code> command as additional servers mount a file system.
						</dd><dt><span class="term">Storage devices and partitions</span></dt><dd>
							Determine the storage devices and partitions to be used for creating logical volumes (using CLVM) in the file systems.
						</dd><dt><span class="term">Time protocol</span></dt><dd><p class="simpara">
							Make sure that the clocks on the GFS2 nodes are synchronized. It is recommended that you use the Precision Time Protocol (PTP) or, if necessary for your configuration, the Network Time Protocol (NTP) software provided with your Red Hat Enterprise Linux distribution.
						</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
								The system clocks in GFS2 nodes must be within a few minutes of each other to prevent unnecessary inode time stamp updating. Unnecessary inode time stamp updating severely impacts cluster performance.
							</p></div></dd></dl></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
					You may see performance problems with GFS2 when many create and delete operations are issued from more than one node in the same directory at the same time. If this causes performance problems in your system, you should localize file creation and deletions by a node to directories specific to that node as much as possible.
				</p></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_gfs2-support-limits-planning-gfs2-deployment"/>GFS2 support considerations</h1></div></div></div><p>
				<a class="xref" href="assembly_planning-gfs2-deployment-configuring-gfs2-file-systems.html#tb-table-gfs2-max" title="Table 1.1. GFS2 Support Limits">Table 1.1, “GFS2 Support Limits”</a> summarizes the current maximum file system size and number of nodes that GFS2 supports.
			</p><div class="table"><a id="tb-table-gfs2-max"/><p class="title"><strong>Table 1.1. GFS2 Support Limits</strong></p><div class="table-contents"><table summary="GFS2 Support Limits" border="1"><colgroup><col class="col_1"/><col class="col_2"/></colgroup><thead><tr><th style="text-align: left" valign="top">Maximum number of node</th><th style="text-align: left" valign="top">16 (x86, Power8 on PowerVM) 4 (s390x under z/VM)</th></tr></thead><tbody><tr><td style="text-align: left" valign="top"> <p>
								Maximum file system size
							</p>
							 </td><td style="text-align: left" valign="top"> <p>
								100G on all supported architectures
							</p>
							 </td></tr></tbody></table></div></div><p>
				GFS2 is based on a 64-bit architecture, which can theoretically accommodate an 8 EB file system. If your system requires larger GFS2 file systems than are currently supported, contact your Red Hat service representative.
			</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
					Although a GFS2 file system can be implemented in a standalone system or as part of a cluster configuration, Red Hat does not support the use of GFS2 as a single-node file system. Red Hat does support a number of high-performance single node file systems which are optimized for single node and thus have generally lower overhead than a cluster file system. Red Hat recommends using these file systems in preference to GFS2 in cases where only a single node needs to mount the file system.
				</p><p>
					Red Hat will continue to support single-node GFS2 file systems for mounting snapshots of cluster file systems (for example, for backup purposes).
				</p></div><p>
				When determining the size of your file system, you should consider your recovery needs. Running the <code class="literal">fsck.gfs2</code> command on a very large file system can take a long time and consume a large amount of memory. Additionally, in the event of a disk or disk subsystem failure, recovery time is limited by the speed of your backup media. For information on the amount of memory the <code class="literal">fsck.gfs2</code> command requires, see <a class="link" href="assembly_gfs2-filesystem-repair-creating-mounting-gfs2.html#proc_determining-needed-memory-for-fsckgfs2-gfs2-filesystem-repair" title="Determing required memory for running fsck.gfs2">Determing required memory for running fsck.gfs2</a>.
			</p><p>
				While a GFS2 file system may be used outside of LVM, Red Hat supports only GFS2 file systems that are created on a shared LVM logical volume.
			</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
					When you configure a GFS2 file system as a cluster file system, you must ensure that all nodes in the cluster have access to the shared storage. Asymmetric cluster configurations in which some nodes have access to the shared storage and others do not are not supported. This does not require that all nodes actually mount the GFS2 file system itself.
				</p></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_gfs2-formattiing-considerations-planning-gfs2-deployment"/>GFS2 formatting considerations</h1></div></div></div><p>
				The Global File System 2 (GFS2) file system allows several computers (“nodes”) in a cluster to cooperatively share the same storage. To achieve this cooperation and maintain data consistency among the nodes, the nodes employ a cluster-wide locking scheme for file system resources. This locking scheme uses communication protocols such as TCP/IP to exchange locking information.
			</p><p>
				This section provides recommendations for how to format your GFS2 file system to optimize performance.
			</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>
					Make sure that your deployment of the Red Hat High Availability Add-On meets your needs and can be supported. Consult with an authorized Red Hat representative to verify your configuration prior to deployment.
				</p></div><h3><a id="file_system_size_smaller_is_better"/>File System Size: Smaller Is Better</h3><p>
				GFS2 is based on a 64-bit architecture, which can theoretically accommodate an 8 EB file system. However, the current supported maximum size of a GFS2 file system for 64-bit hardware is 100TB and the current supported maximum size of a GFS2 file system for 32-bit hardware is 16TB.
			</p><p>
				Note that even though GFS2 large file systems are possible, that does not mean they are recommended. The rule of thumb with GFS2 is that smaller is better: it is better to have 10 1TB file systems than one 10TB file system.
			</p><p>
				There are several reasons why you should keep your GFS2 file systems small:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						Less time is required to back up each file system.
					</li><li class="listitem">
						Less time is required if you need to check the file system with the <code class="literal">fsck.gfs2</code> command.
					</li><li class="listitem">
						Less memory is required if you need to check the file system with the <code class="literal">fsck.gfs2</code> command.
					</li></ul></div><p>
				In addition, fewer resource groups to maintain mean better performance.
			</p><p>
				Of course, if you make your GFS2 file system too small, you might run out of space, and that has its own consequences. You should consider your own use cases before deciding on a size.
			</p><h3><a id="block_size_default_4k_blocks_are_preferred"/>Block Size: Default (4K) Blocks Are Preferred</h3><p>
				The <code class="literal">mkfs.gfs2</code> command attempts to estimate an optimal block size based on device topology. In general, 4K blocks are the preferred block size because 4K is the default page size (memory) for Linux. Unlike some other file systems, GFS2 does most of its operations using 4K kernel buffers. If your block size is 4K, the kernel has to do less work to manipulate the buffers.
			</p><p>
				It is recommended that you use the default block size, which should yield the highest performance. You may need to use a different block size only if you require efficient storage of many very small files.
			</p><h3><a id="journal_size_default_128mb_is_usually_optimal"/>Journal Size: Default (128MB) Is Usually Optimal</h3><p>
				When you run the <code class="literal">mkfs.gfs2</code> command to create a GFS2 file system, you may specify the size of the journals. If you do not specify a size, it will default to 128MB, which should be optimal for most applications.
			</p><p>
				Some system administrators might think that 128MB is excessive and be tempted to reduce the size of the journal to the minimum of 8MB or a more conservative 32MB. While that might work, it can severely impact performance. Like many journaling file systems, every time GFS2 writes metadata, the metadata is committed to the journal before it is put into place. This ensures that if the system crashes or loses power, you will recover all of the metadata when the journal is automatically replayed at mount time. However, it does not take much file system activity to fill an 8MB journal, and when the journal is full, performance slows because GFS2 has to wait for writes to the storage.
			</p><p>
				It is generally recommended to use the default journal size of 128MB. If your file system is very small (for example, 5GB), having a 128MB journal might be impractical. If you have a larger file system and can afford the space, using 256MB journals might improve performance.
			</p><h3><a id="size_and_number_of_resource_groups"/>Size and Number of Resource Groups</h3><p>
				When a GFS2 file system is created with the <code class="literal">mkfs.gfs2</code> command, it divides the storage into uniform slices known as resource groups. It attempts to estimate an optimal resource group size (ranging from 32MB to 2GB). You can override the default with the <code class="literal">-r</code> option of the <code class="literal">mkfs.gfs2</code> command.
			</p><p>
				Your optimal resource group size depends on how you will use the file system. Consider how full it will be and whether or not it will be severely fragmented.
			</p><p>
				You should experiment with different resource group sizes to see which results in optimal performance. It is a best practice to experiment with a test cluster before deploying GFS2 into full production.
			</p><p>
				If your file system has too many resource groups (each of which is too small), block allocations can waste too much time searching tens of thousands (or hundreds of thousands) of resource groups for a free block. The more full your file system, the more resource groups that will be searched, and every one of them requires a cluster-wide lock. This leads to slow performance.
			</p><p>
				If, however, your file system has too few resource groups (each of which is too big), block allocations might contend more often for the same resource group lock, which also impacts performance. For example, if you have a 10GB file system that is carved up into five resource groups of 2GB, the nodes in your cluster will fight over those five resource groups more often than if the same file system were carved into 320 resource groups of 32MB. The problem is exacerbated if your file system is nearly full because every block allocation might have to look through several resource groups before it finds one with a free block. GFS2 tries to mitigate this problem in two ways:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						First, when a resource group is completely full, it remembers that and tries to avoid checking it for future allocations (until a block is freed from it). If you never delete files, contention will be less severe. However, if your application is constantly deleting blocks and allocating new blocks on a file system that is mostly full, contention will be very high and this will severely impact performance.
					</li><li class="listitem">
						Second, when new blocks are added to an existing file (for example, appending) GFS2 will attempt to group the new blocks together in the same resource group as the file. This is done to increase performance: on a spinning disk, seeks take less time when they are physically close together.
					</li></ul></div><p>
				The worst case scenario is when there is a central directory in which all the nodes create files because all of the nodes will constantly fight to lock the same resource group.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_gfs2-block-allocation-issues-planning-gfs2-deployment"/>Block allocation issues</h1></div></div></div><p>
				This section provides a summary of issues related to block allocation in GFS2 file systems. Even though applications that only write data typically do not care how or where a block is allocated, some knowledge of how block allocation works can help you optimize performance.
			</p><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="leave_free_space_in_the_file_system"/>Leave free space in the file system</h2></div></div></div><p>
					When a GFS2 file system is nearly full, the block allocator starts to have a difficult time finding space for new blocks to be allocated. As a result, blocks given out by the allocator tend to be squeezed into the end of a resource group or in tiny slices where file fragmentation is much more likely. This file fragmentation can cause performance problems. In addition, when a GFS2 file system is nearly full, the GFS2 block allocator spends more time searching through multiple resource groups, and that adds lock contention that would not necessarily be there on a file system that has ample free space. This also can cause performance problems.
				</p><p>
					For these reasons, it is recommended that you not run a file system that is more than 85 percent full, although this figure may vary depending on workload.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="have_each_node_allocate_its_own_files_if_possible"/>Have each node allocate its own files, if possible</h2></div></div></div><p>
					Due to the way the distributed lock manager (DLM) works, there will be more lock contention if all files are allocated by one node and other nodes need to add blocks to those files.
				</p><p>
					In GFS (version 1), all locks were managed by a central lock manager whose job was to control locking throughout the cluster. This grand unified lock manager (GULM) was problematic because it was a single point of failure. GFS2’s replacement locking scheme, DLM, spreads the locks throughout the cluster. If any node in the cluster goes down, its locks are recovered by the other nodes.
				</p><p>
					With DLM, the first node to lock a resource (like a file) becomes the “lock master” for that lock. Other nodes may lock that resource, but they have to ask permission from the lock master first. Each node knows which locks for which it is the lock master, and each node knows which node it has lent a lock to. Locking a lock on the master node is much faster than locking one on another node that has to stop and ask permission from the lock’s master.
				</p><p>
					As in many file systems, the GFS2 allocator tries to keep blocks in the same file close to one another to reduce the movement of disk heads and boost performance. A node that allocates blocks to a file will likely need to use and lock the same resource groups for the new blocks (unless all the blocks in that resource group are in use). The file system will run faster if the lock master for the resource group containing the file allocates its data blocks (it is faster to have the node that first opened the file do all the writing of new blocks).
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="preallocate_if_possible"/>Preallocate, if possible</h2></div></div></div><p>
					If files are preallocated, block allocations can be avoided altogether and the file system can run more efficiently. GFS2 includes the <code class="literal">fallocate</code>(1) system call, which you can use to preallocate blocks of data.
				</p></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_gfs2-cluster-considerations-planning-gfs2-deployment"/>Cluster considerations</h1></div></div></div><p>
				When determining the number of nodes that your system will contain, note that there is a trade-off between high availability and performance. With a larger number of nodes, it becomes increasingly difficult to make workloads scale. For that reason, Red Hat does not support using GFS2 for cluster file system deployments greater than 16 nodes.
			</p><p>
				Deploying a cluster file system is not a "drop in" replacement for a single node deployment. Red Hat recommends that you allow a period of around 8-12 weeks of testing on new installations in order to test the system and ensure that it is working at the required performance level. During this period any performance or functional issues can be worked out and any queries should be directed to the Red Hat support team.
			</p><p>
				Red Hat recommends that customers considering deploying clusters have their configurations reviewed by Red Hat support before deployment to avoid any possible support issues later on.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_basic-gfs2-hardware-considerations-planning-gfs2-deployment"/>Hardware considerations</h1></div></div></div><p>
				You should take the following hardware considerations into account when deploying a GFS2 file system.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p class="simpara">
						Use higher quality storage options
					</p><p class="simpara">
						GFS2 can operate on cheaper shared storage options, such as iSCSI or Fibre Channel over Ethernet (FCoE), but you will get better performance if you buy higher quality storage with larger caching capacity. Red Hat performs most quality, sanity, and performance tests on SAN storage with Fibre Channel interconnect. As a general rule, it is always better to deploy something that has been tested first.
					</p></li><li class="listitem"><p class="simpara">
						Test network equipment before deploying
					</p><p class="simpara">
						Higher quality, faster network equipment makes cluster communications and GFS2 run faster with better reliability. However, you do not have to purchase the most expensive hardware. Some of the most expensive network switches have problems passing multicast packets, which are used for passing <code class="literal">fcntl</code> locks (flocks), whereas cheaper commodity network switches are sometimes faster and more reliable. Red Hat recommends trying equipment before deploying it into full production.
					</p></li></ul></div></div></div></body></html>
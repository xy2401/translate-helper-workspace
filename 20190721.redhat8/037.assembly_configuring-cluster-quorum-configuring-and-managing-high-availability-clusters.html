<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 22. Cluster quorum</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters"/>Chapter 22. Cluster quorum</h1></div></div></div><p>
			A Red Hat Enterprise Linux High Availability Add-On cluster uses the <code class="literal">votequorum</code> service, in conjunction with fencing, to avoid split brain situations. A number of votes is assigned to each system in the cluster, and cluster operations are allowed to proceed only when a majority of votes is present. The service must be loaded into all nodes or none; if it is loaded into a subset of cluster nodes, the results will be unpredictable. For information on the configuration and operation of the <code class="literal">votequorum</code> service, see the <code class="literal">votequorum</code>(5) man page.
		</p><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="ref_quorum-options-configuring-cluster-quorum"/>Configuring quorum options</h1></div></div></div><p>
				There are some special features of quorum configuration that you can set when you create a cluster with the <code class="literal">pcs cluster setup</code> command. <a class="xref" href="assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters.html#tb-quorumoptions-HAAR" title="Table 22.1. Quorum Options">Table 22.1, “Quorum Options”</a> summarizes these options.
			</p><div class="table"><a id="tb-quorumoptions-HAAR"/><p class="title"><strong>Table 22.1. Quorum Options</strong></p><div class="table-contents"><table summary="Quorum Options" border="1"><colgroup><col class="col_1"/><col class="col_2"/></colgroup><thead><tr><th style="text-align: left" valign="top">Option</th><th style="text-align: left" valign="top">Description</th></tr></thead><tbody><tr><td style="text-align: left" valign="top"> <p>
								<code class="literal">auto_tie_breaker</code>
							</p>
							 </td><td style="text-align: left" valign="top"> <p>
								When enabled, the cluster can suffer up to 50% of the nodes failing at the same time, in a deterministic fashion. The cluster partition, or the set of nodes that are still in contact with the <code class="literal">nodeid</code> configured in <code class="literal">auto_tie_breaker_node</code> (or lowest <code class="literal">nodeid</code> if not set), will remain quorate. The other nodes will be inquorate.
							</p>
							 <p>
								The <code class="literal">auto_tie_breaker</code> option is principally used for clusters with an even number of nodes, as it allows the cluster to continue operation with an even split. For more complex failures, such as multiple, uneven splits, it is recommended that you use a quorum device, as described in <a class="link" href="assembly_configuring-cluster-quorum-configuring-and-managing-high-availability-clusters.html#assembly_configuring-quorum-devices-configuring-cluster-quorum" title="Quorum devices">Quorum devices</a>.
							</p>
							 <p>
								The <code class="literal">auto_tie_breaker</code> option is incompatible with quorum devices.
							</p>
							 </td></tr><tr><td style="text-align: left" valign="top"> <p>
								<code class="literal">wait_for_all</code>
							</p>
							 </td><td style="text-align: left" valign="top"> <p>
								When enabled, the cluster will be quorate for the first time only after all nodes have been visible at least once at the same time.
							</p>
							 <p>
								The <code class="literal">wait_for_all</code> option is primarily used for two-node clusters and for even-node clusters using the quorum device <code class="literal">lms</code> (last man standing) algorithm.
							</p>
							 <p>
								The <code class="literal">wait_for_all</code> option is automatically enabled when a cluster has two nodes, does not use a quorum device, and <code class="literal">auto_tie_breaker</code> is disabled. You can override this by explicitly setting <code class="literal">wait_for_all</code> to 0.
							</p>
							 </td></tr><tr><td style="text-align: left" valign="top"> <p>
								<code class="literal">last_man_standing</code>
							</p>
							 </td><td style="text-align: left" valign="top"> <p>
								When enabled, the cluster can dynamically recalculate <code class="literal">expected_votes</code> and quorum under specific circumstances. You must enable <code class="literal">wait_for_all</code> when you enable this option. The <code class="literal">last_man_standing</code> option is incompatible with quorum devices.
							</p>
							 </td></tr><tr><td style="text-align: left" valign="top"> <p>
								<code class="literal">last_man_standing_window</code>
							</p>
							 </td><td style="text-align: left" valign="top"> <p>
								The time, in milliseconds, to wait before recalculating <code class="literal">expected_votes</code> and quorum after a cluster loses nodes.
							</p>
							 </td></tr></tbody></table></div></div><p>
				For further information about configuring and using these options, see the <code class="literal">votequorum</code>(5) man page.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_modifying-quorum-options-configuring-cluster-quorum"/>Modifying quorum options</h1></div></div></div><p>
				You can modify general quorum options for your cluster with the <code class="literal">pcs quorum update</code> command. Executing this command requires that the cluster be stopped. For information on the quorum options, see the <code class="literal">votequorum</code>(5) man page.
			</p><p>
				The format of the <code class="literal">pcs quorum update</code> command is as follows.
			</p><pre class="literallayout">pcs quorum update [auto_tie_breaker=[0|1]] [last_man_standing=[0|1]] [last_man_standing_window=[<span class="emphasis"><em>time-in-ms</em></span>] [wait_for_all=[0|1]]</pre><p>
				The following series of commands modifies the <code class="literal">wait_for_all</code> quorum option and displays the updated status of the option. Note that the system does not allow you to execute this command while the cluster is running.
			</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum update wait_for_all=1</code>
Checking corosync is not running on nodes...
Error: node1: corosync is running
Error: node2: corosync is running

[root@node1:~]# <code class="literal">pcs cluster stop --all</code>
node2: Stopping Cluster (pacemaker)...
node1: Stopping Cluster (pacemaker)...
node1: Stopping Cluster (corosync)...
node2: Stopping Cluster (corosync)...

[root@node1:~]# <code class="literal">pcs quorum update wait_for_all=1</code>
Checking corosync is not running on nodes...
node2: corosync is not running
node1: corosync is not running
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded

[root@node1:~]# <code class="literal">pcs quorum config</code>
Options:
  wait_for_all: 1</pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_displaying-quorum-configuration-status-configuring-cluster-quorum"/>Displaying quorum configuration and status</h1></div></div></div><p>
				Once a cluster is running, you can enter the following cluster quorum commands.
			</p><p>
				The following command shows the quorum configuration.
			</p><pre class="literallayout">pcs quorum [config]</pre><p>
				The following command shows the quorum runtime status.
			</p><pre class="literallayout">pcs quorum status</pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_running-inquorate-clusters-configuring-cluster-quorum"/>Running inquorate clusters</h1></div></div></div><p>
				If you take nodes out of a cluster for a long period of time and the loss of those nodes would cause quorum loss, you can change the value of the <code class="literal">expected_votes</code> parameter for the live cluster with the <code class="literal">pcs quorum expected-votes</code> command. This allows the cluster to continue operation when it does not have quorum.
			</p><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>
					Changing the expected votes in a live cluster should be done with extreme caution. If less than 50% of the cluster is running because you have manually changed the expected votes, then the other nodes in the cluster could be started separately and run cluster services, causing data corruption and other unexpected results. If you change this value, you should ensure that the <code class="literal">wait_for_all</code> parameter is enabled.
				</p></div><p>
				The following command sets the expected votes in the live cluster to the specified value. This affects the live cluster only and does not change the configuration file; the value of <code class="literal">expected_votes</code> is reset to the value in the configuration file in the event of a reload.
			</p><pre class="literallayout">pcs quorum expected-votes <span class="emphasis"><em>votes</em></span></pre><p>
				In a situation in which you know that the cluster is inquorate but you want the cluster to proceed with resource management, you can use the <code class="literal">pcs quorum unblock</code> command to prevent the cluster from waiting for all nodes when establishing quorum.
			</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
					This command should be used with extreme caution. Before issuing this command, it is imperative that you ensure that nodes that are not currently in the cluster are switched off and have no access to shared resources.
				</p></div><pre class="literallayout"># <code class="literal">pcs quorum unblock</code></pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_configuring-quorum-devices-configuring-cluster-quorum"/>Quorum devices</h1></div></div></div><p>
				You can allow a cluster to sustain more node failures than standard quorum rules allows by configuring a separate quorum device which acts as a third-party arbitration device for the cluster. A quorum device is recommended for clusters with an even number of nodes and highly recommended for two-node clusters.
			</p><p>
				You must take the following into account when configuring a quorum device.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						It is recommended that a quorum device be run on a different physical network at the same site as the cluster that uses the quorum device. Ideally, the quorum device host should be in a separate rack than the main cluster, or at least on a separate PSU and not on the same network segment as the corosync ring or rings.
					</li><li class="listitem">
						You cannot use more than one quorum device in a cluster at the same time.
					</li><li class="listitem">
						Although you cannot use more than one quorum device in a cluster at the same time, a single quorum device may be used by several clusters at the same time. Each cluster using that quorum device can use different algorithms and quorum options, as those are stored on the cluster nodes themselves. For example, a single quorum device can be used by one cluster with an <code class="literal">ffsplit</code> (fifty/fifty split) algorithm and by a second cluster with an <code class="literal">lms</code> (last man standing) algorithm.
					</li><li class="listitem">
						A quorum device should not be run on an existing cluster node.
					</li></ul></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="proc_installing-quorum-device-packages-configuring-quorum-devices"/>Installing quorum device packages</h2></div></div></div><p>
					Configuring a quorum device for a cluster requires that you install the following packages:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p class="simpara">
							Install <code class="literal">corosync-qdevice</code> on the nodes of an existing cluster.
						</p><pre class="literallayout">[root@node1:~]# <code class="literal">yum install corosync-qdevice</code>
[root@node2:~]# <code class="literal">yum install corosync-qdevice</code></pre></li><li class="listitem"><p class="simpara">
							Install <code class="literal">pcs</code> and <code class="literal">corosync-qnetd</code> on the quorum device host.
						</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">yum install pcs corosync-qnetd</code></pre></li><li class="listitem"><p class="simpara">
							Start the <code class="literal">pcsd</code> service and enable <code class="literal">pcsd</code> at system start on the quorum device host.
						</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">systemctl start pcsd.service</code>
[root@qdevice:~]# <code class="literal">systemctl enable pcsd.service</code></pre></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="proc_configuring-quorum-device-configuring-quorum-devices"/>Configuring a quorum device</h2></div></div></div><p>
					The following procedure configures a quorum device and adds it to the cluster. In this example:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							The node used for a quorum device is <code class="literal">qdevice</code>.
						</li><li class="listitem"><p class="simpara">
							The quorum device model is <code class="literal">net</code>, which is currently the only supported model. The <code class="literal">net</code> model supports the following algorithms:
						</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
									<code class="literal">ffsplit</code>: fifty-fifty split. This provides exactly one vote to the partition with the highest number of active nodes.
								</li><li class="listitem"><p class="simpara">
									<code class="literal">lms</code>: last-man-standing. If the node is the only one left in the cluster that can see the <code class="literal">qnetd</code> server, then it returns a vote.
								</p><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>
										The LMS algorithm allows the cluster to remain quorate even with only one remaining node, but it also means that the voting power of the quorum device is great since it is the same as number_of_nodes - 1. Losing connection with the quorum device means losing number_of_nodes - 1 votes, which means that only a cluster with all nodes active can remain quorate (by overvoting the quorum device); any other cluster becomes inquorate.
									</p></div><p class="simpara">
									For more detailed information on the implementation of these algorithms, see the <code class="literal">corosync-qdevice</code>(8) man page.
								</p></li></ul></div></li><li class="listitem">
							The cluster nodes are <code class="literal">node1</code> and <code class="literal">node2</code>.
						</li></ul></div><p>
					The following procedure configures a quorum device and adds that quorum device to a cluster.
				</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p class="simpara">
							On the node that you will use to host your quorum device, configure the quorum device with the following command. This command configures and starts the quorum device model <code class="literal">net</code> and configures the device to start on boot.
						</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">pcs qdevice setup model net --enable --start</code>
Quorum device 'net' initialized
quorum device enabled
Starting quorum device...
quorum device started</pre><p class="simpara">
							After configuring the quorum device, you can check its status. This should show that the <code class="literal">corosync-qnetd</code> daemon is running and, at this point, there are no clients connected to it. The <code class="literal">--full</code> command option provides detailed output.
						</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">pcs qdevice status net --full</code>
QNetd address:                  *:5403
TLS:                            Supported (client certificate required)
Connected clients:              0
Connected clusters:             0
Maximum send/receive size:      32768/32768 bytes</pre></li><li class="listitem"><p class="simpara">
							Enable the ports on the firewall needed by the <code class="literal">pcsd</code> daemon and the <code class="literal">net</code> quorum device by enabling the <code class="literal">high-availability</code> service on <code class="literal">firewalld</code> with following commands.
						</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">firewall-cmd --permanent --add-service=high-availability</code>
[root@qdevice:~]# <code class="literal">firewall-cmd --add-service=high-availability</code></pre></li><li class="listitem"><p class="simpara">
							From one of the nodes in the existing cluster, authenticate user <code class="literal">hacluster</code> on the node that is hosting the quorum device. This allows <code class="literal">pcs</code> on the cluster to connect to <code class="literal">pcs</code> on the <code class="literal">qdevice</code> host, but does not allow <code class="literal">pcs</code> on the <code class="literal">qdevice</code> host to connect to <code class="literal">pcs</code> on the cluster.
						</p><pre class="literallayout">[root@node1:~] # <code class="literal">pcs host auth qdevice</code>
Username: hacluster
Password:
qdevice: Authorized</pre></li><li class="listitem"><p class="simpara">
							Add the quorum device to the cluster.
						</p><p class="simpara">
							Before adding the quorum device, you can check the current configuration and status for the quorum device for later comparison. The output for these commands indicates that the cluster is not yet using a quorum device.
						</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum config</code>
Options:</pre><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum status</code>
Quorum information
------------------
Date:             Wed Jun 29 13:15:36 2016
Quorum provider:  corosync_votequorum
Nodes:            2
Node ID:          1
Ring ID:          1/8272
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   2
Highest expected: 2
Total votes:      2
Quorum:           1
Flags:            2Node Quorate

Membership information
----------------------
    Nodeid      Votes    Qdevice Name
         1          1         NR node1 (local)
         2          1         NR node2</pre><p class="simpara">
							The following command adds the quorum device that you have previously created to the cluster. You cannot use more than one quorum device in a cluster at the same time. However, one quorum device can be used by several clusters at the same time. This example command configures the quorum device to use the <code class="literal">ffsplit</code> algorithm. For information on the configuration options for the quorum device, see the <code class="literal">corosync-qdevice</code>(8) man page.
						</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum device add model net host=qdevice algorithm=ffsplit</code>
Setting up qdevice certificates on nodes...
node2: Succeeded
node1: Succeeded
Enabling corosync-qdevice...
node1: corosync-qdevice enabled
node2: corosync-qdevice enabled
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded
Corosync configuration reloaded
Starting corosync-qdevice...
node1: corosync-qdevice started
node2: corosync-qdevice started</pre></li><li class="listitem"><p class="simpara">
							Check the configuration status of the quorum device.
						</p><p class="simpara">
							From the cluster side, you can execute the following commands to see how the configuration has changed.
						</p><p class="simpara">
							The <code class="literal">pcs quorum config</code> shows the quorum device that has been configured.
						</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum config</code>
Options:
Device:
  Model: net
    algorithm: ffsplit
    host: qdevice</pre><p class="simpara">
							The <code class="literal">pcs quorum status</code> command shows the quorum runtime status, indicating that the quorum device is in use.
						</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum status</code>
Quorum information
------------------
Date:             Wed Jun 29 13:17:02 2016
Quorum provider:  corosync_votequorum
Nodes:            2
Node ID:          1
Ring ID:          1/8272
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   3
Highest expected: 3
Total votes:      3
Quorum:           2
Flags:            Quorate Qdevice

Membership information
----------------------
    Nodeid      Votes    Qdevice Name
         1          1    A,V,NMW node1 (local)
         2          1    A,V,NMW node2
         0          1            Qdevice</pre><p class="simpara">
							The <code class="literal">pcs quorum device status</code> shows the quorum device runtime status.
						</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum device status</code>
Qdevice information
-------------------
Model:                  Net
Node ID:                1
Configured node list:
    0   Node ID = 1
    1   Node ID = 2
Membership node list:   1, 2

Qdevice-net information
----------------------
Cluster name:           mycluster
QNetd host:             qdevice:5403
Algorithm:              ffsplit
Tie-breaker:            Node with lowest node ID
State:                  Connected</pre><p class="simpara">
							From the quorum device side, you can execute the following status command, which shows the status of the <code class="literal">corosync-qnetd</code> daemon.
						</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">pcs qdevice status net --full</code>
QNetd address:                  *:5403
TLS:                            Supported (client certificate required)
Connected clients:              2
Connected clusters:             1
Maximum send/receive size:      32768/32768 bytes
Cluster "mycluster":
    Algorithm:          ffsplit
    Tie-breaker:        Node with lowest node ID
    Node ID 2:
        Client address:         ::ffff:192.168.122.122:50028
        HB interval:            8000ms
        Configured node list:   1, 2
        Ring ID:                1.2050
        Membership node list:   1, 2
        TLS active:             Yes (client certificate verified)
        Vote:                   ACK (ACK)
    Node ID 1:
        Client address:         ::ffff:192.168.122.121:48786
        HB interval:            8000ms
        Configured node list:   1, 2
        Ring ID:                1.2050
        Membership node list:   1, 2
        TLS active:             Yes (client certificate verified)
        Vote:                   ACK (ACK)</pre></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="proc_managing-quorum-device-service-configuring-quorum-devices"/>Managing the Quorum Device Service</h2></div></div></div><p>
					PCS provides the ability to manage the quorum device service on the local host (<code class="literal">corosync-qnetd</code>), as shown in the following example commands. Note that these commands affect only the <code class="literal">corosync-qnetd</code> service.
				</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">pcs qdevice start net</code>
[root@qdevice:~]# <code class="literal">pcs qdevice stop net</code>
[root@qdevice:~]# <code class="literal">pcs qdevice enable net</code>
[root@qdevice:~]# <code class="literal">pcs qdevice disable net</code>
[root@qdevice:~]# <code class="literal">pcs qdevice kill net</code></pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="proc_managing-quorum-device-settingsconfiguring-quorum-devices"/>Managing the quorum device settings in a cluster</h2></div></div></div><p>
					The following sections describe the PCS commands that you can use to manage the quorum device settings in a cluster.
				</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="s3-changeqdevice-HAAR"/>Changing quorum device settings</h3></div></div></div><p>
						You can change the setting of a quorum device with the <code class="literal">pcs quorum device update</code> command.
					</p><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>
							To change the <code class="literal">host</code> option of quorum device model <code class="literal">net</code>, use the <code class="literal">pcs quorum device remove</code> and the <code class="literal">pcs quorum device add</code> commands to set up the configuration properly, unless the old and the new host are the same machine.
						</p></div><p>
						The following command changes the quorum device algorithm to <code class="literal">lms</code>.
					</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum device update model algorithm=lms</code>
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded
Corosync configuration reloaded
Reloading qdevice configuration on nodes...
node1: corosync-qdevice stopped
node2: corosync-qdevice stopped
node1: corosync-qdevice started
node2: corosync-qdevice started</pre></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="removing_a_quorum_device"/>Removing a quorum device</h3></div></div></div><p>
						Use the following command to remove a quorum device configured on a cluster node.
					</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum device remove</code>
Sending updated corosync.conf to nodes...
node1: Succeeded
node2: Succeeded
Corosync configuration reloaded
Disabling corosync-qdevice...
node1: corosync-qdevice disabled
node2: corosync-qdevice disabled
Stopping corosync-qdevice...
node1: corosync-qdevice stopped
node2: corosync-qdevice stopped
Removing qdevice certificates from nodes...
node1: Succeeded
node2: Succeeded</pre><p>
						After you have removed a quorum device, you should see the following error message when displaying the quorum device status.
					</p><pre class="literallayout">[root@node1:~]# <code class="literal">pcs quorum device status</code>
Error: Unable to get quorum status: corosync-qdevice-tool: Can't connect to QDevice socket (is QDevice running?): No such file or directory</pre></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="destroying_a_quorum_device"/>Destroying a quorum device</h3></div></div></div><p>
						To disable and stop a quorum device on the quorum device host and delete all of its configuration files, use the following command.
					</p><pre class="literallayout">[root@qdevice:~]# <code class="literal">pcs qdevice destroy net</code>
Stopping quorum device...
quorum device stopped
quorum device disabled
Quorum device 'net' configuration files removed</pre></div></div></div></div></body></html>
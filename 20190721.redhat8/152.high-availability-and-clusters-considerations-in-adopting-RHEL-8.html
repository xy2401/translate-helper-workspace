<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 13. High availability and clusters</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="high-availability-and-clusters-considerations-in-adopting-RHEL-8"/>Chapter 13. High availability and clusters</h1></div></div></div><p>
			In Red Hat Enterprise Linux 8, <code class="literal">pcs</code> fully supports the Corosync 3 cluster engine and the Kronosnet (knet) network abstraction layer for cluster communication. When planning an upgrade to a RHEL 8 cluster from an existing RHEL 7 cluster, some of the considerations you must take into account are as follows:
		</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
					<span class="strong"><strong>Application versions:</strong></span> What version of the highly-available application will the RHEL 8 cluster require?
				</li><li class="listitem">
					<span class="strong"><strong>Application process order:</strong></span> What may need to change in the start and stop processes of the application?
				</li><li class="listitem">
					<span class="strong"><strong>Cluster infrastructure:</strong></span> Since <code class="literal">pcs</code> supports multiple network connections in RHEL 8, does the number of NICs known to the cluster change?
				</li><li class="listitem">
					<span class="strong"><strong>Needed packages:</strong></span> Do you need to install all of the same packages on the new cluster?
				</li></ul></div><p>
			Because of these and other considerations for running a Pacemaker cluster in RHEL 8, it is not possible to perform in-place upgrades from RHEL 7 to RHEL 8 clusters and you must configure a new cluster in RHEL 8. You cannot run a cluster that includes nodes running both RHEL 7 and RHEL 8.
		</p><p>
			Additionally, you should plan for the following before performing an upgrade:
		</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
					<span class="strong"><strong>Final cutover:</strong></span> What is the process to stop the application running on the old cluster and start it on the new cluster to reduce application downtime?
				</li><li class="listitem">
					<span class="strong"><strong>Testing:</strong></span> Is it possible to test your migration strategy ahead of time in a development/test environment?
				</li></ul></div><p>
			The major differences in cluster creation and administration between RHEL 7 and RHEL 8 are listed in the following sections.
		</p><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="new_formats_for_literal_pcs_cluster_setup_literal_literal_pcs_cluster_node_add_literal_and_literal_pcs_cluster_node_remove_literal_commands"/>New formats for <code class="literal">pcs cluster setup</code>, <code class="literal">pcs cluster node add</code> and <code class="literal">pcs cluster node remove</code> commands</h1></div></div></div><p>
				In Red Hat Enterprise Linux 8, <code class="literal">pcs</code> fully supports the use of node names, which are now required and replace node addresses in the role of node identifier. Node addresses are now optional.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						In the <code class="literal">pcs host auth</code> command, node addresses default to node names.
					</li><li class="listitem">
						In the <code class="literal">pcs cluster setup</code> and <code class="literal">pcs cluster node add</code> commands, node addresses default to the node addresses specified in the <code class="literal">pcs host auth</code> command.
					</li></ul></div><p>
				With these changes, the formats for the commands to set up a cluster, add a node to a cluster, and remove a node from a cluster have changed. For information on these new command formats, see the help display for the <code class="literal">pcs cluster setup</code>, <code class="literal">pcs cluster node add</code> and <code class="literal">pcs cluster node remove</code> commands.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="master_resources_renamed_to_promotable_clone_resources"/>Master resources renamed to promotable clone resources</h1></div></div></div><p>
				Red Hat Enterprise Linux (RHEL) 8 supports Pacemaker 2.0, in which a master/slave resource is no longer a separate type of resource but a standard clone resource with a <code class="literal">promotable</code> meta-attribute set to <code class="literal">true</code>. The following changes have been implemented in support of this update:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						It is no longer possible to create master resources with the <code class="literal">pcs</code> command. Instead, it is possible to create <code class="literal">promotable</code> clone resources. Related keywords and commands have been changed from <code class="literal">master</code> to <code class="literal">promotable</code>.
					</li><li class="listitem">
						All existing master resources are displayed as promotable clone resources.
					</li><li class="listitem">
						When managing a RHEL7 cluster in the Web UI, master resources are still called master, as RHEL7 clusters do not support promotable clones.
					</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="new_commands_for_authenticating_nodes_in_a_cluster"/>New commands for authenticating nodes in a cluster</h1></div></div></div><p>
				Red Hat Enterprise Linux (RHEL) 8 incorporates the following changes to the commands used to authenticate nodes in a cluster.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						The new command for authentication is <code class="literal">pcs host auth</code>. This command allows users to specify host names, addresses and <code class="literal">pcsd</code> ports.
					</li><li class="listitem">
						The <code class="literal">pcs cluster auth</code> command authenticates only the nodes in a local cluster and does not accept a node list
					</li><li class="listitem">
						It is now possible to specify an address for each node. <code class="literal">pcs</code>/<code class="literal">pcsd</code> will then communicate with each node using the specified address. These addresses can be different than the ones <code class="literal">corosync</code> uses internally.
					</li><li class="listitem">
						The <code class="literal">pcs pcsd clear-auth</code> command has been replaced by the <code class="literal">pcs pcsd deauth</code> and <code class="literal">pcs host deauth</code> commands. The new commands allow users to deauthenticate a single host as well as all hosts.
					</li><li class="listitem">
						Previously, node authentication was bidirectional, and running the <code class="literal">pcs cluster auth</code> command caused all specified nodes to be authenticated against each other. The <code class="literal">pcs host auth</code> command, however, causes only the local host to be authenticated against the specified nodes. This allows better control of what node is authenticated against what other nodes when running this command. On cluster setup itself, and also when adding a node, <code class="literal">pcs</code> automatically synchronizes tokens on the cluster, so all nodes in the cluster are still automatically authenticated as before and the cluster nodes can communicate with each other.
					</li></ul></div><p>
				Note that these changes are not backward compatible. Nodes that were authenticated on a RHEL 7 system will need to be authenticated again.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="lvm_volumes_in_a_red_hat_high_availability_active_passive_cluster"/>LVM volumes in a Red Hat High Availability active/passive cluster</h1></div></div></div><p>
				When configuring LVM volumes as resources in a Red Hat HA active/passive cluster in RHEL 8, you configure the volumes as an <code class="literal">LVM-activate</code> resource. In RHEL 7, you configured the volumes as an <code class="literal">LVM</code> resource. For an example of a cluster configuration procedure that includes configuring an LVM volume as a resource in an active/passive cluster in RHEL 8, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_high_availability_clusters/index#assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters">Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster</a>.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="shared_lvm_volumes_in_a_red_hat_high_availability_active_active_cluster"/>Shared LVM volumes in a Red Hat High Availability active/active cluster</h1></div></div></div><p>
				In Red Hat Enterprise Linux 8, LVM uses the LVM lock daemon <code class="literal">lvmlockd</code> instead of <code class="literal">clvmd</code> for managing shared storage devices in an active/active cluster. This requires that you configure the logical volumes on which you mount a GFS2 file system as shared logical volumes. Additionally, this requires that you use the <code class="literal">LVM-activate</code> resource agent to manage an LVM volume and that you use the <code class="literal">lvmlockd</code> resource agent to manage the <code class="literal">lvmlockd</code> daemon. For a full procedure for configuring a RHEL 8 Pacemaker cluster that includes GFS2 file systems using shared logical volumes, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_high_availability_clusters/index#proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster">Configuring a GFS2 file system in a cluster</a>.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="gfs2_file_systems_in_a_rhel_8_pacemaker_cluster"/>GFS2 file systems in a RHEL 8 Pacemaker cluster</h1></div></div></div><p>
				In Red Hat Enterprise Linux 8, LVM uses the LVM lock daemon <code class="literal">lvmlockd</code> instead of <code class="literal">clvmd</code> for managing shared storage devices in an active/active cluster as described in <a class="xref" href="file-systems-and-storage_considerations-in-adopting-RHEL-8.html#removal-of-clvmd-for-managing-shared-storage-devices_file-systems-and-storage" title="Removal of clvmd for managing shared storage devices">the section called “Removal of <code class="literal">clvmd</code> for managing shared storage devices”</a>.
			</p><p>
				In order to use GFS2 file systems that were created on a RHEL 7 system in a RHEL 8 cluster, you must configure the logical volumes on which they are mounted as shared logical volumes in a RHEL 8 system, and you must start locking for the volume group. For an example of the procedure that configures existing RHEL 7 logical volumes as shared logical volumes for use in a RHEL 8 Pacemaker cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_high_availability_clusters/index#proc_migrate-gfs2-rhel7-rhel8-configuring-gfs2-cluster">Migrating a GFS2 file system from RHEL7 to RHEL8</a>.
			</p></div></div></body></html>
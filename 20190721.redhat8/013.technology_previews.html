<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 6. Technology previews</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="technology_previews"/>Chapter 6. Technology previews</h1></div></div></div><p>
			This part provides a list of all Technology Previews available in Red Hat Enterprise Linux 8.0.
		</p><p>
			For information on Red Hat scope of support for Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
		</p><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="kernel_technology_preview"/>Kernel</h1></div></div></div><p><a id="BZ-1559616"/></p><div class="title"><strong><span class="strong"><strong>eBPF</strong></span> available as a Technology Preview</strong></div><p>
					The <span class="strong"><strong>extended Berkeley Packet Filtering (eBPF)</strong></span> feature is available as a Technology Preview for both networking and tracing. <span class="strong"><strong>eBPF</strong></span> enables the user space to attach custom programs onto a variety of points (sockets, trace points, packet reception) to receive and process data. The feature includes a new system call <code class="literal">bpf()</code>, which supports creating various types of maps, and also to insert various types of programs into the kernel. Note that the <code class="literal">bpf()</code> syscall can be successfully used only by a user with the <code class="literal">CAP_SYS_ADMIN</code> capability, such as a root user. See the <code class="literal">bpf</code>(2) man page for more information.
				</p><p>
				(BZ#1559616)
			</p><p><a id="BZ-1548302"/></p><div class="title"><strong><code class="literal">BCC</code> is available as a Technology Preview</strong></div><p>
					<code class="literal">BPF Compiler Collection (BCC)</code> is a user space tool kit for creating efficient kernel tracing and manipulation programs that is available as a Technology Preview in Red Hat Enterprise Linux 8. <code class="literal">BCC</code> provides tools for I/O analysis, networking, and monitoring of Linux operating systems using the <code class="literal">extended Berkeley Packet Filtering (eBPF)</code>.
				</p><p>
				(BZ#1548302)
			</p><p><a id="BZ-1401552"/></p><div class="title"><strong><span class="strong"><strong>Control Group v2</strong></span> available as a Technology Preview in RHEL 8</strong></div><p>
					<span class="strong"><strong>Control Group v2</strong></span> mechanism is a unified hierarchy control group. <span class="strong"><strong>Control Group v2</strong></span> organizes processes hierarchically and distributes system resources along the hierarchy in a controlled and configurable manner.
				</p><p>
				Unlike the previous version, <span class="strong"><strong>Control Group v2</strong></span> has only a single hierarchy. This single hierarchy enables the Linux kernel to:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						Categorize processes based on the role of their owner.
					</li><li class="listitem">
						Eliminate issues with conflicting policies of multiple hierarchies.
					</li></ul></div><p>
				<span class="strong"><strong>Control Group v2</strong></span> supports numerous controllers:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p class="simpara">
						CPU controller regulates the distribution of CPU cycles. This controller implements:
					</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
								Weight and absolute bandwidth limit models for normal scheduling policy.
							</li><li class="listitem">
								Absolute bandwidth allocation model for real time scheduling policy.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Memory controller regulates the memory distribution. Currently, the following types of memory usages are tracked:
					</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
								Userland memory - page cache and anonymous memory.
							</li><li class="listitem">
								Kernel data structures such as dentries and inodes.
							</li><li class="listitem">
								TCP socket buffers.
							</li></ul></div></li><li class="listitem">
						I/O controller regulates the distribution of I/O resources.
					</li><li class="listitem">
						Writeback controller interacts with both Memory and I/O controllers and is <span class="strong"><strong>Control Group v2</strong></span> specific.
					</li></ul></div><p>
				The information above was based on link: <a class="link" href="https://www.kernel.org/doc/Documentation/cgroup-v2.txt">https://www.kernel.org/doc/Documentation/cgroup-v2.txt</a>. You can refer to the same link to obtain more information about particular <span class="strong"><strong>Control Group v2</strong></span> controllers.
			</p><p>
				(BZ#1401552)
			</p><p><a id="BZ-1520209"/></p><div class="title"><strong><code class="literal">early kdump</code> available as a Technology Preview in Red Hat Enterprise Linux 8</strong></div><p>
					The <code class="literal">early kdump</code> feature allows the crash kernel and initramfs to load early enough to capture the <code class="literal">vmcore</code> information even for early crashes. For more details about <code class="literal">early kdump</code>, see the <code class="literal">/usr/share/doc/kexec-tools/early-kdump-howto.txt</code> file.
				</p><p>
				(BZ#1520209)
			</p><p><a id="BZ-1524683"/></p><div class="title"><strong>The <code class="literal">ibmvnic</code> device driver available as a Technology Preview</strong></div><p>
					With Red Hat Enterprise Linux 8.0, the IBM Virtual Network Interface Controller (vNIC) driver for IBM POWER architectures, <code class="literal">ibmvnic</code>, is available as a Technology Preview. vNIC is a PowerVM virtual networking technology that delivers enterprise capabilities and simplifies network management. It is a high-performance, efficient technology that when combined with SR-IOV NIC provides bandwidth control Quality of Service (QoS) capabilities at the virtual NIC level. vNIC significantly reduces virtualization overhead, resulting in lower latencies and fewer server resources, including CPU and memory, required for network virtualization.
				</p><p>
				(BZ#1524683)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="graphics_infrastructures_2"/>Graphics infrastructures</h1></div></div></div><p><a id="BZ-1698565"/></p><div class="title"><strong>VNC remote console available as a Technology Preview for the 64-bit ARM architecture</strong></div><p>
					On the 64-bit ARM architecture, the Virtual Network Computing (VNC) remote console is available as a Technology Preview. Note that the rest of the graphics stack is currently unverified for the 64-bit ARM architecture.
				</p><p>
				(BZ#1698565)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="hardware_enablement_2"/>Hardware enablement</h1></div></div></div><p><a id="BZ-1654482"/></p><div class="title"><strong>The cluster-aware MD RAID1 is available as a technology preview.</strong></div><p>
					RAID1 cluster is not enabled by default in the kernel space. If you want to have a try with RAID1 cluster, you need to build the kernel with RAID1 cluster as a module first, use the following steps:
				</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem">
						Enter the <code class="literal">make menuconfig</code> command.
					</li><li class="listitem">
						Enter the <code class="literal">make &amp;&amp; make modules &amp;&amp; make modules_install &amp;&amp; make install</code> command.
					</li><li class="listitem">
						Enter the <code class="literal">reboot</code> command.
					</li></ol></div><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1654482">BZ#1654482</a>)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="identity_management_3"/>Identity Management</h1></div></div></div><p><a id="BZ-1664718"/></p><div class="title"><strong>DNSSEC available as Technology Preview in IdM</strong></div><p>
					Identity Management (IdM) servers with integrated DNS now support DNS Security Extensions (DNSSEC), a set of extensions to DNS that enhance security of the DNS protocol. DNS zones hosted on IdM servers can be automatically signed using DNSSEC. The cryptographic keys are automatically generated and rotated.
				</p><p>
				Users who decide to secure their DNS zones with DNSSEC are advised to read and follow these documents:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						DNSSEC Operational Practices, Version 2: <a class="link" href="http://tools.ietf.org/html/rfc6781#section-2">http://tools.ietf.org/html/rfc6781#section-2</a>
					</li><li class="listitem">
						Secure Domain Name System (DNS) Deployment Guide: <a class="link" href="http://dx.doi.org/10.6028/NIST.SP.800-81-2">http://dx.doi.org/10.6028/NIST.SP.800-81-2</a>
					</li><li class="listitem">
						DNSSEC Key Rollover Timing Considerations: <a class="link" href="http://tools.ietf.org/html/rfc7583">http://tools.ietf.org/html/rfc7583</a>
					</li></ul></div><p>
				Note that IdM servers with integrated DNS use DNSSEC to validate DNS answers obtained from other DNS servers. This might affect the availability of DNS zones that are not configured in accordance with recommended naming practices.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1664718">BZ#1664718</a>)
			</p><p><a id="BZ-1664719"/></p><div class="title"><strong>Identity Management JSON-RPC API available as Technology Preview</strong></div><p>
					An API is available for Identity Management (IdM). To view the API, IdM also provides an API browser as Technology Preview.
				</p><p>
				In Red Hat Enterprise Linux 7.3, the IdM API was enhanced to enable multiple versions of API commands. Previously, enhancements could change the behavior of a command in an incompatible way. Users are now able to continue using existing tools and scripts even if the IdM API changes. This enables:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						Administrators to use previous or later versions of IdM on the server than on the managing client.
					</li><li class="listitem">
						Developers to use a specific version of an IdM call, even if the IdM version changes on the server.
					</li></ul></div><p>
				In all cases, the communication with the server is possible, regardless if one side uses, for example, a newer version that introduces new options for a feature.
			</p><p>
				For details on using the API, see <a class="link" href="https://access.redhat.com/articles/2728021">Using the Identity Management API to Communicate with the IdM Server (TECHNOLOGY PREVIEW)</a>.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1664719">BZ#1664719</a>)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="file_systems_and_storage_3"/>File systems and storage</h1></div></div></div><p><a id="BZ-1663281"/></p><div class="title"><strong>Aero adapters available as a Technology Preview</strong></div><p>
					The following Aero adapters are available as a Technology Preview:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						PCI ID 0x1000:0x00e2 and 0x1000:0x00e6, controlled by the <code class="literal">mpt3sas</code> driver
					</li><li class="listitem">
						PCI ID 0x1000:Ox10e5 and 0x1000:0x10e6, controlled by the <code class="literal">megaraid_sas</code> driver
					</li></ul></div><p>
				(BZ#1663281)
			</p><p><a id="JIRA-RHELPLAN-1212"/></p><div class="title"><strong>Stratis is now available</strong></div><p>
					Stratis is a new local storage manager. It provides managed file systems on top of pools of storage with additional features to the user.
				</p><p>
				Stratis enables you to more easily perform storage tasks such as:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						Manage snapshots and thin provisioning
					</li><li class="listitem">
						Automatically grow file system sizes as needed
					</li><li class="listitem">
						Maintain file systems
					</li></ul></div><p>
				To administer Stratis storage, use the <code class="literal">stratis</code> utility, which communicates with the <code class="literal">stratisd</code> background service.
			</p><p>
				Stratis is provided as a Technology Preview.
			</p><p>
				For more information, see the Stratis documentation: <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_file_systems/managing-layered-local-storage-with-stratis_managing-file-systems">Managing layered local storage with Stratis</a>.
			</p><p>
				(JIRA:RHELPLAN-1212)
			</p><p><a id="BZ-1690207"/></p><div class="title"><strong>OverlayFS</strong></div><p>
					OverlayFS is a type of union file system. It enables you to overlay one file system on top of another. Changes are recorded in the upper file system, while the lower file system remains unmodified. This allows multiple users to share a file-system image, such as a container or a DVD-ROM, where the base image is on read-only media. See the Linux kernel documentation for additional information: <a class="link" href="https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt">https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt</a>.
				</p><p>
				OverlayFS remains a Technology Preview under most circumstances. As such, the kernel logs warnings when this technology is activated.
			</p><p>
				Full support is available for OverlayFS when used with supported container engines (<code class="literal">podman</code>, <code class="literal">cri-o</code>, or <code class="literal">buildah</code>) under the following restrictions:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						OverlayFS is supported for use only as a container engine graph driver. Its use is supported only for container COW content, not for persistent storage. You must place any persistent storage on non-OverlayFS volumes. Only the default container engine configuration can be used; that is, one level of overlay, one lowerdir, and both lower and upper levels are on the same file system.
					</li><li class="listitem">
						Only XFS is currently supported for use as a lower layer file system.
					</li></ul></div><p>
				Additionally, the following rules and limitations apply to using OverlayFS:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						The OverlayFS kernel ABI and userspace behavior are not considered stable, and might see changes in future updates.
					</li><li class="listitem"><p class="simpara">
						OverlayFS provides a restricted set of the POSIX standards. Test your application thoroughly before deploying it with OverlayFS. The following cases are not POSIX-compliant:
					</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
								Lower files opened with <code class="literal">O_RDONLY</code> do not receive <code class="literal">st_atime</code> updates when the files are read.
							</li><li class="listitem">
								Lower files opened with <code class="literal">O_RDONLY</code>, then mapped with <code class="literal">MAP_SHARED</code> are inconsistent with subsequent modification.
							</li><li class="listitem"><p class="simpara">
								Fully compliant <code class="literal">st_ino</code> or <code class="literal">d_ino</code> values are not enabled by default on RHEL 8, but you can enable full POSIX compliance for them with a module option or mount option.
							</p><p class="simpara">
								To get consistent inode numbering, use the <code class="literal">xino=on</code> mount option.
							</p><p class="simpara">
								You can also use the <code class="literal">redirect_dir=on</code> and <code class="literal">index=on</code> options to improve POSIX compliance. These two options make the format of the upper layer incompatible with an overlay without these options. That is, you might get unexpected results or errors if you create an overlay with <code class="literal">redirect_dir=on</code> or <code class="literal">index=on</code>, unmount the overlay, then mount the overlay without these options.
							</p></li></ul></div></li><li class="listitem"><p class="simpara">
						Commands used with XFS:
					</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
								XFS file systems must be created with the <code class="literal">-n ftype=1</code> option enabled for use as an overlay.
							</li><li class="listitem">
								With the rootfs and any file systems created during system installation, set the <code class="literal">--mkfsoptions=-n ftype=1</code> parameters in the Anaconda kickstart.
							</li><li class="listitem">
								When creating a new file system after the installation, run the <code class="literal"># mkfs -t xfs -n ftype=1 /PATH/TO/DEVICE</code> command.
							</li><li class="listitem">
								To determine whether an existing file system is eligible for use as an overlay, run the <code class="literal"># xfs_info /PATH/TO/DEVICE | grep ftype</code> command to see if the <code class="literal">ftype=1</code> option is enabled.
							</li></ul></div></li><li class="listitem">
						SELinux security labels are enabled by default in all supported container engines with OverlayFS.
					</li><li class="listitem">
						There are several known issues associated with OverlayFS in this release. For details, see <span class="emphasis"><em>Non-standard behavior</em></span> in the Linux kernel documentation: <a class="link" href="https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt">https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt</a>.
					</li></ul></div><p>
				(BZ#1690207)
			</p><p><a id="BZ-1627455"/></p><div class="title"><strong>File system DAX is now available for ext4 and XFS as a Technology Preview</strong></div><p>
					In Red Hat Enterprise Linux 8.0, file system DAX is available as a Technology Preview. DAX provides a means for an application to directly map persistent memory into its address space. To use DAX, a system must have some form of persistent memory available, usually in the form of one or more Non-Volatile Dual In-line Memory Modules (NVDIMMs), and a file system that supports DAX must be created on the NVDIMM(s). Also, the file system must be mounted with the <code class="literal">dax</code> mount option. Then, an <code class="literal">mmap</code> of a file on the dax-mounted file system results in a direct mapping of storage into the application’s address space.
				</p><p>
				(BZ#1627455)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="high_availability_and_clusters_3"/>High availability and clusters</h1></div></div></div><p><a id="BZ-1619620"/></p><div class="title"><strong>Pacemaker <code class="literal">podman</code> bundles available as a Technology Preview</strong></div><p>
					Pacemaker container bundles now run on the <code class="literal">podman</code> container platform, with the container bundle feature being available as a Technology Preview. There is one exception to this feature being Technology Preview: Red Hat fully supports the use of Pacemaker bundles for Red Hat Openstack.
				</p><p>
				(BZ#1619620)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="networking_technology_preview"/>Networking</h1></div></div></div><p><a id="BZ-1503672"/></p><div class="title"><strong><span class="strong"><strong>XDP</strong></span> available as a Technology Preview</strong></div><p>
					The eXpress Data Path (XDP) feature, which is available as a Technology Preview, provides a means to attach extended Berkeley Packet Filter (eBPF) programs for high-performance packet processing at an early point in the kernel ingress data path, allowing efficient programmable packet analysis, filtering, and manipulation.
				</p><p>
				(BZ#1503672)
			</p><p><a id="BZ-1699825"/></p><div class="title"><strong>eBPF for tc available as a Technology Preview</strong></div><p>
					As a Technology Preview, the Traffic Control (tc) kernel subsystem and the <span class="strong"><strong>tc</strong></span> tool can attach extended Berkeley Packet Filtering (eBPF) programs as packet classifiers and actions for both ingress and egress queueing disciplines. This enables programmable packet processing inside the kernel network data path.
				</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1699825">BZ#1699825</a>)
			</p><p><a id="BZ-1633143"/></p><div class="title"><strong><code class="literal">AF_XDP</code> available as a Technology Preview</strong></div><p>
					<code class="literal">Address Family eXpress Data Path</code> (<code class="literal">AF_XDP</code>) socket is designed for high-performance packet processing. It accompanies <code class="literal">XDP</code> and grants efficient redirection of programmatically selected packets to user space applications for further processing.
				</p><p>
				(BZ#1633143)
			</p><p><a id="BZ-1570255"/></p><div class="title"><strong>KTLS available as a Technology Preview</strong></div><p>
					In Red Hat Enterprise Linux 8, Kernel Transport Layer Security (KTLS) is provided as a Technology Preview. KTLS handles TLS records using the symmetric encryption or decryption algorithms in the kernel for the AES-GCM cipher. KTLS also provides the interface for offloading TLS record encryption to Network Interface Controllers (NICs) that support this functionality.
				</p><p>
				(BZ#1570255)
			</p><p><a id="BZ-1581898"/></p><div class="title"><strong><code class="literal">TIPC</code> available as a Technology Preview</strong></div><p>
					The Transparent Inter Process Communication (<code class="literal">TIPC</code>) is a protocol specially designed for efficient communication within clusters of loosely paired nodes. It works as a kernel module and provides a <code class="literal">tipc</code> tool in <code class="literal">iproute2</code> package to allow designers to create applications that can communicate quickly and reliably with other applications regardless of their location within the cluster. This feature is available as a Technology Preview.
				</p><p>
				(BZ#1581898)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="virtualization_3"/>Virtualization</h1></div></div></div><p><a id="BZ-1501618"/></p><div class="title"><strong>AMD SEV for KVM virtual machines</strong></div><p>
					As a Technology Preview, RHEL 8 introduces the Secure Encrypted Virtualization (SEV) feature for AMD EPYC host machines that use the KVM hypervisor. If enabled on a virtual machine (VM), SEV encrypts VM memory so that the host cannot access data on the VM. This increases the security of the VM if the host is successfully infected by malware.
				</p><p>
				Note that the number of VMs that can use this feature at a time on a single host is determined by the host hardware. Current AMD EPYC processors support up to 15 running VMs using SEV.
			</p><p>
				Also note that for VMs with SEV configured to be able to boot, you must also configure the VM with a hard memory limit. To do so, add the following to the VM’s XML configuration:
			</p><pre class="literallayout">&lt;memtune&gt;
  &lt;hard_limit unit='KiB'&gt;N&lt;/hard_limit&gt;
&lt;/memtune&gt;</pre><p>
				The recommended value for N is equal to or greater then the guest RAM + 256 MiB. For example, if the guest is assigned 2 GiB RAM, N should be 2359296 or greater.
			</p><p>
				(BZ#1501618, BZ#1501607)
			</p><p><a id="BZ-1528684"/></p><div class="title"><strong>Intel vGPU</strong></div><p>
					As a Technology Preview, it is now possible to divide a physical Intel GPU device into multiple virtual devices referred to as <code class="literal">mediated devices</code>. These mediated devices can then be assigned to multiple virtual machines (VMs) as virtual GPUs. As a result, these VMs share the performance of a single physical Intel GPU.
				</p><p>
				Note that only selected Intel GPUs are compatible with the vGPU feature. In addition, assigning a physical GPU to VMs makes it impossible for the host to use the GPU, and may prevent graphical display output on the host from working.
			</p><p>
				(BZ#1528684)
			</p><p><a id="BZ-1505999"/></p><div class="title"><strong>Nested virtualization now available on IBM POWER 9</strong></div><p>
					As a Technology Preview, it is now possible to use the nested virtualization features on RHEL 8 host machines running on IBM POWER 9 systems. Nested virtualization enables KVM virtual machines (VMs) to act as hypervisors, which allows for running VMs inside VMs.
				</p><p>
				Note that nested virtualization also remains a Technology Preview on AMD64 and Intel 64 systems.
			</p><p>
				Also note that for nested virtualization to work on IBM POWER 9, the host, the guest, and the nested guests currently all need to run one of the following operating systems:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						RHEL 8
					</li><li class="listitem">
						RHEL 7 for POWER 9
					</li></ul></div><p>
				(BZ#1505999, BZ#1518937)
			</p><p><a id="BZ-1519039"/></p><div class="title"><strong>KVM virtualization is usable in RHEL 8 Hyper-V virtual machines</strong></div><p>
					As a Technology Preview, nested KVM virtualization can now be used on the Microsoft Hyper-V hypervisor. As a result, you can create virtual machines on a RHEL 8 guest system running on a Hyper-V host.
				</p><p>
				Note that currently, this feature only works on Intel systems. In addition, nested virtualization is in some cases not enabled by default on Hyper-V. To enable it, see the following Microsoft documentation:
			</p><p>
				<a class="link" href="https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/user-guide/nested-virtualization">https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/user-guide/nested-virtualization</a>
			</p><p>
				(BZ#1519039)
			</p></div></div></body></html>
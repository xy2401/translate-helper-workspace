<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 8. Known issues</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="known-issues"/>Chapter 8. Known issues</h1></div></div></div><p>
			This part describes known issues in Red Hat Enterprise Linux 8.
		</p><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="the_web_console"/>The web console</h1></div></div></div><p><a id="BZ-1631905"/></p><div class="title"><strong>Logging to RHEL web console with session_recording shell is not possible</strong></div><p>
					Currently, the RHEL web console logins will fail for tlog recording-enabled users. RHEL web console requires a user’s shell to be present in the <code class="literal">/etc/shells</code> directory to allow a successful login. However, if <code class="literal">tlog-rec-session</code> is added to <code class="literal">/etc/shells</code>, a recorded user is able to disable recording by changing the shell from <code class="literal">tlog-rec-session</code> to another shell from <code class="literal">/etc/shells</code>, using the "chsh" utility. Red Hat does not recommend adding <code class="literal">tlog-rec-session</code> to <code class="literal">/etc/shells</code> for this reason.
				</p><p>
				(BZ#1631905)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="installer_and_image_creation_3"/>Installer and image creation</h1></div></div></div><p><a id="BZ-1640697"/></p><div class="title"><strong>The <code class="literal">auth</code> and <code class="literal">authconfig</code> Kickstart commands require the AppStream repository</strong></div><p>
					The <code class="literal">authselect-compat</code> package is required by the <code class="literal">auth</code> and <code class="literal">authconfig</code> Kickstart commands during installation. Without this package, the installation fails if <code class="literal">auth</code> or <code class="literal">authconfig</code> are used. However, by design, the <code class="literal">authselect-compat</code> package is only available in the AppStream repository.
				</p><p>
				To work around this problem, verify that the BaseOS and AppStream repositories are available to the installer or use the <code class="literal">authselect</code> Kickstart command during installation.
			</p><p>
				(BZ#1640697)
			</p><p><a id="BZ-1687489"/></p><div class="title"><strong>The <code class="literal">xorg-x11-drv-fbdev</code>, <code class="literal">xorg-x11-drv-vesa</code>, and <code class="literal">xorg-x11-drv-vmware</code> video drivers are not installed by default</strong></div><p>
					Workstations with specific models of NVIDIA graphics cards and workstations with specific AMD accelerated processing units will not display the graphical login window after a RHEL 8.0 Server installation.
				</p><p>
				To work around this problem, perform a RHEL 8.0 <code class="literal">Workstation</code> installation on a workstation machine. If a RHEL 8.0 <code class="literal">Server</code> installation is required on the workstation, manually install the <code class="literal">base-x</code> package group after installation by running the <code class="literal">yum -y groupinstall base-x</code> command.
			</p><p>
				In addition, virtual machines relying on EFI for graphics support, such as Hyper-V, are also affected. If you selected the <code class="literal">Server with GUI</code> base environment on Hyper-V, you might be unable to log in due to a black screen displayed on reboot. To work around this problem on Hyper-v, enable multi- or single-user mode using the following steps:
			</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem">
						Reboot the virtual machine.
					</li><li class="listitem">
						During the booting process, select the required kernel using the up and down arrow keys on your keyboard.
					</li><li class="listitem">
						Press the <code class="literal">e</code> key on your keyboard to edit the kernel command line.
					</li><li class="listitem">
						Add <code class="literal">systemd.unit=multi-user.target</code> to the kernel command line in GRUB.
					</li><li class="listitem">
						Press <code class="literal">Ctrl-X</code> to start the virtual machine.
					</li><li class="listitem">
						After logging in, run the <code class="literal">yum -y groupinstall base-x</code> command.
					</li><li class="listitem">
						Reboot the virtual machine to access the graphical mode.
					</li></ol></div><p>
				(BZ#1687489)
			</p><p><a id="BZ-1672405"/></p><div class="title"><strong>Installation fails when using the <code class="literal">reboot --kexec</code> command</strong></div><p>
					The RHEL 8 installation fails when using a Kickstart file that contains the <code class="literal">reboot --kexec</code> command. To avoid the problem, use the <code class="literal">reboot</code> command instead of <code class="literal">reboot --kexec</code> in your Kickstart file.
				</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1672405">BZ#1672405</a>)
			</p><p><a id="BZ-1692746"/></p><div class="title"><strong>Copying the content of the <code class="literal">Binary DVD.iso</code> file to a partition omits the <code class="literal">.treeinfo</code> and <code class="literal">.discinfo</code> files</strong></div><p>
					During local installation, while copying the content of the RHEL 8 Binary DVD.iso image file to a partition, the <code class="literal">*</code> in the <code class="literal">cp &lt;path&gt;/\* &lt;mounted partition&gt;/dir</code> command fails to copy the <code class="literal">.treeinfo</code> and <code class="literal">.discinfo</code> files. These files are required for a successful installation. As a result, the BaseOS and AppStream repositories are not loaded, and a debug-related log message in the <code class="literal">anaconda.log</code> file is the only record of the problem.
				</p><p>
				To work around the problem, copy the missing <code class="literal">.treeinfo</code> and <code class="literal">.discinfo</code> files to the partition.
			</p><p>
				(BZ#1692746)
			</p><p><a id="BZ-1696609"/></p><div class="title"><strong>Anaconda installation includes low limits of minimal resources setting requirements</strong></div><p>
					Anaconda initiates the installation on systems with minimal resource settings required available and do not provide previous message warning about the required resources for performing the installation successfully. As a result, the installation can fail and the output errors do not provide clear messages for possible debug and recovery. To work around this problem, make sure that the system has the minimal resources settings required for installation: 2GB memory on PPC64(LE) and 1GB on x86_64. As a result, it should be possible to perform a successful installation.
				</p><p>
				(BZ#1696609)
			</p><p><a id="BZ-1697896"/></p><div class="title"><strong>The <code class="literal">reboot --kexec</code> and <code class="literal">inst.kexec</code> commands do not provide a predictable system state</strong></div><p>
					Performing a RHEL installation with the <code class="literal">reboot --kexec</code> Kickstart command or the <code class="literal">inst.kexec</code> kernel boot parameters do not provide the same predictable system state as a full reboot. As a consequence, switching to the installed system without rebooting can produce unpredictable results.
				</p><p>
				Note that the <code class="literal">kexec</code> feature is deprecated and will be removed in a future release of Red Hat Enterprise Linux.
			</p><p>
				(BZ#1697896)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="kernel_2"/>Kernel</h1></div></div></div><p><a id="BZ-1623712"/></p><div class="title"><strong>The <span class="strong"><strong>i40iw</strong></span> module does not load automatically on boot</strong></div><p>
					Due to many i40e NICs not supporting iWarp and the <span class="strong"><strong>i40iw</strong></span> module not fully supporting suspend/resume, this module is not automatically loaded by default to ensure suspend/resume works properly. To work around this problem, manually edit the <code class="literal">/lib/udev/rules.d/90-rdma-hw-modules.rules</code> file to enable automated load of <span class="strong"><strong>i40iw</strong></span>.
				</p><p>
				Also note that if there is another RDMA device installed with a i40e device on the same machine, the non-i40e RDMA device triggers the <span class="strong"><strong>rdma</strong></span> service, which loads all enabled RDMA stack modules, including the <span class="strong"><strong>i40iw</strong></span> module.
			</p><p>
				(BZ#1623712)
			</p><p><a id="BZ-1598448"/></p><div class="title"><strong>The system sometimes becomes unresponsive when many devices are connected</strong></div><p>
					When Red Hat Enterprise Linux 8 configures a large number of devices, a large number of console messages occurs on the system console. This happens, for example, when there are a large number of logical unit numbers (LUNs), with multiple paths to each LUN. The flood of console messages, in addition to other work the kernel is doing, might cause the kernel watchdog to force a kernel panic because the kernel appears to be hung.
				</p><p>
				Because the scan happens early in the boot cycle, the system becomes unresponsive when many devices are connected. This typically occurs at boot time.
			</p><p>
				If <code class="literal">kdump</code> is enabled on your machine during the device scan event after boot, the hard lockup results in a capture of a <code class="literal">vmcore</code> image.
			</p><p>
				To work around this problem, increase the watchdog lockup timer. To do so, add the <code class="literal">watchdog_thresh=<span class="emphasis"><em>N</em></span></code> option to the kernel command line. Replace <code class="literal"><span class="emphasis"><em>N</em></span></code> with the number of seconds:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						If you have less than a thousand devices, use <code class="literal">30</code>.
					</li><li class="listitem">
						If you have more than a thousand devices, use <code class="literal">60</code>.
					</li></ul></div><p>
				For storage, the number of device is the number of paths to all the LUNs: generally, the number of <code class="literal">/dev/sd*</code> devices.
			</p><p>
				After applying the workaround, the system no longer becomes unresponsive when configuring a large amount of devices.
			</p><p>
				(BZ#1598448)
			</p><p><a id="BZ-1153521"/></p><div class="title"><strong>KSM sometimes ignores NUMA memory policies</strong></div><p>
					When the kernel shared memory (KSM) feature is enabled with the <code class="literal">merge_across_nodes=1</code> parameter, KSM ignores memory policies set by the mbind() function, and may merge pages from some memory areas to Non-Uniform Memory Access (NUMA) nodes that do not match the policies.
				</p><p>
				To work around this problem, disable KSM or set the <code class="literal">merge_across_nodes</code> parameter to <code class="literal">0</code> if using NUMA memory binding with QEMU. As a result, NUMA memory policies configured for the KVM VM will work as expected.
			</p><p>
				(BZ#1153521)
			</p><p><a id="BZ-1697310"/></p><div class="title"><strong>The <code class="literal">qede</code> driver hangs the NIC and makes it unusable</strong></div><p>
					Due to a bug, the <code class="literal">qede</code> driver for the 41000 and 45000 QLogic series NICs can cause Firmware upgrade and debug data collection operations to fail and make the NIC unusable or in hung state until reboot (PCI reset) of the host makes the NIC operational again.
				</p><p>
				This issue has been detected in all of the following scenarios:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						when upgrading Firmware of the NIC using the inbox driver
					</li><li class="listitem">
						when collecting debug data running the <code class="literal">ethtool -d ethx</code> command
					</li><li class="listitem">
						running the <code class="literal">sosreport</code> command as it includes <code class="literal">ethtool -d ethx.</code>
					</li><li class="listitem">
						when the inbox driver initiates automatic debug data collection, such as IO timeout, Mail Box Command timeout and a Hardware Attention.
					</li></ul></div><p>
				A future erratum from Red Hat will be released via Red Hat Bug Advisory (RHBA) to address this issue. To work around this problem, create a case in <a class="link" href="https://access.redhat.com/support">https://access.redhat.com/support</a> to request a supported fix for the issue until the RHBA is released.
			</p><p>
				(BZ#1697310)
			</p><p><a id="BZ-1695142"/></p><div class="title"><strong>Radix tree symbols were added to <code class="literal">kernel-abi-whitelists</code></strong></div><p>
					The following radix tree symbols have been added to the <code class="literal">kernel-abi-whitelists</code> package in Red Hat Enterprise Linux 8:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						<code class="literal">__radix_tree_insert</code>
					</li><li class="listitem">
						<code class="literal">__radix_tree_next_slot</code>
					</li><li class="listitem">
						<code class="literal">radix_tree_delete</code>
					</li><li class="listitem">
						<code class="literal">radix_tree_gang_lookup</code>
					</li><li class="listitem">
						<code class="literal">radix_tree_gang_lookup_tag</code>
					</li><li class="listitem">
						<code class="literal">radix_tree_next_chunk</code>
					</li><li class="listitem">
						<code class="literal">radix_tree_preload</code>
					</li><li class="listitem">
						<code class="literal">radix_tree_tag_set</code>
					</li></ul></div><p>
				The symbols above were not supposed to be present and will be removed from the RHEL8 whitelist.
			</p><p>
				(BZ#1695142)
			</p><p><a id="BZ-1689746"/></p><div class="title"><strong><code class="literal">podman</code> fails to checkpoint a container in RHEL 8</strong></div><p>
					The version of the Checkpoint and Restore In Userspace (CRIU) package is outdated in Red Hat Enterprise Linux 8. As a consequence, CRIU does not support container checkpoint and restore functionality and the <code class="literal">podman</code> utility fails to checkpoint containers. When running the <code class="literal">podman container checkpoint</code> command, the following error message is displayed: 'checkpointing a container requires at least CRIU 31100'
				</p><p>
				(BZ#1689746)
			</p><p><a id="BZ-1662911"/></p><div class="title"><strong><code class="literal">early-kdump</code> and standard <code class="literal">kdump</code> fail if the <code class="literal">add_dracutmodules+=earlykdump</code> option is used in <code class="literal">dracut.conf</code></strong></div><p>
					Currently, an inconsistency occurs between the kernel version being installed for <code class="literal">early-kdump</code> and the kernel version <code class="literal">initramfs</code> is generated for. As a consequence, booting with <code class="literal">early-kdump</code> enabled, <code class="literal">early-kdump</code> fails. In addition, if <code class="literal">early-kdump</code> detects that it is being included in a standard <code class="literal">kdump</code> initramfs image, it forces an exit. Therefore the standard <code class="literal">kdump</code> service also fails when trying to rebuild <code class="literal">kdump</code> initramfs if <code class="literal">early-kdump</code> is added as a default <code class="literal">dracut</code> module. As a consequence, <code class="literal">early-kdump</code> and standard <code class="literal">kdump</code> both fail. To work around this problem, do not add <code class="literal">add_dracutmodules+=earlykdump</code> or any equivalent configuration in the <code class="literal">dracut.conf</code> file. As a result, <code class="literal">early-kdump</code> is not included by <code class="literal">dracut</code> by default, which prevents the problem from occuring. However, if an <code class="literal">early-kdump</code> image is required, it has to be created manually.
				</p><p>
				(BZ#1662911)
			</p><p><a id="BZ-1659609"/></p><div class="title"><strong>Debug kernel fails to boot in crash capture environment in RHEL 8</strong></div><p>
					Due to memory-demanding nature of the debug kernel, a problem occurs when the debug kernel is in use and a kernel panic is triggered. As a consequence, the debug kernel is not able to boot as the capture kernel, and a stack trace is generated instead. To work around this problem, increase the crash kernel memory accordingly. As a result, the debug kernel successfully boots in the crash capture environment.
				</p><p>
				(BZ#1659609)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="software_management_2"/>Software management</h1></div></div></div><p><a id="BZ-1642458"/></p><div class="title"><strong>Running <code class="literal">yum list</code> under a non-root user causes <span class="strong"><strong>YUM</strong></span> crash</strong></div><p>
					When running the <code class="literal">yum list</code> command under a non-root user after the <code class="literal">libdnf</code> package has been updated, <span class="strong"><strong>YUM</strong></span> can terminate unexpectedly. If you hit this bug, run <code class="literal">yum list</code> under root to resolve the problem. As a result, subsequent attempts to run <code class="literal">yum list</code> under a non-root user no longer cause <span class="strong"><strong>YUM</strong></span> crash.
				</p><p>
				(BZ#1642458)
			</p><p><a id="BZ-1679509"/></p><div class="title"><strong><span class="strong"><strong>YUM v4</strong></span> skips unavailable repositories by default</strong></div><p>
					<span class="strong"><strong>YUM v4</strong></span> defaults to the "skip_if_unavailable=True" setting for all repositories. As a consequence, if the required repository is not available, the packages from the repository are not considered in the install, search, or update operations. Subsequently, some <code class="literal">yum</code> commands and yum-based scripts succeed with exit code 0 even if there are unavailable repositories.
				</p><p>
				Currently, there is no other workaround available than updating the <code class="literal">libdnf</code> package.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1679509">BZ#1679509</a>)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="infrastructure_services_2"/>Infrastructure services</h1></div></div></div><p><a id="BZ-1599459"/></p><div class="title"><strong>The <code class="literal">nslookup</code> and <code class="literal">host</code> utilities ignore replies from name servers with recursion not available</strong></div><p>
					If more name servers are configured and recursion is not available for a name server, the <code class="literal">nslookup</code> and <code class="literal">host</code> utilities ignore replies from such name server unless it is the one that is last configured. In case of the last configured name server, answer is accepted even without the <code class="literal">recursion available</code> flag. However, if the last configured name server is not responding or unreachable, name resolution fails.
				</p><p>
				To work around the problem:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						Ensure that configured name servers always reply with the <code class="literal">recursion available</code> flag set.
					</li><li class="listitem">
						Allow recursion for all internal clients.
					</li></ul></div><p>
				To troubleshoot the problem, you can also use the <code class="literal">dig</code> utility to detect whether recursion is available or not.
			</p><p>
				(BZ#1599459)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="shells_and_command_line_tools_3"/>Shells and command-line tools</h1></div></div></div><p><a id="BZ-1584510"/></p><div class="title"><strong><code class="literal">Python</code> binding of the <code class="literal">net-snmp</code> package is unavailable</strong></div><p>
					The <code class="literal">Net-SNMP</code> suite of tools does not provide binding for <code class="literal">Python 3</code>, which is the default <code class="literal">Python</code> implementation in RHEL 8. Consequently, <code class="literal">python-net-snmp</code>, <code class="literal">python2-net-snmp</code>, or <code class="literal">python3-net-snmp</code> packages are unavailable in RHEL 8.
				</p><p>
				(BZ#1584510)
			</p><p><a id="BZ-1658691"/></p><div class="title"><strong><code class="literal">systemd</code> in debug mode produces unnecessary log messages</strong></div><p>
					The <code class="literal">systemd</code> system and service manager in debug mode produces unnecessary log messages that start with:
				</p><pre class="literallayout">"Failed to add rule for system call ..."</pre><p>
				List the messages by running:
			</p><pre class="literallayout">journalctl -b _PID=1</pre><p>
				These debug messages are harmless, and you can safely ignore them.
			</p><p>
				Currently, there is no workaround available.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1658691">BZ#1658691</a>)
			</p><p><a id="BZ-1503922"/></p><div class="title"><strong><code class="literal">ksh</code> with the <code class="literal">KEYBD</code> trap mishandles multibyte characters</strong></div><p>
					The Korn Shell (KSH) is unable to correctly handle multibyte characters when the <code class="literal">KEYBD</code> trap is enabled. Consequently, when the user enters, for example, Japanese characters, <code class="literal">ksh</code> displays an incorrect string. To work around this problem, disable the <code class="literal">KEYBD</code> trap in the <code class="literal">/etc/kshrc</code> file by commenting out the following line:
				</p><pre class="screen">trap keybd_trap KEYBD</pre><p>
				For more details, see a related <a class="link" href="https://access.redhat.com/solutions/3219671">Knowledgebase solution</a>.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1503922">BZ#1503922</a>)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="dynamic_programming_languages_web_and_database_servers_2"/>Dynamic programming languages, web and database servers</h1></div></div></div><p><a id="BZ-1566048"/></p><div class="title"><strong>Database servers are not installable in parallel</strong></div><p>
					The <code class="literal">mariadb</code> and <code class="literal">mysql</code> modules cannot be installed in parallel in RHEL 8.0 due to conflicting RPM packages.
				</p><p>
				By design, it is impossible to install more than one version (stream) of the same module in parallel. For example, you need to choose only one of the available streams from the <code class="literal">postgresql</code> module, either <code class="literal">10</code> (default) or <code class="literal">9.6</code>. Parallel installation of components is possible in Red Hat Software Collections for RHEL 6 and RHEL 7. In RHEL 8, different versions of database servers can be used in containers.
			</p><p>
				(BZ#1566048)
			</p><p><a id="BZ-1633224"/></p><div class="title"><strong>Problems in <code class="literal">mod_cgid</code> logging</strong></div><p>
					If the <code class="literal">mod_cgid</code> Apache httpd module is used under a threaded multi-processing module (MPM), which is the default situation in RHEL 8, the following logging problems occur:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						The <code class="literal">stderr</code> output of the CGI script is not prefixed with standard timestamp information.
					</li><li class="listitem">
						The <code class="literal">stderr</code> output of the CGI script is not correctly redirected to a log file specific to the <code class="literal">VirtualHost</code>, if configured.
					</li></ul></div><p>
				(BZ#1633224)
			</p><p><a id="BZ-1632600"/></p><div class="title"><strong>The <code class="literal">IO::Socket::SSL</code> Perl module does not support TLS 1.3</strong></div><p>
					New features of the TLS 1.3 protocol, such as session resumption or post-handshake authentication, were implemented in the RHEL 8 <code class="literal">OpenSSL</code> library but not in the <code class="literal">Net::SSLeay</code> Perl module, and thus are unavailable in the <code class="literal">IO::Socket::SSL</code> Perl module. Consequently, client certificate authentication might fail and reestablishing sessions might be slower than with the TLS 1.2 protocol.
				</p><p>
				To work around this problem, disable usage of TLS 1.3 by setting the <code class="literal">SSL_version</code> option to the <code class="literal">!TLSv1_3</code> value when creating an <code class="literal">IO::Socket::SSL</code> object.
			</p><p>
				(BZ#1632600)
			</p><p><a id="BZ-1641744"/></p><div class="title"><strong>Generated Scala documentation is unreadable</strong></div><p>
					When generating documentation using the <code class="literal">scaladoc</code> command, the resulting HTML page is unusable due to missing JavaScript resources.
				</p><p>
				(BZ#1641744)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="desktop_3"/>Desktop</h1></div></div></div><p><a id="BZ-1641763"/></p><div class="title"><strong><code class="literal">qxl</code> does not work on VMs based on Wayland</strong></div><p>
					The <code class="literal">qxl</code> driver is not able to provide kernel mode setting features on certain hypervisors. Consequently, the graphics based on the Wayland protocol are not available to virtual machines (VMs) that use <code class="literal">qxl</code>, and the Wayland-based login screen does not start.
				</p><p>
				To work around the problem, use either :
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						The <span class="strong"><strong>Xorg</strong></span> display server instead of <span class="strong"><strong>GNOME Shell on Wayland</strong></span> on VMs based on QuarkXpress Element Library (QXL) graphics.
					</li></ul></div><p>
				Or
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						The <code class="literal">virtio</code> driver instead of the <code class="literal">qxl</code> driver for your VMs.
					</li></ul></div><p>
				(BZ#1641763)
			</p><p><a id="BZ-1678627"/></p><div class="title"><strong>The console prompt is not displayed when running <code class="literal">systemctl isolate multi-user.target</code></strong></div><p>
					When running the <code class="literal">systemctl isolate multi-user.target</code> command from GNOME Terminal in a GNOME Desktop session, only a cursor is displayed, and not the console prompt. To work around the problem, press the <code class="literal">Ctrl+Alt+F2</code> keys. As a result, the console prompt appears.
				</p><p>
				The behavior applies both to <span class="strong"><strong>GNOME Shell on Wayland</strong></span> and <span class="strong"><strong>X.Org</strong></span> display server.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1678627">BZ#1678627</a>)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="graphics_infrastructures_3"/>Graphics infrastructures</h1></div></div></div><p><a id="BZ-1655413"/></p><div class="title"><strong>Desktop running on <span class="strong"><strong>X.Org</strong></span> hangs when changing to low screen resolutions</strong></div><p>
					When using the GNOME desktop with the <span class="strong"><strong>X.Org</strong></span> display server, the desktop becomes unresponsive if you attempt to change the screen resolution to low values. To work around the problem, do not set the screen resolution to a value lower than 800 × 600 pixels.
				</p><p>
				(BZ#1655413)
			</p><p><a id="BZ-1694705"/></p><div class="title"><strong><code class="literal">radeon</code> fails to reset hardware correctly</strong></div><p>
					The <code class="literal">radeon</code> kernel driver currently does not reset hardware in the kexec context correctly. Instead, <code class="literal">radeon</code> falls over, which causes the rest of the <span class="strong"><strong>kdump</strong></span> service to fail.
				</p><p>
				To work around this problem, blacklist <code class="literal">radeon</code> in <span class="strong"><strong>kdump</strong></span> by adding the following line to the <code class="literal">/etc/kdump.conf</code> file:
			</p><pre class="screen">dracut_args --omit-drivers "radeon"
force_rebuild 1</pre><p>
				Restart the machine and <span class="strong"><strong>kdump</strong></span>. After starting <span class="strong"><strong>kdump</strong></span>, the <code class="literal">force_rebuild 1</code> line may be removed from the configuration file.
			</p><p>
				Note that in this scenario, no graphics will be available during <span class="strong"><strong>kdump</strong></span>, but <span class="strong"><strong>kdump</strong></span> will work successfully.
			</p><p>
				(BZ#1694705)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="hardware_enablement_3"/>Hardware enablement</h1></div></div></div><p><a id="BZ-1645433"/></p><div class="title"><strong>Backup slave MII status does not work when using the ARP link monitor</strong></div><p>
					By default, devices managed by the i40e driver, do source pruning, which drops packets that have the source Media Access Control (MAC) address that matches one of the receive filters. As a consequence, backup slave Media Independent Interface (MII) status does not work when using the Address Resolution Protocol (ARP) monitoring in channel bonding. To work around this problem, disable source pruning by the following command:
				</p><pre class="literallayout"># ethtool --set-priv-flags &lt;ethX&gt; disable-source-pruning on</pre><p>
				As a result, the backup slave MII status will work as expected.
			</p><p>
				(BZ#1645433)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="identity_management_4"/>Identity Management</h1></div></div></div><p><a id="BZ-1448094"/></p><div class="title"><strong>The KCM credential cache is not suitable for a large number of credentials in a single credential cache</strong></div><p>
					The Kerberos Credential Manager (KCM) can handle ccache sizes of up to 64 kB. If it contains too many credentials, Kerberos operations, such as <span class="strong"><strong>kinit</strong></span>, fail due to a hardcoded limit on the buffer used to transfer data between the <span class="strong"><strong>sssd-kcm</strong></span> component and the underlying database.
				</p><p>
				To work around this problem, add the <code class="literal">ccache_storage = memory</code> option in the <span class="strong"><strong>kcm</strong></span> section of the <code class="literal">/etc/sssd/sssd.conf</code> file. This instructs the <span class="strong"><strong>kcm</strong></span> responder to only store the credential caches in-memory, not persistently. If you do this, restarting the system or <span class="strong"><strong>sssd-kcm</strong></span> clears the credential caches.
			</p><p>
				(BZ#1448094)
			</p><p><a id="BZ-1657295"/></p><div class="title"><strong>Changing <code class="literal">/etc/nsswitch.conf</code> requires a manual system reboot</strong></div><p>
					Any change to the <code class="literal">/etc/nsswitch.conf</code> file, for example running the <code class="literal">authselect select profile_id</code> command, requires a system reboot so that all relevant processes use the updated version of the <code class="literal">/etc/nsswitch.conf</code> file. If a system reboot is not possible, restart the service that joins your system to Active Directory, which is the <code class="literal">System Security Services Daemon</code> (SSSD) or <code class="literal">winbind</code>.
				</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1657295">BZ#1657295</a>)
			</p><p><a id="BZ-1382750"/></p><div class="title"><strong>Conflicting timeout values prevent SSSD from connecting to servers</strong></div><p>
					Some of the default timeout values related to the failover operations used by the System Security Services Daemon (SSSD) are conflicting. Consequently, the timeout value reserved for SSSD to talk to a single server prevents SSSD from trying other servers before the connecting operation as a whole time out. To work around the problem, set the value of the <code class="literal">ldap_opt_timeout</code> timeout parameter higher than the value of the <code class="literal">dns_resolver_timeout</code> parameter, and set the value of the <code class="literal">dns_resolver_timeout</code> parameter higher than the value of the <code class="literal">dns_resolver_op_timeout</code> parameter.
				</p><p>
				(BZ#1382750)
			</p><p><a id="BZ-1446101"/></p><div class="title"><strong>SSSD can look up only unique certificates in ID overrides</strong></div><p>
					When multiple ID overrides contain the same certificate, the System Security Services Daemon (SSSD) is unable to resolve queries for the users that match the certificate. An attempt to look up these users does not return any user. Note that looking up users by using their user name or UID works as expected.
				</p><p>
				(BZ#1446101)
			</p><p><a id="BZ-1447945"/></p><div class="title"><strong>SSSD does not correctly handle multiple certificate matching rules with the same priority</strong></div><p>
					If a given certificate matches multiple certificate matching rules with the same priority, the System Security Services Daemon (SSSD) uses only one of the rules. As a workaround, use a single certificate matching rule whose LDAP filter consists of the filters of the individual rules concatenated with the <code class="literal">|</code> (or) operator. For examples of certificate matching rules, see the sss-certamp(5) man page.
				</p><p>
				(BZ#1447945)
			</p><p><a id="BZ-1652562"/></p><div class="title"><strong>SSSD returns incorrect LDAP group membership for local users</strong></div><p>
					If the System Security Services Daemon (SSSD) serves users from the local files, the files provider does not include group memberships from other domains. As a consequence, if a local user is a member of an LDAP group, the <code class="literal">id local_user</code> command does not return the user’s LDAP group membership. To work around the problem, either revert the order of the databases where the system is looking up the group membership of users in the <code class="literal">/etc/nsswitch.conf</code> file, replacing <code class="literal">sss files</code> with <code class="literal">files sss</code>, or disable the implicit <code class="literal">files</code> domain by adding
				</p><pre class="literallayout">enable_files_domain=False</pre><p>
				to the <code class="literal">[sssd]</code> section in the <code class="literal">/etc/sssd/sssd.conf</code> file.
			</p><p>
				As a result, <code class="literal">id local_user</code> returns correct LDAP group membership for local users.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1652562">BZ#1652562</a>)
			</p><p><a id="BZ-1659457"/></p><div class="title"><strong>Sudo rules might not work with <code class="literal">id_provider=ad</code> if sudo rules reference group names</strong></div><p>
					System Security Services Daemon (SSSD) does not resolve Active Directory group names during the <code class="literal">initgroups</code> operation because of an optimization of communication between AD and SSSD by using a cache. The cache entry contains only a Security Identifiers (SID) and not group names until the group is requested by name or ID. Therefore, sudo rules do not match the AD group unless the groups are fully resolved prior to running sudo.
				</p><p>
				To work around this problem, you need to disable the optimization: Open the <code class="literal">/etc/sssd/sssd.conf</code> file and add the <code class="literal">ldap_use_tokengroups = false</code> parameter in the <code class="literal">[domain/example.com]</code> section.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1659457">BZ#1659457</a>)
			</p><p><a id="BZ-1669407"/></p><div class="title"><strong>Default PAM settings for <code class="literal">systemd-user</code> have changed in RHEL 8 which may influence SSSD behavior</strong></div><p>
					The Pluggable authentication modules (PAM) stack has changed in Red Hat Enterprise Linux 8. For example, the <code class="literal">systemd</code> user session now starts a PAM conversation using the <code class="literal">systemd-user</code> PAM service. This service now recursively includes the <code class="literal">system-auth</code> PAM service, which may include the <code class="literal">pam_sss.so</code> interface. This means that the SSSD access control is always called.
				</p><p>
				Be aware of the change when designing access control rules for RHEL 8 systems. For example, you can add the <code class="literal">systemd-user</code> service to the allowed services list.
			</p><p>
				Please note that for some access control mechanisms, such as IPA HBAC or AD GPOs, the <code class="literal">systemd-user</code> service is has been added to the allowed services list by default and you do not need to take any action.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1669407">BZ#1669407</a>)
			</p><p><a id="BZ-1673296"/></p><div class="title"><strong>IdM server does not work in FIPS</strong></div><p>
					Due to an incomplete implementation of the SSL connector for Tomcat, an Identity Management (IdM) server with a certificate server installed does not work on machines with the FIPS mode enabled.
				</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1673296">BZ#1673296</a>)
			</p><p><a id="BZ-1657665"/></p><div class="title"><strong>Samba denies access when using the <code class="literal">sss</code> ID mapping plug-in</strong></div><p>
					To use Samba as a file server on a RHEL host joined to an Active Directory (AD) domain, the Samba Winbind service must be running even if SSSD is used to manage user and groups from AD. If you join the domain using the <code class="literal">realm join --client-software=sssd</code> command or without specifying the <code class="literal">--client-software</code> parameter in this command, <code class="literal">realm</code> creates only the <code class="literal">/etc/sssd/sssd.conf</code> file. When you run Samba on the domain member with this configuration and add a configuration that uses the <code class="literal">sss</code> ID mapping back end to the <code class="literal">/etc/samba/smb.conf</code> file to share directories, changes in the ID mapping back end can cause errors. Consequently, Samba denies access to files in certain cases, even if the user or group exists and it is known by SSSD.
				</p><p>
				If you plan to upgrade from a previous RHEL version and the <code class="literal">ldap_id_mapping</code> parameter in the <code class="literal">/etc/sssd/sssd.conf</code> file is set to <code class="literal">True</code>, which is the default, no workaround is available. In this case, do not upgrade the host to RHEL 8 until the problem has been fixed.
			</p><p>
				Possible workarounds in other scenarios:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						For new installations, join the domain using the <code class="literal">realm join --client-software=winbind</code> command. This configures the system to use Winbind instead of SSSD for all user and group lookups. In this case, Samba uses the <code class="literal">rid</code> or <code class="literal">ad</code> ID mapping plug-in in <code class="literal">/etc/samba/smb.conf</code> depending on whether you set the <code class="literal">--automatic-id-mapping</code> option to <code class="literal">yes</code> (default) or <code class="literal">no</code>. If you plan to use SSSD in future or on other systems, using <code class="literal">--automatic-id-mapping=no</code> allows an easier migration but requires that you store POSIX UIDs and GIDs in AD for all users and groups.
					</li><li class="listitem"><p class="simpara">
						When upgrading from a previous RHEL version, and if the <code class="literal">ldap_id_mapping</code> parameter in the <code class="literal">/etc/sssd/sssd.conf</code> file is set to <code class="literal">False</code> and the system uses the <code class="literal">uidNumber</code> and <code class="literal">gidNumber</code> attributes from AD for ID mapping:
					</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem">
								Change the <code class="literal">idmap config &lt;domain&gt; : backend = sss</code> entry in the <code class="literal">/etc/samba/smb.conf</code> file to <code class="literal">idmap config &lt;domain&gt; : backend = ad</code>
							</li><li class="listitem">
								Use the <code class="literal">systemctl status winbind</code> command to restart the Winbind.
							</li></ol></div></li></ul></div><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1657665">BZ#1657665</a>)
			</p><p><a id="BZ-1652269"/></p><div class="title"><strong>The <code class="literal">nuxwdog</code> service fails in HSM environments and requires to install the <code class="literal">keyutils</code> package in non-HSM environments</strong></div><p>
					The <code class="literal">nuxwdog</code> watchdog service has been integrated into Certificate System. As a consequence, <code class="literal">nuxwdog</code> is no longer provided as a separate package. To use the watchdog service, install the <code class="literal">pki-server</code> package.
				</p><p>
				Note that the <code class="literal">nuxwdog</code> service has following known issues:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						The <code class="literal">nuxwdog</code> service does not work if you use a hardware storage module (HSM). For this issue, no workaround is available.
					</li><li class="listitem">
						In a non-HSM environment, Red Hat Enterprise Linux 8.0 does not automatically install the <code class="literal">keyutils</code> package as a dependency. To install the package manually, use the <code class="literal">dnf install keyutils</code> command.
					</li></ul></div><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1652269">BZ#1652269</a>)
			</p><p><a id="BZ-1651577"/></p><div class="title"><strong>Adding ID overrides of AD users works only in the IdM CLI</strong></div><p>
					Currently, adding ID overrides of Active Directory (AD) users to Identity Management (IdM) groups for the purpose of granting access to management roles fails in the IdM Web UI. To work around the problem, use the IdM command-line interface (CLI) instead.
				</p><p>
				Note that if you installed the <code class="literal">ipa-idoverride-memberof-plugin</code> package on the IdM server after previously performing certain operations using the <code class="literal">ipa</code> utility, Red Hat recommends cleaning up the <code class="literal">ipa</code> utility’s cache to force it to refresh its view about the IdM server metadata.
			</p><p>
				To do so, remove the content of the <code class="literal">~/.cache/ipa</code> directory for the user under which the <code class="literal">ipa</code> utility is executed. For example, for root:
			</p><pre class="literallayout"># <span class="strong"><strong>rm -r /root/.cache/ipa</strong></span></pre><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1651577">BZ#1651577</a>)
			</p><p><a id="BZ-1665051"/></p><div class="title"><strong>No information about required DNS records displayed when enabling support for AD trust in IdM</strong></div><p>
					When enabling support for Active Directory (AD) trust in Red Hat Enterprise Linux Identity Management (IdM) installation with external DNS management, no information about required DNS records is displayed. Forest trust to AD is not successful until the required DNS records are added. To work around this problem, run the 'ipa dns-update-system-records --dry-run' command to obtain a list of all DNS records required by IdM. When external DNS for IdM domain defines the required DNS records, establishing forest trust to AD is possible.
				</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1665051">BZ#1665051</a>)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="compilers_and_development_tools_3"/>Compilers and development tools</h1></div></div></div><p><a id="BZ-1169184"/></p><div class="title"><strong>Synthetic functions generated by GCC confuse SystemTap</strong></div><p>
					GCC optimization can generate synthetic functions for partially inlined copies of other functions. Tools such as SystemTap and GDB can not distinguish these synthetic functions from real functions. As a consequence, SystemTap can place probes on both synthetic and real function entry points, and thus register multiple probe hits for a single real function call.
				</p><p>
				To work around this problem, SystemTap scripts must be adapted with measures such as detecting recursion and suppressing probes related to inlined partial functions. For example, a script
			</p><pre class="screen">probe kernel.function("can_nice").call { }</pre><p>
				can try to avoid the described problem as follows:
			</p><pre class="screen">global in_can_nice%

probe kernel.function("can_nice").call {
  in_can_nice[tid()] ++;
  if (in_can_nice[tid()] &gt; 1) { next }
  /* code for real probe handler */
}

probe kernel.function("can_nice").return {
  in_can_nice[tid()] --;
}</pre><p>
				Note that this example script does not take into account all possible scenarios, such as missed kprobes or kretprobes, or genuine intended recursion.
			</p><p>
				(BZ#1169184)
			</p><p><a id="BZ-1618748"/></p><div class="title"><strong>The <code class="literal">ltrace</code> tool does not report function calls</strong></div><p>
					Because of improvements to binary hardening applied to all RHEL components, the <code class="literal">ltrace</code> tool can no longer detect function calls in binary files coming from RHEL components. As a consequence, <code class="literal">ltrace</code> output is empty because it does not report any detected calls when used on such binary files. There is no workaround currently available.
				</p><p>
				As a note, <code class="literal">ltrace</code> can correctly report calls in custom binary files built without the respective hardening flags.
			</p><p>
				(BZ#1618748, BZ#1655368)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="file_systems_and_storage_5"/>File systems and storage</h1></div></div></div><p><a id="BZ-1626629"/></p><div class="title"><strong>Unable to discover an iSCSI target using the <code class="literal">iscsiuio</code> package</strong></div><p>
					Red Hat Enterprise Linux 8 does not allow concurrent access to PCI register areas. As a consequence, a <code class="literal">could not set host net params (err 29)</code> error was set and the connection to the discovery portal failed. To work around this problem, set the kernel parameter <code class="literal">iomem=relaxed</code> in the kernel command line for the iSCSI offload. This specifically involves any offload using the <code class="literal">bnx2i</code> driver. As a result, connection to the discovery portal is now successful and <code class="literal">iscsiuio</code> package now works correctly.
				</p><p>
				(BZ#1626629)
			</p><p><a id="BZ-1696492"/></p><div class="title"><strong>VDO volumes lose deduplication advice after moving to a different-endian platform</strong></div><p>
					Virtual Data Optimizer (VDO) writes the Universal Deduplication Service (UDS) index header in the endian format native to your platform. VDO considers the UDS index corrupt and overwrites it with a new, blank index if you move your VDO volume to a platform that uses a different endian.
				</p><p>
				As a consequence, any deduplication advice stored in the UDS index prior to being overwritten is lost. VDO is then unable to deduplicate newly written data against the data that was stored before you moved the volume, leading to lower space savings.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1696492">BZ#1696492</a>)
			</p><p><a id="BZ-1620330"/></p><div class="title"><strong>The XFS DAX mount option is incompatible with shared copy-on-write data extents</strong></div><p>
					An XFS file system formatted with the shared copy-on-write data extents feature is not compatible with the <code class="literal">-o dax</code> mount option. As a consequence, mounting such a file system with <code class="literal">-o dax</code> fails.
				</p><p>
				To work around the problem, format the file system with the <code class="literal">reflink=0</code> metadata option to disable shared copy-on-write data extents:
			</p><pre class="screen"># mkfs.xfs -m reflink=0 <span class="emphasis"><em>block-device</em></span></pre><p>
				As a result, mounting the file system with <code class="literal">-o dax</code> is successful.
			</p><p>
				For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/using-nvdimm-persistent-memory-storage_managing-storage-devices#creating-a-file-system-dax-namespace-on-an-nvdimm_using-nvdimm-persistent-memory-storage">Creating a file system DAX namespace on an NVDIMM</a>.
			</p><p>
				(BZ#1620330)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="networking_4"/>Networking</h1></div></div></div><p><a id="BZ-1593711"/></p><div class="title"><strong><code class="literal">nftables</code> does not support multi-dimensional IP set types</strong></div><p>
					The <code class="literal">nftables</code> packet-filtering framework does not support set types with concatenations and intervals. Consequently, you cannot use multi-dimensional IP set types, such as <code class="literal">hash:net,port</code>, with <code class="literal">nftables</code>.
				</p><p>
				To work around this problem, use the <code class="literal">iptables</code> framework with the <code class="literal">ipset</code> tool if you require multi-dimensional IP set types.
			</p><p>
				(BZ#1593711)
			</p><p><a id="BZ-1658734"/></p><div class="title"><strong>The <code class="literal">TRACE</code> target in the <code class="literal">iptables-extensions(8)</code> man page does not refer to the <code class="literal">nf_tables</code> variant</strong></div><p>
					The description of the <code class="literal">TRACE</code> target in the <code class="literal">iptables-extensions(8)</code> man page refers only to the <code class="literal">compat</code> variant, but Red Hat Enterprise Linux (RHEL) 8.0 uses the <code class="literal">nf_tables</code> variant. The <code class="literal">nftables</code>-based <code class="literal">iptables</code> utility in RHEL uses the <code class="literal">meta nftrace</code> expression internally. Therefore, the kernel does not print <code class="literal">TRACE</code> events in the kernel log but sends them to the user space instead. However, the man page does not reference the <code class="literal">xtables-monitor</code> command-line utility to display these events.
				</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1658734">BZ#1658734</a>)
			</p><p><a id="BZ-1649790"/></p><div class="title"><strong>The <code class="literal">ebtables</code> command does not support <code class="literal">broute</code> table</strong></div><p>
					The <code class="literal">nftables</code>-based <code class="literal">ebtables</code> command in Red Hat Enterprise Linux 8.0 does not support the <code class="literal">broute</code> table. Consequently, users can not use this feature.
				</p><p>
				(BZ#1649790)
			</p><p><a id="BZ-1649647"/></p><div class="title"><strong>IPsec network traffic fails during IPsec offloading when GRO is disabled</strong></div><p>
					IPsec offloading is not expected to work when Generic Receive Offload (GRO) is disabled on the device. If IPsec offloading is configured on a network interface and GRO is disabled on that device, IPsec network traffic fails.
				</p><p>
				To work around this problem, keep GRO enabled on the device.
			</p><p>
				(BZ#1649647)
			</p><p><a id="BZ-1571655"/></p><div class="title"><strong><span class="strong"><strong>NetworkManager</strong></span> now uses the <code class="literal">internal</code> DHCP plug-in by default</strong></div><p>
					<span class="strong"><strong>NetworkManager</strong></span> supports the <code class="literal">internal</code> and <code class="literal">dhclient</code> DHCP plug-ins. By default, <span class="strong"><strong>NetworkManager</strong></span> in Red Hat Enterprise Linux (RHEL) 7 uses the <code class="literal">dhclient</code> and RHEL 8 the <code class="literal">internal</code> plug-in. In certain situations, the plug-ins behave differently. For example, <code class="literal">dhclient</code> can use additional settings specified in the <code class="literal">/etc/dhcp/</code> directory.
				</p><p>
				If you upgrade from RHEL 7 to RHEL 8 and <span class="strong"><strong>NetworkManager</strong></span> behaves different, add the following setting to the <code class="literal">[main]</code> section in the <code class="literal">/etc/NetworkManager/NetworkManager.conf</code> file to use the <code class="literal">dhclient</code> plug-in:
			</p><pre class="screen">[main]
dhcp=dhclient</pre><p>
				(BZ#1571655)
			</p><p><a id="BZ-1697326"/></p><div class="title"><strong>Advanced options of <code class="literal">IPsec based VPN</code> cannot be changed using <code class="literal">gnome-control-center</code></strong></div><p>
					When configuring an <code class="literal">IPsec based VPN</code> connection using the <code class="literal">gnome-control-center</code> application, the <code class="literal">Advanced</code> dialog will only display the configuration, but will not allow doing any change. As a consequence, users cannot change any advanced IPsec options. To work around this problem, use the <code class="literal">nm-connection-editor</code> or <code class="literal">nmcli</code> tools to perform configuration of the advanced properties.
				</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1697326">BZ#1697326</a>)
			</p><p><a id="BZ-1663556"/></p><div class="title"><strong>The /etc/hosts.allow and /etc/hosts.deny files contain inaccurate information</strong></div><p>
					The tcp_wrappers package is removed in Red Hat Enterprise Linux (RHEL) 8, but not its files, /etc/hosts.allow and /etc/hosts.deny. As a consequence, these files contain outdated information, which is not applicable for RHEL 8.
				</p><p>
				To work around this problem, use firewall rules for filtering access to the services. For filtering based on usernames and hostnames, use the application-specific configuration.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1663556">BZ#1663556</a>)
			</p><p><a id="BZ-1597671"/></p><div class="title"><strong>IP defragmentation cannot be sustainable under network traffic overload</strong></div><p>
					In Red Hat Enterprise Linux 8, the garbage collection kernel thread has been removed and IP fragments expire only on timeout. As a result, CPU usage under Denial of Service (DoS) is much lower, and the maximum sustainable fragments drop rate is limited by the amount of memory configured for the IP reassembly unit. With the default settings workloads requiring fragmented traffic in presence of packet drops, packet reorder or many concurrent fragmented flows may incur in relevant performance regression.
				</p><p>
				In this case, users can use the appropriate tuning of the IP fragmentation cache in the <code class="literal">/proc/sys/net/ipv4</code> directory setting the <code class="literal">ipfrag_high_thresh</code> variable to limit the amount of memory and the <code class="literal">ipfrag_time</code> variable to keep per seconds an IP fragment in memory. For example,
			</p><p>
				echo 419430400 &gt; /proc/sys/net/ipv4/ipfrag_high_thresh echo 1 &gt; /proc/sys/net/ipv4/ipfrag_time
			</p><p>
				The above applies to IPv4 traffic. For IPv6 the relevant tunables are: <code class="literal">ip6frag_high_thresh</code> and <code class="literal">ip6frag_time</code> in the <code class="literal">/proc/sys/net/ipv6/</code> directory.
			</p><p>
				Note that any workload relying on high-speed fragmented traffic can cause stability and performance issues, especially with packet drops, and such kind of deployments are highly discouraged in production.
			</p><p>
				(BZ#1597671)
			</p><p><a id="BZ-1701968"/></p><div class="title"><strong>Network interface name changes in RHEL 8</strong></div><p>
					In Red Hat Enterprise Linux 8, the same consistent network device naming scheme is used by default as in RHEL 7. However, some kernel drivers, such as <code class="literal">e1000e</code>, <code class="literal">nfp</code>, <code class="literal">qede</code>, <code class="literal">sfc</code>, <code class="literal">tg3</code> and <code class="literal">bnxt_en</code> changed their consistent name on a fresh installation of RHEL 8. However, the names are preserved on upgrade from RHEL 7.
				</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1701968">BZ#1701968</a>)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="security_4"/>Security</h1></div></div></div><p><a id="BZ-1646563"/></p><div class="title"><strong><code class="literal">libssh</code> does not comply with the system-wide crypto policy</strong></div><p>
					The <code class="literal">libssh</code> library does not follow system-wide cryptographic policy settings. As a consequence, the set of supported algorithms is not changed when the administrator changes the crypto policies level using the <code class="literal">update-crypto-policies</code> command.
				</p><p>
				To work around this problem, the set of advertised algorithms needs to be set individually by every application that uses <code class="literal">libssh</code>. As a result, when the system is set to the LEGACY or FUTURE policy level, applications that use <code class="literal">libssh</code> behave inconsistently when compared to <code class="literal">OpenSSH</code>.
			</p><p>
				(BZ#1646563)
			</p><p><a id="BZ-1679512"/></p><div class="title"><strong>Certain <code class="literal">rsyslog</code> priority strings do not work correctly</strong></div><p>
					Support for the <span class="strong"><strong>GnuTLS</strong></span> priority string for <code class="literal">imtcp</code> that allows fine-grained control over encryption is not complete. Consequently, the following priority strings do not work properly in <code class="literal">rsyslog</code>:
				</p><pre class="screen">NONE:+VERS-ALL:-VERS-TLS1.3:+MAC-ALL:+DHE-RSA:+AES-256-GCM:+SIGN-RSA-SHA384:+COMP-ALL:+GROUP-ALL</pre><p>
				To work around this problem, use only correctly working priority strings:
			</p><pre class="screen">NONE:+VERS-ALL:-VERS-TLS1.3:+MAC-ALL:+ECDHE-RSA:+AES-128-CBC:+SIGN-RSA-SHA1:+COMP-ALL:+GROUP-ALL</pre><p>
				As a result, current configurations must be limited to the strings that work correctly.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1679512">BZ#1679512</a>)
			</p><p><a id="JIRA-RHELPLAN-10431"/></p><div class="title"><strong>Negative effects of the default logging setup on performance</strong></div><p>
					The default logging environment setup might consume 4 GB of memory or even more and adjustments of rate-limit values are complex when <code class="literal">systemd-journald</code> is running with <code class="literal">rsyslog</code>.
				</p><p>
				See the <a class="link" href="https://access.redhat.com/articles/4095141">Negative effects of the RHEL default logging setup on performance and their mitigations</a> Knowledgebase article for more information.
			</p><p>
				(JIRA:RHELPLAN-10431)
			</p><p><a id="BZ-1646197"/></p><div class="title"><strong><span class="strong"><strong>OpenSCAP</strong></span> <code class="literal">rpmverifypackage</code> does not work correctly</strong></div><p>
					The <code class="literal">chdir</code> and <code class="literal">chroot</code> system calls are called twice by the <code class="literal">rpmverifypackage</code> probe. Consequently, an error occurs when the probe is utilized during an <span class="strong"><strong>OpenSCAP</strong></span> scan with custom Open Vulnerability and Assessment Language (OVAL) content.
				</p><p>
				To work around this problem, do not use the <code class="literal">rpmverifypackage_test</code> OVAL test in your content or use only the content from the <code class="literal">scap-security-guide</code> package where <code class="literal">rpmverifypackage_test</code> is not used.
			</p><p>
				(BZ#1646197)
			</p><p><a id="BZ-1640715"/></p><div class="title"><strong><span class="strong"><strong>SCAP Workbench</strong></span> fails to generate results-based remediations from tailored profiles</strong></div><p>
					The following error occurs when trying to generate results-based remediation roles from a customized profile using the <span class="strong"><strong>SCAP Workbench</strong></span> tool:
				</p><pre class="screen">Error generating remediation role .../remediation.sh: Exit code of oscap was 1: [output truncated]</pre><p>
				To work around this problem, use the <code class="literal">oscap</code> command with the <code class="literal">--tailoring-file</code> option.
			</p><p>
				(BZ#1640715)
			</p><p><a id="BZ-1665082"/></p><div class="title"><strong>Kickstart uses <code class="literal">org_fedora_oscap</code> instead of <code class="literal">com_redhat_oscap</code> in RHEL 8</strong></div><p>
					The Kickstart references the Open Security Content Automation Protocol (OSCAP) Anaconda add-on as <code class="literal">org_fedora_oscap</code> instead of <code class="literal">com_redhat_oscap</code> which might cause confusion. That is done to preserve backward compatibility with Red Hat Enterprise Linux 7.
				</p><p>
				(BZ#1665082)
			</p><p><a id="BZ-1636431"/></p><div class="title"><strong><span class="strong"><strong>OpenSCAP</strong></span> <code class="literal">rpmverifyfile</code> does not work</strong></div><p>
					The <span class="strong"><strong>OpenSCAP</strong></span> scanner does not correctly change the current working directory in offline mode, and the <code class="literal">fchdir</code> function is not called with the correct arguments in the <span class="strong"><strong>OpenSCAP</strong></span> <code class="literal">rpmverifyfile</code> probe. Consequently, scanning arbitrary file systems using the <code class="literal">oscap-chroot</code> command fails if <code class="literal">rpmverifyfile_test</code> is used in an SCAP content. As a result, <code class="literal">oscap-chroot</code> aborts in the described scenario.
				</p><p>
				(BZ#1636431)
			</p><p><a id="BZ-1618489"/></p><div class="title"><strong><code class="literal">OpenSCAP</code> does not provide offline scanning of virtual machines and containers</strong></div><p>
					Refactoring of <code class="literal">OpenSCAP</code> codebase caused certain RPM probes to fail to scan VM and containers file systems in offline mode. For that reason, the following tools were removed from the <code class="literal">openscap-utils</code> package: <code class="literal">oscap-vm</code> and <code class="literal">oscap-chroot</code>. Also, the <code class="literal">openscap-containers</code> package was completely removed.
				</p><p>
				(BZ#1618489)
			</p><p><a id="BZ-1642373"/></p><div class="title"><strong>A utility for security and compliance scanning of containers is not available</strong></div><p>
					In Red Hat Enterprise Linux 7, the <code class="literal">oscap-docker</code> utility can be used for scanning of Docker containers based on Atomic technologies. In Red Hat Enterprise Linux 8, the Docker- and Atomic-related <span class="strong"><strong>OpenSCAP</strong></span> commands are not available. As a result, <code class="literal">oscap-docker</code> or an equivalent utility for security and compliance scanning of containers is not available in RHEL 8 at the moment.
				</p><p>
				(BZ#1642373)
			</p><p><a id="BZ-1681178"/></p><div class="title"><strong>The <code class="literal">OpenSSL TLS</code> library does not detect if the <code class="literal">PKCS#11</code> token supports creation of <code class="literal">raw RSA</code> or <code class="literal">RSA-PSS</code> signatures</strong></div><p>
					The <code class="literal">TLS-1.3</code> protocol requires the support for <code class="literal">RSA-PSS</code> signature. If the <code class="literal">PKCS#11</code> token does not support <code class="literal">raw RSA</code> or <code class="literal">RSA-PSS</code> signatures, the server applications which use <code class="literal">OpenSSL</code> <code class="literal">TLS</code> library will fail to work with the <code class="literal">RSA</code> key if it is held by the <code class="literal">PKCS#11</code> token. As a result, <code class="literal">TLS</code> communication will fail.
				</p><p>
				To work around this problem, configure server or client to use the <code class="literal">TLS-1.2</code> version as the highest <code class="literal">TLS</code> protocol version available.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1681178">BZ#1681178</a>)
			</p><p><a id="BZ-1664802"/></p><div class="title"><strong>Apache <code class="literal">httpd</code> fails to start if it uses an RSA private key stored in a PKCS#11 device and an RSA-PSS certificate</strong></div><p>
					The PKCS#11 standard does not differentiate between RSA and RSA-PSS key objects and uses the <code class="literal">CKK_RSA</code> type for both. However, OpenSSL uses different types for RSA and RSA-PSS keys. As a consequence, the <code class="literal">openssl-pkcs11</code> engine cannot determine which type should be provided to OpenSSL for PKCS#11 RSA key objects. Currently, the engine sets the key type as RSA keys for all PKCS#11 <code class="literal">CKK_RSA</code> objects. When OpenSSL compares the types of an RSA-PSS public key obtained from the certificate with the type contained in an RSA private key object provided by the engine, it concludes that the types are different. Therefore, the certificate and the private key do not match. The check performed in the <code class="literal">X509_check_private_key()</code> OpenSSL function returns an error in this scenario. The <code class="literal">httpd</code> web server calls this function in its startup process to check if the provided certificate and key match. Since this check always fails for a certificate containing an RSA-PSS public key and a RSA private key stored in the PKCS#11 module, <code class="literal">httpd</code> fails to start using this configuration. There is no workaround available for this issue.
				</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1664802">BZ#1664802</a>)
			</p><p><a id="BZ-1664807"/></p><div class="title"><strong><code class="literal">httpd</code> fails to start if it uses an ECDSA private key without corresponding public key stored in a PKCS#11 device</strong></div><p>
					Unlike RSA keys, ECDSA private keys do not necessarily contain public key information. In this case, you cannot obtain the public key from an ECDSA private key. For this reason, a PKCS#11 device stores public key information in a separate object whether it is a public key object or a certificate object. OpenSSL expects the <code class="literal">EVP_PKEY</code> structure provided by an engine for a private key to contain the public key information. When filling the <code class="literal">EVP_PKEY</code> structure to be provided to OpenSSL, the engine in the <code class="literal">openssl-pkcs11</code> package tries to fetch the public key information only from matching public key objects and ignores the present certificate objects.
				</p><p>
				When OpenSSL requests an ECDSA private key from the engine, the provided <code class="literal">EVP_PKEY</code> structure does not contain the public key information if the public key is not present in the PKCS#11 device, even when a matching certificate that contains the public key is available. As a consequence, since the Apache <code class="literal">httpd</code> web server calls the <code class="literal">X509_check_private_key()</code> function, which requires the public key, in its start-up process, <code class="literal">httpd</code> fails to start in this scenario. To work around the problem, store both the private and public key in the PKCS#11 device when using ECDSA keys. As a result, <code class="literal">httpd</code> starts correctly when ECDSA keys are stored in the PKCS#11 device.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1664807">BZ#1664807</a>)
			</p><p><a id="BZ-1671262"/></p><div class="title"><strong>OpenSSH does not handle PKCS #11 URIs for keys with mismatching labels correctly</strong></div><p>
					The OpenSSH suite can identify key pairs by a label. The label might differ on private and public keys stored on a smart card. Consequently, specifying PKCS #11 URIs with the object part (key label) can prevent OpenSSH from finding appropriate objects in PKCS #11.
				</p><p>
				To work around this problem, specify PKCS #11 URIs without the object part. As a result, OpenSSH is able to use keys on smart cards referenced using PKCS #11 URIs. use keys on smart cards referenced using PKCS #11 URIs.
			</p><p>
				(BZ#1671262)
			</p><p><a id="BZ-1674536"/></p><div class="title"><strong>Output of <code class="literal">iptables-ebtables</code> is not 100% compatible with <code class="literal">ebtables</code></strong></div><p>
					In RHEL 8, the <code class="literal">ebtables</code> command is provided by the <code class="literal">iptables-ebtables</code> package, which contains an <code class="literal">nftables</code>-based reimplementation of the tool. This tool has a different code base, and its output deviates in aspects, which are either negligible or deliberate design choices.
				</p><p>
				Consequently, when migrating your scripts parsing some <code class="literal">ebtables</code> output, adjust the scripts to reflect the following:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						MAC address formatting has been changed to be fixed in length. Where necessary, individual byte values contain a leading zero to maintain the format of two characters per octet.
					</li><li class="listitem">
						Formatting of IPv6 prefixes has been changed to conform with RFC 4291. The trailing part after the slash character no longer contains a netmask in the IPv6 address format but a prefix length. This change applies to valid (left-contiguous) masks only, while others are still printed in the old formatting.
					</li></ul></div><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1674536">BZ#1674536</a>)
			</p><p><a id="BZ-1678661"/></p><div class="title"><strong><code class="literal">curve25519-sha256</code> is not supported by default in OpenSSH</strong></div><p>
					The <code class="literal">curve25519-sha256</code> SSH key exchange algorithm is missing in the system-wide crypto policies configurations for the OpenSSH client and server even though it is compliant with the default policy level. As a consequence, if a client or a server uses <code class="literal">curve25519-sha256</code> and this algorithm is not supported by the host, the connection might fail.
				</p><p>
				To work around this problem, you can manually override the configuration of system-wide crypto policies by modifying the <code class="literal">openssh.config</code> and <code class="literal">opensshserver.config</code> files in the <code class="literal">/etc/crypto-policies/back-ends/</code> directory for the OpenSSH client and server. Note that this configuration is overwritten with every change of system-wide crypto policies. See the <code class="literal">update-crypto-policies(8)</code> man page for more information.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1678661">BZ#1678661</a>)
			</p><p><a id="BZ-1685470"/></p><div class="title"><strong><code class="literal">OpenSSL</code> incorrectly handles PKCS #11 tokens that does not support raw RSA or RSA-PSS signatures</strong></div><p>
					The <code class="literal">OpenSSL</code> library does not detect key-related capabilities of PKCS #11 tokens. Consequently, establishing a TLS connection fails when a signature is created with a token that does not support raw RSA or RSA-PSS signatures.
				</p><p>
				To work around the problem, add the following lines after the <code class="literal">.include</code> line at the end of the <code class="literal">crypto_policy</code> section in the <code class="literal">/etc/pki/tls/openssl.cnf</code> file:
			</p><pre class="screen">SignatureAlgorithms = RSA+SHA256:RSA+SHA512:RSA+SHA384:ECDSA+SHA256:ECDSA+SHA512:ECDSA+SHA384
MaxProtocol = TLSv1.2</pre><p>
				As a result, a TLS connection can be established in the described scenario.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1685470">BZ#1685470</a>)
			</p><p><a id="BZ-1651763"/></p><div class="title"><strong>SSH connections with VMware-hosted systems do not work</strong></div><p>
					The current version of the <code class="literal">OpenSSH</code> suite introduces a change of the default IP Quality of Service (IPQoS) flags in SSH packets, which is not correctly handled by the VMware virtualization platform. Consequently, it is not possible to establish an SSH connection with systems on VMware.
				</p><p>
				To work around this problem, include the <code class="literal">IPQoS=throughput</code> in the <code class="literal">ssh_config</code> file. As a result, SSH connections with VMware-hosted systems work correctly.
			</p><p>
				See the <a class="link" href="https://access.redhat.com/solutions/3706841">RHEL 8 running in VMWare Workstation unable to connect via SSH to other hosts</a> Knowledgebase solution article for more information.
			</p><p>
				(BZ#1651763)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="subscription_management_2"/>Subscription management</h1></div></div></div><p><a id="BZ-1661414"/></p><div class="title"><strong>No message is printed for the successful setting and unsetting of <code class="literal">service-level</code></strong></div><p>
					When the <span class="strong"><strong>candlepin</strong></span> service does not have a 'syspurpose' functionality, subscription manager uses a different code path to set the <code class="literal">service-level</code> argument. This code path does not print the result of the operation. As a consequence, no message is displayed when the service level is set by subscription manager. This is especially problematic when the <code class="literal">service-level</code> set has a typo or is not truly available.
				</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1661414">BZ#1661414</a>)
			</p><p><a id="BZ-1687900"/></p><div class="title"><strong><code class="literal">syspurpose addons</code> have no effect on the <code class="literal">subscription-manager attach --auto</code> output.</strong></div><p>
					In Red Hat Enterprise Linux 8, four attributes of the <code class="literal">syspurpose</code> command-line tool have been added: <code class="literal">role</code>,<code class="literal">usage</code>, <code class="literal">service_level_agreement</code> and <code class="literal">addons</code>. Currently, only <code class="literal">role</code>, <code class="literal">usage</code> and <code class="literal">service_level_agreement</code> affect the output of running the <code class="literal">subscription-manager attach --auto</code> command. Users who attempt to set values to the <code class="literal">addons</code> argument will not observe any effect on the subscriptions that are auto-attached.
				</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1687900">BZ#1687900</a>)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="virtualization_5"/>Virtualization</h1></div></div></div><p><a id="BZ-1666961"/></p><div class="title"><strong>ESXi virtual machines that were customized using cloud-init and cloned boot very slowly</strong></div><p>
					Currently, if the <code class="literal">cloud-init</code> service is used to modify a virtual machine (VM) that runs on the VMware ESXi hypervisor to use static IP and the VM is then cloned, the new cloned VM in some cases takes a very long time to reboot. This is caused <code class="literal">cloud-init</code> rewriting the VM’s static IP to DHCP and then searching for an available datasource.
				</p><p>
				To work around this problem, you can uninstall <code class="literal">cloud-init</code> after the VM is booted for the first time. As a result, the subsequent reboots will not be slowed down.
			</p><p>
				(BZ#1666961, <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1706482">BZ#1706482</a>)
			</p><p><a id="BZ-1689216"/></p><div class="title"><strong>Enabling nested virtualization blocks live migration</strong></div><p>
					Currently, the nested virtualization feature is incompatible with live migration. Therefore, enabling nested virtualization on a RHEL 8 host prevents migrating any virtual machines (VMs) from the host, as well as saving VM state snapshots to disk.
				</p><p>
				Note that nested virtualization is currently provided as a Technology Preview in RHEL 8, and is therefore not supported. In addition, nested virtualization is disabled by default. If you want to enable it, use the <code class="literal">kvm_intel.nested</code> or <code class="literal">kvm_amd.nested</code> module parameters.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1689216">BZ#1689216</a>)
			</p><p><a id="BZ-1641190"/></p><div class="title"><strong>Using <code class="literal">cloud-init</code> to provision virtual machines on Microsoft Azure fails</strong></div><p>
					Currently, it is not possible to use the <code class="literal">cloud-init</code> utility to provision a RHEL 8 virtual machine (VM) on the Microsoft Azure platform. To work around this problem, use one of the following methods:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						Use the <code class="literal">WALinuxAgent</code> package instead of <code class="literal">cloud-init</code> to provision VMs on Microsoft Azure.
					</li><li class="listitem"><p class="simpara">
						Add the following setting to the <code class="literal">[main]</code> section in the <code class="literal">/etc/NetworkManager/NetworkManager.conf</code> file:
					</p><pre class="screen">[main]
dhcp=dhclient</pre></li></ul></div><p>
				(BZ#1641190)
			</p><p><a id="BZ-1583445"/></p><div class="title"><strong>Generation 2 RHEL 8 virtual machines sometimes fail to boot on Hyper-V Server 2016 hosts</strong></div><p>
					When using RHEL 8 as the guest operating system on a virtual machine (VM) running on a Microsoft Hyper-V Server 2016 host, the VM in some cases fails to boot and returns to the GRUB boot menu. In addition, the following error is logged in the Hyper-V event log:
				</p><pre class="screen">The guest operating system reported that it failed with the following error code: 0x1E</pre><p>
				This error occurs due to a UEFI firmware bug on the Hyper-V host. To work around this problem, use Hyper-V Server 2019 as the host.
			</p><p>
				(BZ#1583445)
			</p><p><a id="BZ-1664592"/></p><div class="title"><strong><code class="literal">virsh iface-\*</code> commands do not work consistently</strong></div><p>
					Currently, <code class="literal">virsh iface-*</code> commands, such as <code class="literal">virsh iface-start</code> and <code class="literal">virsh iface-destroy</code>, frequently fail due to configuration dependencies. Therefore, it is recommended not to use <code class="literal">virsh iface-\*</code> commands for configuring and managing host network connections. Instead, use the NetworkManager program and its related management applications.
				</p><p>
				(BZ#1664592)
			</p><p><a id="BZ-1561132"/></p><div class="title"><strong>Linux virtual machine extensions for Azure sometimes do not work</strong></div><p>
					RHEL 8 does not include the <code class="literal">python2</code> package by default. As a consequence, running Linux virtual machine extensions for Azure, also known as <code class="literal">azure-linux-extensions</code>, on a RHEL 8 VM in some cases fails.
				</p><p>
				To increase the probability that <code class="literal">azure-linux-extensions</code> will work as expected, install <code class="literal">python2</code> on the RHEL 8 VM manually:
			</p><p>
				# <span class="strong"><strong>yum install python2</strong></span>
			</p><p>
				(BZ#1561132)
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="supportability_2"/>Supportability</h1></div></div></div><p><a id="BZ-1688274"/></p><div class="title"><strong><code class="literal">redhat-support-tool</code> does not collect <code class="literal">sosreport</code> automatically from <code class="literal">opencase</code></strong></div><p>
					The <code class="literal">redhat-support-tool</code> command cannot create a <code class="literal">sosreport</code> archive. To work around this problem, run the <code class="literal">sosreport</code> command separately and then enter the <code class="literal">redhat-support-tool addattachment -c</code> command to upload the archive or use web UI on the Customer Portal. As a result, a case will be created and <code class="literal">sosreport</code> will be uploaded.
				</p><p>
				Note that the <code class="literal">findkerneldebugs</code>, <code class="literal">btextract</code>, <code class="literal">analyze</code> <code class="literal">diagnose</code> commands do not work as expected and will be fixed in future releases.
			</p><p>
				(<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1688274">BZ#1688274</a>)
			</p></div></div></body></html>
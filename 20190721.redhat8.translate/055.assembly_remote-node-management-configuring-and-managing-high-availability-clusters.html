<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 24. Integrating non-corosync nodes into a cluster: the pacemaker_remote service</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_remote-node-management-configuring-and-managing-high-availability-clusters"/>Chapter 24. Integrating non-corosync nodes into a cluster: the pacemaker_remote service</h1></div></div></div><p>
			The <code class="literal">pacemaker_remote</code> service allows nodes not running <code class="literal">corosync</code> to integrate into the cluster and have the cluster manage their resources just as if they were real cluster nodes.
		</p><p>
			Among the capabilities that the <code class="literal">pacemaker_remote</code> service provides are the following:
		</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
					The <code class="literal">pacemaker_remote</code> service allows you to scale beyond the <code class="literal">corosync</code> 16-node limit.
				</li><li class="listitem">
					The <code class="literal">pacemaker_remote</code> service allows you to manage a virtual environment as a cluster resource and also to manage individual services within the virtual environment as cluster resources.
				</li></ul></div><p>
			The following terms are used to describe the <code class="literal">pacemaker_remote</code> service.
		</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
					<span class="emphasis"><em>cluster node</em></span> — A node running the High Availability services (<code class="literal">pacemaker</code> and <code class="literal">corosync</code>).
				</li><li class="listitem">
					<span class="emphasis"><em>remote node</em></span> — A node running <code class="literal">pacemaker_remote</code> to remotely integrate into the cluster without requiring <code class="literal">corosync</code> cluster membership. A remote node is configured as a cluster resource that uses the <code class="literal">ocf:pacemaker:remote</code> resource agent.
				</li><li class="listitem">
					<span class="emphasis"><em>guest node</em></span> — A virtual guest node running the <code class="literal">pacemaker_remote</code> service. The virtual guest resource is managed by the cluster; it is both started by the cluster and integrated into the cluster as a remote node.
				</li><li class="listitem">
					<span class="emphasis"><em>pacemaker_remote</em></span> — A service daemon capable of performing remote application management within remote nodes and KVM guest nodes in a Pacemaker cluster environment. This service is an enhanced version of Pacemaker’s local executor daemon (<code class="literal">pacemaker-execd</code>) that is capable of managing resources remotely on a node not running corosync.
				</li></ul></div><p>
			A Pacemaker cluster running the <code class="literal">pacemaker_remote</code> service has the following characteristics.
		</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
					Remote nodes and guest nodes run the <code class="literal">pacemaker_remote</code> service (with very little configuration required on the virtual machine side).
				</li><li class="listitem">
					The cluster stack (<code class="literal">pacemaker</code> and <code class="literal">corosync</code>), running on the cluster nodes, connects to the <code class="literal">pacemaker_remote</code> service on the remote nodes, allowing them to integrate into the cluster.
				</li><li class="listitem">
					The cluster stack (<code class="literal">pacemaker</code> and <code class="literal">corosync</code>), running on the cluster nodes, launches the guest nodes and immediately connects to the <code class="literal">pacemaker_remote</code> service on the guest nodes, allowing them to integrate into the cluster.
				</li></ul></div><p>
			The key difference between the cluster nodes and the remote and guest nodes that the cluster nodes manage is that the remote and guest nodes are not running the cluster stack. This means the remote and guest nodes have the following limitations:
		</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
					they do not take place in quorum
				</li><li class="listitem">
					they do not execute fencing device actions
				</li><li class="listitem">
					they are not eligible to be the cluster’s Designated Controller (DC)
				</li><li class="listitem">
					they do not themselves run the full range of <code class="literal">pcs</code> commands
				</li></ul></div><p>
			On the other hand, remote nodes and guest nodes are not bound to the scalability limits associated with the cluster stack.
		</p><p>
			Other than these noted limitations, the remote and guest nodes behave just like cluster nodes in respect to resource management, and the remote and guest nodes can themselves be fenced. The cluster is fully capable of managing and monitoring resources on each remote and guest node: You can build constraints against them, put them in standby, or perform any other action you perform on cluster nodes with the <code class="literal">pcs</code> commands. Remote and guest nodes appear in cluster status output just as cluster nodes do.
		</p><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="ref_host-and-guest-authentication-of-remote-nodes-remote-node-management"/>Host and guest authentication of pacemaker_remote nodes</h1></div></div></div><p>
				The connection between cluster nodes and pacemaker_remote is secured using Transport Layer Security (TLS) with pre-shared key (PSK) encryption and authentication over TCP (using port 3121 by default). This means both the cluster node and the node running <code class="literal">pacemaker_remote</code> must share the same private key. By default this key must be placed at <code class="literal">/etc/pacemaker/authkey</code> on both cluster nodes and remote nodes.
			</p><p>
				The <code class="literal">pcs cluster node add-guest</code> command sets up the <code class="literal">authkey</code> for guest nodes and the <code class="literal">pcs cluster node add-remote</code> command sets up the <code class="literal">authkey</code> for remote nodes.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_configuring-kvm-guest-nodes-remote-node-management"/>Configuring KVM guest nodes</h1></div></div></div><p>
				A Pacemaker guest node is a virtual guest node running the <code class="literal">pacemaker_remote</code> service. The virtual guest node is managed by the cluster.
			</p><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ref_guest-node-resource-options-configuring-kvm-guest-nodes"/>Guest node resource options</h2></div></div></div><p>
					When configuring a virtual machine to act as a guest node, you create a <code class="literal">VirtualDomain</code> resource, which manages the virtual machine. For descriptions of the options you can set for a <code class="literal">VirtualDomain</code> resource, see <a class="xref" href="assembly_configuring-virtual-domain-as-a-resource-configuring-and-managing-high-availability-clusters.html#tb-virtdomain-options-HAAR" title="Table 21.1. Resource Options for Virtual Domain Resources">Table 21.1, “Resource Options for Virtual Domain Resources”</a>.
				</p><p>
					In addition to the <code class="literal">VirtualDomain</code> resource options, metadata options define the resource as a guest node and define the connection parameters. You set these resource options with the <code class="literal">pcs cluster node add-guest</code> command. <a class="xref" href="assembly_remote-node-management-configuring-and-managing-high-availability-clusters.html#tb-remoteklm-options-HAAR" title="Table 24.1. Metadata Options for Configuring KVM Resources as Remote Nodes">Table 24.1, “Metadata Options for Configuring KVM Resources as Remote Nodes”</a> describes these metadata options.
				</p><div class="table"><a id="tb-remoteklm-options-HAAR"/><p class="title"><strong>Table 24.1. Metadata Options for Configuring KVM Resources as Remote Nodes</strong></p><div class="table-contents"><table summary="Metadata Options for Configuring KVM Resources as Remote Nodes" border="1"><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left" valign="top">Field</th><th style="text-align: left" valign="top">Default</th><th style="text-align: left" valign="top">Description</th></tr></thead><tbody><tr><td style="text-align: left" valign="top"> <p>
									<code class="literal">remote-node</code>
								</p>
								 </td><td style="text-align: left" valign="top"> <p>
									&lt;none&gt;
								</p>
								 </td><td style="text-align: left" valign="top"> <p>
									The name of the guest node this resource defines. This both enables the resource as a guest node and defines the unique name used to identify the guest node. <span class="emphasis"><em>WARNING</em></span>: This value cannot overlap with any resource or node IDs.
								</p>
								 </td></tr><tr><td style="text-align: left" valign="top"> <p>
									<code class="literal">remote-port</code>
								</p>
								 </td><td style="text-align: left" valign="top"> <p>
									3121
								</p>
								 </td><td style="text-align: left" valign="top"> <p>
									Configures a custom port to use for the guest connection to <code class="literal">pacemaker_remote</code>
								</p>
								 </td></tr><tr><td style="text-align: left" valign="top"> <p>
									<code class="literal">remote-addr</code>
								</p>
								 </td><td style="text-align: left" valign="top"> <p>
									The address provided in the <code class="literal">pcs host auth</code> command
								</p>
								 </td><td style="text-align: left" valign="top"> <p>
									The IP address or host name to connect to
								</p>
								 </td></tr><tr><td style="text-align: left" valign="top"> <p>
									<code class="literal">remote-connect-timeout</code>
								</p>
								 </td><td style="text-align: left" valign="top"> <p>
									60s
								</p>
								 </td><td style="text-align: left" valign="top"> <p>
									Amount of time before a pending guest connection will time out
								</p>
								 </td></tr></tbody></table></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="proc_integrating-vm-as-guest-node-configuring-kvm-guest-nodes"/>Integrating a virtual machine as a guest node</h2></div></div></div><p>
					The following procedure is a high-level summary overview of the steps to perform to have Pacemaker launch a virtual machine and to integrate that machine as a guest node, using <code class="literal">libvirt</code> and KVM virtual guests.
				</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem">
							Configure the <code class="literal">VirtualDomain</code> resources.
						</li><li class="listitem"><p class="simpara">
							Enter the following commands on every virtual machine to install <code class="literal">pacemaker_remote</code> packages, start the <code class="literal">pcsd</code> service and enable it to run on startup, and allow TCP port 3121 through the firewall.
						</p><pre class="literallayout"># <code class="literal">yum install pacemaker-remote resource-agents pcs</code>
# <code class="literal">systemctl start pcsd.service</code>
# <code class="literal">systemctl enable pcsd.service</code>
# <code class="literal">firewall-cmd --add-port 3121/tcp --permanent</code>
# <code class="literal">firewall-cmd --add-port 2224/tcp --permanent</code>
# <code class="literal">firewall-cmd --reload</code></pre></li><li class="listitem">
							Give each virtual machine a static network address and unique host name, which should be known to all nodes. For information on setting a static IP address for the guest virtual machine, see the <span class="emphasis"><em><span class="citetitle">Virtualization Deployment and Administration Guide</span></em></span>.
						</li><li class="listitem"><p class="simpara">
							If you have not already done so, authenticate <code class="literal">pcs</code> to the node you will be integrating as a quest node.
						</p><pre class="literallayout"># <code class="literal">pcs host auth <span class="emphasis"><em>nodename</em></span></code></pre></li><li class="listitem"><p class="simpara">
							Use the following command to convert an existing <code class="literal">VirtualDomain</code> resource into a guest node. This command must be run on a cluster node and not on the guest node which is being added. In addition to converting the resource, this command copies the <code class="literal">/etc/pacemaker/authkey</code> to the guest node and starts and enables the <code class="literal">pacemaker_remote</code> daemon on the guest node. The node name for the guest node, which you can define arbitrarily, can differ from the host name for the node.
						</p><pre class="literallayout"># <code class="literal">pcs cluster node add-guest <span class="emphasis"><em>nodename</em></span> <span class="emphasis"><em>resource_id</em></span></code> [<code class="literal"><span class="emphasis"><em>options</em></span></code>]</pre></li><li class="listitem"><p class="simpara">
							After creating the <code class="literal">VirtualDomain</code> resource, you can treat the guest node just as you would treat any other node in the cluster. For example, you can create a resource and place a resource constraint on the resource to run on the guest node as in the following commands, which are run from a cluster node. You can include guest nodes in groups, which allows you to group a storage device, file system, and VM.
						</p><pre class="literallayout"># <code class="literal">pcs resource create webserver apache configfile=/etc/httpd/conf/httpd.conf op monitor interval=30s</code>
# <code class="literal">pcs constraint location webserver prefers</code> <code class="literal"><span class="emphasis"><em>nodename</em></span></code></pre></li></ol></div></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_configuring-remote-nodes-remote-node-management"/>Configuring Pacemaker remote nodes</h1></div></div></div><p>
				A remote node is defined as a cluster resource with <code class="literal">ocf:pacemaker:remote</code> as the resource agent. You create this resource with the <code class="literal">pcs cluster node add-remote</code> command.
			</p><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ref_remote-node-resource-options-configuring-remote-nodes"/>Remote node resource options</h2></div></div></div><p>
					<a class="xref" href="assembly_remote-node-management-configuring-and-managing-high-availability-clusters.html#tb-remotenode-options-HAAR" title="Table 24.2. Resource Options for Remote Nodes">Table 24.2, “Resource Options for Remote Nodes”</a> describes the resource options you can configure for a <code class="literal">remote</code> resource.
				</p><div class="table"><a id="tb-remotenode-options-HAAR"/><p class="title"><strong>Table 24.2. Resource Options for Remote Nodes</strong></p><div class="table-contents"><table summary="Resource Options for Remote Nodes" border="1"><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left" valign="top">Field</th><th style="text-align: left" valign="top">Default</th><th style="text-align: left" valign="top">Description</th></tr></thead><tbody><tr><td style="text-align: left" valign="top"> <p>
									<code class="literal">reconnect_interval</code>
								</p>
								 </td><td style="text-align: left" valign="top"> <p>
									0
								</p>
								 </td><td style="text-align: left" valign="top"> <p>
									Time in seconds to wait before attempting to reconnect to a remote node after an active connection to the remote node has been severed. This wait is recurring. If reconnect fails after the wait period, a new reconnect attempt will be made after observing the wait time. When this option is in use, Pacemaker will keep attempting to reach out and connect to the remote node indefinitely after each wait interval.
								</p>
								 </td></tr><tr><td style="text-align: left" valign="top"> <p>
									<code class="literal">server</code>
								</p>
								 </td><td style="text-align: left" valign="top"> <p>
									Address specified with <code class="literal">pcs host auth</code> command
								</p>
								 </td><td style="text-align: left" valign="top"> <p>
									Server to connect to. This can be an IP address or host name.
								</p>
								 </td></tr><tr><td style="text-align: left" valign="top"> <p>
									<code class="literal">port</code>
								</p>
								 </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> <p>
									TCP port to connect to.
								</p>
								 </td></tr></tbody></table></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="proc-integrating-remote-nodes-configuring-remote-nodes"/>Remote node configuration overview</h2></div></div></div><p>
					This section provides a high-level summary overview of the steps to perform to configure a Pacemaker Remote node and to integrate that node into an existing Pacemaker cluster environment.
				</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p class="simpara">
							On the node that you will be configuring as a remote node, allow cluster-related services through the local firewall.
						</p><pre class="literallayout"># <code class="literal">firewall-cmd --permanent --add-service=high-availability</code>
success
# <code class="literal">firewall-cmd --reload</code>
success</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
								If you are using <code class="literal">iptables</code> directly, or some other firewall solution besides <code class="literal">firewalld</code>, simply open the following ports: TCP ports 2224 and 3121.
							</p></div></li><li class="listitem"><p class="simpara">
							Install the <code class="literal">pacemaker_remote</code> daemon on the remote node.
						</p><pre class="literallayout"># <code class="literal">yum install -y pacemaker-remote resource-agents pcs</code></pre></li><li class="listitem"><p class="simpara">
							Start and enable <code class="literal">pcsd</code> on the remote node.
						</p><pre class="literallayout"># <code class="literal">systemctl start pcsd.service</code>
# <code class="literal">systemctl enable pcsd.service</code></pre></li><li class="listitem"><p class="simpara">
							If you have not already done so, authenticate <code class="literal">pcs</code> to the node you will be adding as a remote node.
						</p><pre class="literallayout"># <code class="literal">pcs host auth remote1</code></pre></li><li class="listitem"><p class="simpara">
							Add the remote node resource to the cluster with the following command. This command also syncs all relevant configuration files to the new node, starts the node, and configures it to start <code class="literal">pacemaker_remote</code> on boot. This command must be run on a cluster node and not on the remote node which is being added.
						</p><pre class="literallayout"># <code class="literal">pcs cluster node add-remote remote1</code></pre></li><li class="listitem"><p class="simpara">
							After adding the <code class="literal">remote</code> resource to the cluster, you can treat the remote node just as you would treat any other node in the cluster. For example, you can create a resource and place a resource constraint on the resource to run on the remote node as in the following commands, which are run from a cluster node.
						</p><pre class="literallayout"># <code class="literal">pcs resource create webserver apache configfile=/etc/httpd/conf/httpd.conf op monitor interval=30s</code>
# <code class="literal">pcs constraint location webserver prefers remote1</code></pre><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>
								Never involve a remote node connection resource in a resource group, colocation constraint, or order constraint.
							</p></div></li><li class="listitem">
							Configure fencing resources for the remote node. Remote nodes are fenced the same way as cluster nodes. Configure fencing resources for use with remote nodes the same as you would with cluster nodes. Note, however, that remote nodes can never initiate a fencing action. Only cluster nodes are capable of actually executing a fencing operation against another node.
						</li></ol></div></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_changing-default-port-location-remote-node-management"/>Changing the default port location</h1></div></div></div><p>
				If you need to change the default port location for either Pacemaker or <code class="literal">pacemaker_remote</code>, you can set the <code class="literal">PCMK_remote_port</code> environment variable that affects both of these daemons. This environment variable can be enabled by placing it in the <code class="literal">/etc/sysconfig/pacemaker</code> file as follows.
			</p><pre class="literallayout"><span class="marked">==</span>==# Pacemaker Remote
...
#
# Specify a custom port for Pacemaker Remote connections
PCMK_remote_port=3121</pre><p>
				When changing the default port used by a particular guest node or remote node, the <code class="literal">PCMK_remote_port</code> variable must be set in that node’s <code class="literal">/etc/sysconfig/pacemaker</code> file, and the cluster resource creating the guest node or remote node connection must also be configured with the same port number (using the <code class="literal">remote-port</code> metadata option for guest nodes, or the <code class="literal">port</code> option for remote nodes).
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_upgrading-systems-with-pacemaker-remote-remote-node-management"/>Upgrading systems with pacemaker_remote nodes</h1></div></div></div><p>
				If the <code class="literal">pacemaker_remote</code> service is stopped on an active Pacemaker Remote node, the cluster will gracefully migrate resources off the node before stopping the node. This allows you to perform software upgrades and other routine maintenance procedures without removing the node from the cluster. Once <code class="literal">pacemaker_remote</code> is shut down, however, the cluster will immediately try to reconnect. If <code class="literal">pacemaker_remote</code> is not restarted within the resource’s monitor timeout, the cluster will consider the monitor operation as failed.
			</p><p>
				If you wish to avoid monitor failures when the <code class="literal">pacemaker_remote</code> service is stopped on an active Pacemaker Remote node, you can use the following procedure to take the node out of the cluster before performing any system administration that might stop <code class="literal">pacemaker_remote</code>
			</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem">
						Stop the node’s connection resource with the <code class="literal">pcs resource disable <span class="emphasis"><em>resourcename</em></span></code>, which will move all services off the node. For guest nodes, this will also stop the VM, so the VM must be started outside the cluster (for example, using <code class="literal">virsh</code>) to perform any maintenance.
					</li><li class="listitem">
						Perform the required maintenance.
					</li><li class="listitem">
						When ready to return the node to the cluster, re-enable the resource with the <code class="literal">pcs resource enable</code>.
					</li></ol></div></div></div></body></html>
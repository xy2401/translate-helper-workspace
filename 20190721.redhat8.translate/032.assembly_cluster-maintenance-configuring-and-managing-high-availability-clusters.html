<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 25. Performing cluster maintenance</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters"/>Chapter 25. Performing cluster maintenance</h1></div></div></div><p>
			In order to perform maintenance on the nodes of your cluster, you may need to stop or move the resources and services running on that cluster. Or you may need to stop the cluster software while leaving the services untouched. Pacemaker provides a variety of methods for performing system maintenance.
		</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
					If you need to stop a node in a cluster while continuing to provide the services running on that cluster on another node, you can put the cluster node in standby mode. A node that is in standby mode is no longer able to host resources. Any resource currently active on the node will be moved to another node, or stopped if no other node is eligible to run the resource. For information on standby mode, see <a class="link" href="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters.html#proc_stopping-individual-node-cluster-maintenance" title="Putting a node into standby mode">Putting a node into standby mode</a>.
				</li><li class="listitem"><p class="simpara">
					If you need to move an individual resource off the node on which it is currently running without stopping that resource, you can use the <code class="literal">pcs resource move</code> command to move the resource to a different node. For information on the <code class="literal">pcs resource move</code> command, see <a class="link" href="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters.html#assembly_manually-move-resources-cluster-maintenance" title="Manually moving cluster resources">Manually moving cluster resources</a>.
				</p><p class="simpara">
					When you execute the <code class="literal">pcs resource move</code> command, this adds a constraint to the resource to prevent it from running on the node on which it is currently running. When you are ready to move the resource back, you can execute the <code class="literal">pcs resource clear</code> or the <code class="literal">pcs constraint delete</code> command to remove the constraint. This does not necessarily move the resources back to the original node, however, since where the resources can run at that point depends on how you have configured your resources initially. You can relocate a resource to its preferred node with the <code class="literal">pcs resource relocate run</code> command, as described in <a class="link" href="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters.html#proc_moving-resource-to-preferred-node-manually-move-resources" title="Moving a resource to its preferred node">Moving a resource to its preferred node</a>.
				</p></li><li class="listitem">
					If you need to stop a running resource entirely and prevent the cluster from starting it again, you can use the <code class="literal">pcs resource disable</code> command. For information on the <code class="literal">pcs resource disable</code> command, see <a class="link" href="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters.html#proc_disabling-resources-cluster-maintenance" title="Enabling, disabling, and banning cluster resources">Enabling, disabling, and banning cluster resources</a>.
				</li><li class="listitem">
					If you want to prevent Pacemaker from taking any action for a resource (for example, if you want to disable recovery actions while performing maintenance on the resource, or if you need to reload the <code class="literal">/etc/sysconfig/pacemaker</code> settings), use the <code class="literal">pcs resource unmanage</code> command, as described in <a class="link" href="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters.html#proc_unmanaging-resources-cluster-maintenance" title="Setting a resource to unmanaged mode">Setting a resource to unmanaged mode</a>. Pacemaker Remote connection resources should never be unmanaged.
				</li><li class="listitem">
					If you need to put the cluster in a state where no services will be started or stopped, you can set the <code class="literal">maintenance-mode</code> cluster property. Putting the cluster into maintenance mode automatically unmanages all resources. For information on putting the cluster in maintenance mode, see <a class="link" href="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters.html#proc_setting-maintenance-mode-cluster-maintenance" title="Putting a cluster in maintenance mode">Putting a cluster in maintenance mode</a>.
				</li><li class="listitem">
					If you need to update the packages that make up the RHEL High Availability and Resilient Storage Add-Ons, you can update the packages on one node at a time or on the entire cluster as a whole, as summarized in <a class="link" href="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters.html#proc_updating-cluster-packages-cluster-maintenance" title="Updating a RHEL high availability cluster">Updating a Red Hat Enterprise Linux high availability cluster</a>.
				</li><li class="listitem">
					If you need to perform maintenance on a Pacemaker remote node, you can remove that node from the cluster by disabling the remote node resource, as described in <a class="link" href="assembly_cluster-maintenance-configuring-and-managing-high-availability-clusters.html#proc_upgrading-remote-nodes-cluster-maintenance" title="Upgrading remote nodes and guest nodes">Upgrading remote nodes and guest nodes</a>.
				</li></ul></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_stopping-individual-node-cluster-maintenance"/>Putting a node into standby mode</h1></div></div></div><p>
				When a cluster node is in standby mode, the node is no longer able to host resources. Any resources currently active on the node will be moved to another node.
			</p><p>
				The following command puts the specified node into standby mode. If you specify the <code class="literal">--all</code>, this command puts all nodes into standby mode.
			</p><p>
				You can use this command when updating a resource’s packages. You can also use this command when testing a configuration, to simulate recovery without actually shutting down a node.
			</p><pre class="literallayout">pcs node standby <span class="emphasis"><em>node</em></span> | --all</pre><p>
				The following command removes the specified node from standby mode. After running this command, the specified node is then able to host resources. If you specify the <code class="literal">--all</code>, this command removes all nodes from standby mode.
			</p><pre class="literallayout">pcs node unstandby <span class="emphasis"><em>node</em></span> | --all</pre><p>
				Note that when you execute the <code class="literal">pcs node standby</code> command, this prevents resources from running on the indicated node. When you execute the <code class="literal">pcs node unstandby</code> command, this allows resources to run on the indicated node. This does not necessarily move the resources back to the indicated node; where the resources can run at that point depends on how you have configured your resources initially.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_manually-move-resources-cluster-maintenance"/>Manually moving cluster resources</h1></div></div></div><p>
				You can override the cluster and force resources to move from their current location. There are two occasions when you would want to do this:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						When a node is under maintenance, and you need to move all resources running on that node to a different node
					</li><li class="listitem">
						When individually specified resources needs to be moved
					</li></ul></div><p>
				To move all resources running on a node to a different node, you put the node in standby mode.
			</p><p>
				You can move individually specified resources in either of the following ways.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						You can use the <code class="literal">pcs resource move</code> command to move a resource off a node on which it is currently running.
					</li><li class="listitem">
						You can use the <code class="literal">pcs resource relocate run</code> command to move a resource to its preferred node, as determined by current cluster status, constraints, location of resources and other settings.
					</li></ul></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="proc_moving-resource-from-node-manually-move-resources"/>Moving a resource from its current node</h2></div></div></div><p>
					To move a resource off the node on which it is currently running, use the following command, specifying the <span class="emphasis"><em>resource_id</em></span> of the resource as defined. Specify the <code class="literal">destination_node</code> if you want to indicate on which node to run the resource that you are moving.
				</p><pre class="literallayout">pcs resource move <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>destination_node</em></span>] [--master] [lifetime=<span class="emphasis"><em>lifetime</em></span>]</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
						When you execute the <code class="literal">pcs resource move</code> command, this adds a constraint to the resource to prevent it from running on the node on which it is currently running. You can execute the <code class="literal">pcs resource clear</code> or the <code class="literal">pcs constraint delete</code> command to remove the constraint. This does not necessarily move the resources back to the original node; where the resources can run at that point depends on how you have configured your resources initially.
					</p></div><p>
					If you specify the <code class="literal">--master</code> parameter of the <code class="literal">pcs resource move</code> command, the scope of the constraint is limited to the master role and you must specify <span class="emphasis"><em>master_id</em></span> rather than <span class="emphasis"><em>resource_id</em></span>.
				</p><p>
					You can optionally configure a <code class="literal">lifetime</code> parameter for the <code class="literal">pcs resource move</code> command to indicate a period of time the constraint should remain. You specify the units of a <code class="literal">lifetime</code> parameter according to the format defined in ISO 8601, which requires that you specify the unit as a capital letter such as Y (for years), M (for months), W (for weeks), D (for days), H (for hours), M (for minutes), and S (for seconds).
				</p><p>
					To distinguish a unit of minutes(M) from a unit of months(M), you must specify PT before indicating the value in minutes. For example, a <code class="literal">lifetime</code> parameter of 5M indicates an interval of five months, while a <code class="literal">lifetime</code> parameter of PT5M indicates an interval of five minutes.
				</p><p>
					The <code class="literal">lifetime</code> parameter is checked at intervals defined by the <code class="literal">cluster-recheck-interval</code> cluster property. By default this value is 15 minutes. If your configuration requires that you check this parameter more frequently, you can reset this value with the following command.
				</p><pre class="literallayout">pcs property set cluster-recheck-interval=<span class="emphasis"><em>value</em></span></pre><p>
					You can optionally configure a <code class="literal">--wait[=<span class="emphasis"><em>n</em></span>]</code> parameter for the <code class="literal">pcs resource move</code> command to indicate the number of seconds to wait for the resource to start on the destination node before returning 0 if the resource is started or 1 if the resource has not yet started. If you do not specify n, the default resource timeout will be used.
				</p><p>
					The following command moves the resource <code class="literal">resource1</code> to node <code class="literal">example-node2</code> and prevents it from moving back to the node on which it was originally running for one hour and thirty minutes.
				</p><pre class="literallayout">pcs resource move resource1 example-node2 lifetime=PT1H30M</pre><p>
					The following command moves the resource <code class="literal">resource1</code> to node <code class="literal">example-node2</code> and prevents it from moving back to the node on which it was originally running for thirty minutes.
				</p><pre class="literallayout">pcs resource move resource1 example-node2 lifetime=PT30M</pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="proc_moving-resource-to-preferred-node-manually-move-resources"/>Moving a resource to its preferred node</h2></div></div></div><p>
					After a resource has moved, either due to a failover or to an administrator manually moving the node, it will not necessarily move back to its original node even after the circumstances that caused the failover have been corrected. To relocate resources to their preferred node, use the following command. A preferred node is determined by the current cluster status, constraints, resource location, and other settings and may change over time.
				</p><pre class="literallayout">pcs resource relocate run [<span class="emphasis"><em>resource1</em></span>] [<span class="emphasis"><em>resource2</em></span>] ...</pre><p>
					If you do not specify any resources, all resource are relocated to their preferred nodes.
				</p><p>
					This command calculates the preferred node for each resource while ignoring resource stickiness. After calculating the preferred node, it creates location constraints which will cause the resources to move to their preferred nodes. Once the resources have been moved, the constraints are deleted automatically. To remove all constraints created by the <code class="literal">pcs resource relocate run</code> command, you can enter the <code class="literal">pcs resource relocate clear</code> command. To display the current status of resources and their optimal node ignoring resource stickiness, enter the <code class="literal">pcs resource relocate show</code> command.
				</p></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_disabling-resources-cluster-maintenance"/>Enabling, disabling, and banning cluster resources</h1></div></div></div><p>
				In addition to the <code class="literal">pcs resource move</code> and <code class="literal">pcs resource relocate</code> commands, there are a variety of other commands you can use to control the behavior of cluster resources.
			</p><p>
				You can manually stop a running resource and prevent the cluster from starting it again with the following command. Depending on the rest of the configuration (constraints, options, failures, and so on), the resource may remain started. If you specify the <code class="literal">--wait</code> option, <span class="strong"><strong><span class="application">pcs</span></strong></span> will wait up to 'n' seconds for the resource to stop and then return 0 if the resource is stopped or 1 if the resource has not stopped. If 'n' is not specified it defaults to 60 minutes.
			</p><pre class="literallayout">pcs resource disable <span class="emphasis"><em>resource_id</em></span> [--wait[=<span class="emphasis"><em>n</em></span>]]</pre><p>
				You can use the following command to allow the cluster to start a resource. Depending on the rest of the configuration, the resource may remain stopped. If you specify the <code class="literal">--wait</code> option, <span class="strong"><strong><span class="application">pcs</span></strong></span> will wait up to 'n' seconds for the resource to start and then return 0 if the resource is started or 1 if the resource has not started. If 'n' is not specified it defaults to 60 minutes.
			</p><pre class="literallayout">pcs resource enable <span class="emphasis"><em>resource_id</em></span> [--wait[=<span class="emphasis"><em>n</em></span>]]</pre><p>
				Use the following command to prevent a resource from running on a specified node, or on the current node if no node is specified.
			</p><pre class="literallayout">pcs resource ban <span class="emphasis"><em>resource_id</em></span> [<span class="emphasis"><em>node</em></span>] [--master] [lifetime=<span class="emphasis"><em>lifetime</em></span>] [--wait[=<span class="emphasis"><em>n</em></span>]]</pre><p>
				Note that when you execute the <code class="literal">pcs resource ban</code> command, this adds a -INFINITY location constraint to the resource to prevent it from running on the indicated node. You can execute the <code class="literal">pcs resource clear</code> or the <code class="literal">pcs constraint delete</code> command to remove the constraint. This does not necessarily move the resources back to the indicated node; where the resources can run at that point depends on how you have configured your resources initially.
			</p><p>
				If you specify the <code class="literal">--master</code> parameter of the <code class="literal">pcs resource ban</code> command, the scope of the constraint is limited to the master role and you must specify <span class="emphasis"><em>master_id</em></span> rather than <span class="emphasis"><em>resource_id</em></span>.
			</p><p>
				You can optionally configure a <code class="literal">lifetime</code> parameter for the <code class="literal">pcs resource ban</code> command to indicate a period of time the constraint should remain.
			</p><p>
				You can optionally configure a <code class="literal">--wait[=<span class="emphasis"><em>n</em></span>]</code> parameter for the <code class="literal">pcs resource ban</code> command to indicate the number of seconds to wait for the resource to start on the destination node before returning 0 if the resource is started or 1 if the resource has not yet started. If you do not specify n, the default resource timeout will be used.
			</p><p>
				You can use the <code class="literal">debug-start</code> parameter of the <code class="literal">pcs resource</code> command to force a specified resource to start on the current node, ignoring the cluster recommendations and printing the output from starting the resource. This is mainly used for debugging resources; starting resources on a cluster is (almost) always done by Pacemaker and not directly with a <code class="literal">pcs</code> command. If your resource is not starting, it is usually due to either a misconfiguration of the resource (which you debug in the system log), constraints that prevent the resource from starting, or the resource being disabled. You can use this command to test resource configuration, but it should not normally be used to start resources in a cluster.
			</p><p>
				The format of the <code class="literal">debug-start</code> command is as follows.
			</p><pre class="literallayout">pcs resource debug-start <span class="emphasis"><em>resource_id</em></span></pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_unmanaging-resources-cluster-maintenance"/>Setting a resource to unmanaged mode</h1></div></div></div><p>
				When a resource is in <code class="literal">unmanaged</code> mode, the resource is still in the configuration but Pacemaker does not manage the resource.
			</p><p>
				The following command sets the indicated resources to <code class="literal">unmanaged</code> mode.
			</p><pre class="literallayout">pcs resource unmanage <span class="emphasis"><em>resource1</em></span>  [<span class="emphasis"><em>resource2</em></span>] ...</pre><p>
				The following command sets resources to <code class="literal">managed</code> mode, which is the default state.
			</p><pre class="literallayout">pcs resource manage <span class="emphasis"><em>resource1</em></span>  [<span class="emphasis"><em>resource2</em></span>] ...</pre><p>
				You can specify the name of a resource group with the <code class="literal">pcs resource manage</code> or <code class="literal">pcs resource unmanage</code> command. The command will act on all of the resources in the group, so that you can set all of the resources in a group to <code class="literal">managed</code> or <code class="literal">unmanaged</code> mode with a single command and then manage the contained resources individually.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_setting-maintenance-mode-cluster-maintenance"/>Putting a cluster in maintenance mode</h1></div></div></div><p>
				When a cluster is in maintenance mode, the cluster does not start or stop any services until told otherwise. When maintenance mode is completed, the cluster does a sanity check of the current state of any services, and then stops or starts any that need it.
			</p><p>
				To put a cluster in maintenance mode, use the following command to set the <code class="literal">maintenance-mode</code> cluster property to <code class="literal">true</code>.
			</p><pre class="literallayout"># <code class="literal">pcs property set maintenance-mode=true</code></pre><p>
				To remove a cluster from maintenance mode, use the following command to set the <code class="literal">maintenance-mode</code> cluster property to <code class="literal">false</code>.
			</p><pre class="literallayout"># <code class="literal">pcs property set maintenance-mode=false</code></pre><p>
				u can remove a cluster property from the configuration with the following command.
			</p><pre class="literallayout">pcs property unset <span class="emphasis"><em>property</em></span></pre><p>
				Alternately, you can remove a cluster property from a configuration by leaving the value field of the <code class="literal">pcs property set</code> command blank. This restores that property to its default value. For example, if you have previously set the <code class="literal">symmetric-cluster</code> property to <code class="literal">false</code>, the following command removes the value you have set from the configuration and restores the value of <code class="literal">symmetric-cluster</code> to <code class="literal">true</code>, which is its default value.
			</p><pre class="literallayout"># <code class="literal">pcs property set symmetric-cluster=</code></pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_updating-cluster-packages-cluster-maintenance"/>Updating a RHEL high availability cluster</h1></div></div></div><p>
				Updating packages that make up the RHEL High Availability and Resilient Storage Add-Ons, either individually or as a whole, can be done in one of two general ways:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						<span class="emphasis"><em>Rolling Updates</em></span>: Remove one node at a time from service, update its software, then integrate it back into the cluster. This allows the cluster to continue providing service and managing resources while each node is updated.
					</li><li class="listitem">
						<span class="emphasis"><em>Entire Cluster Update</em></span>: Stop the entire cluster, apply updates to all nodes, then start the cluster back up.
					</li></ul></div><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>
					It is critical that when performing software update procedures for Red Hat Enterprise LInux High Availability and Resilient Storage clusters, you ensure that any node that will undergo updates is not an active member of the cluster before those updates are initiated.
				</p></div><p>
				For a full description of each of these methods and the procedures to follow for the updates, see <a class="link" href="https://access.redhat.com/articles/2059253/">Recommended Practices for Applying Software Updates to a RHEL High Availability or Resilient Storage Cluster</a>.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_upgrading-remote-nodes-cluster-maintenance"/>Upgrading remote nodes and guest nodes</h1></div></div></div><p>
				If the <code class="literal">pacemaker_remote</code> service is stopped on an active remote node or guest node, the cluster will gracefully migrate resources off the node before stopping the node. This allows you to perform software upgrades and other routine maintenance procedures without removing the node from the cluster. Once <code class="literal">pacemaker_remote</code> is shut down, however, the cluster will immediately try to reconnect. If <code class="literal">pacemaker_remote</code> is not restarted within the resource’s monitor timeout, the cluster will consider the monitor operation as failed.
			</p><p>
				If you wish to avoid monitor failures when the <code class="literal">pacemaker_remote</code> service is stopped on an active Pacemaker Remote node, you can use the following procedure to take the node out of the cluster before performing any system administration that might stop <code class="literal">pacemaker_remote</code>
			</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem">
						Stop the node’s connection resource with the <code class="literal">pcs resource disable <span class="emphasis"><em>resourcename</em></span></code>, which will move all services off the node. For guest nodes, this will also stop the VM, so the VM must be started outside the cluster (for example, using <code class="literal">virsh</code>) to perform any maintenance.
					</li><li class="listitem">
						Perform the required maintenance.
					</li><li class="listitem">
						When ready to return the node to the cluster, re-enable the resource with the <code class="literal">pcs resource enable</code>.
					</li></ol></div></div></div></body></html>
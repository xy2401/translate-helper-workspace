<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 10. Kernel</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="kernel_considerations-in-adopting-RHEL-8"/>Chapter 10. Kernel</h1></div></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="resource-control_kernel"/>Resource control</h1></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="control_group_v2_available_as_a_technology_preview_in_rhel_8"/>Control group v2 available as a Technology Preview in RHEL 8</h2></div></div></div><p>
					<span class="strong"><strong>Control group v2</strong></span> mechanism is a unified hierarchy control group. <span class="strong"><strong>Control group v2</strong></span> organizes processes hierarchically and distributes system resources along the hierarchy in a controlled and configurable manner.
				</p><p>
					Unlike the previous version, <span class="strong"><strong>control group v2</strong></span> has only a single hierarchy. This single hierarchy enables the Linux kernel to:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							Categorize processes based on the role of their owner.
						</li><li class="listitem">
							Eliminate issues with conflicting policies of multiple hierarchies.
						</li></ul></div><p>
					<span class="strong"><strong>Control group v2</strong></span> supports numerous controllers:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p class="simpara">
							CPU controller regulates the distribution of CPU cycles. This controller implements:
						</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
									Weight and absolute bandwidth limit models for normal scheduling policy.
								</li><li class="listitem">
									Absolute bandwidth allocation model for real time scheduling policy.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Memory controller regulates the memory distribution. Currently, the following types of memory usages are tracked:
						</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
									Userland memory - page cache and anonymous memory.
								</li><li class="listitem">
									Kernel data structures such as dentries and inodes.
								</li><li class="listitem">
									TCP socket buffers.
								</li></ul></div></li><li class="listitem">
							I/O controller regulates the distribution of I/O resources.
						</li><li class="listitem">
							Remote Direct Memory Access (RDMA) controller limits RDMA/IB specific resources that certain processes can use. These processes are grouped through the RDMA controller.
						</li><li class="listitem">
							Process number controller enables the control group to stop any new tasks from being <code class="literal">fork()</code>’d or <code class="literal">clone()</code>’d after a certain limit.
						</li><li class="listitem">
							Writeback controller acts as a mechanism, which balances conflicts between I/O and the memory controllers.
						</li></ul></div><p>
					The information above was based on <span class="emphasis"><em><span class="citetitle"><a class="link" href="https://www.kernel.org/doc/Documentation/cgroup-v2.txt">cgroups-v2 online documentation</a></span></em></span>. You can refer to the same link to obtain more information about particular <span class="strong"><strong>control group v2</strong></span> controllers.
				</p></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="memory-management_kernel"/>Memory management</h1></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="fifty-two_bit_pa_for_64_bit_arm_available"/>52-bit PA for 64-bit ARM available</h2></div></div></div><p>
					With this update, support for 52-bit physical addressing (PA) for the 64-bit ARM architecture is available. This provides a larger physical address space than previous 48-bit PA.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="five_level_page_tables_x86_64"/>5-level page tables x86_64</h2></div></div></div><p>
					With Red Hat Enterprise Linux 7, existing memory bus had 48/46 bit of virtual/physical memory addressing capacity, and the Linux kernel implemented 4 levels of page tables to manage these virtual addresses to physical addresses. The physical bus addressing line put the physical memory upper limit capacity at 64 TB.
				</p><p>
					These limits have been extended to 57/52 bit of virtual/physical memory addressing with 128 PiB of virtual address space (64PB user/64PB kernel) and 4 PB of physical memory capacity.
				</p><p>
					With the extended address range, the memory management in Red Hat Enterprise Linux 8 adds support for 5-level page table implementation, to be able to handle the expanded address range. By default RHEL8 will disable the 5-level page table support even on systems that support this feature. This is due to a potential performance degradation when using 5 level of page tables if extended virtual or physical address space is not needed. A boot argument will enable systems with hardware that supports this feature to use it.
				</p></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="performance-analysis-and-observability-tools_kernel"/>Performance analysis and observability tools</h1></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="bpftool_added_to_kernel"/>bpftool added to kernel</h2></div></div></div><p>
					The <code class="literal">bpftool</code> utility that serves for inspection and simple manipulation of programs and maps based on extended Berkeley Packet Filtering (eBPF) has been added into the Linux kernel. <code class="literal">bpftool</code> is a part of the kernel source tree, and is provided by the <span class="strong"><strong>bpftool</strong></span> package, which is included as a sub-package of the <span class="strong"><strong>kernel</strong></span> package.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ebpf_available_as_a_technology_preview"/>eBPF available as a Technology Preview</h2></div></div></div><p>
					The <span class="strong"><strong>extended Berkeley Packet Filtering (eBPF)</strong></span> feature is available as a Technology Preview for both networking and tracing. <span class="strong"><strong>eBPF</strong></span> enables the user space to attach custom programs onto a variety of points (sockets, trace points, packet reception) to receive and process data. The feature includes a new system call <code class="literal">bpf()</code>, which supports creating various types of maps, and also to insert various types of programs into the kernel. Note that the <code class="literal">bpf()</code> syscall can be successfully used only by a user with the <code class="literal">CAP_SYS_ADMIN</code> capability, such as a root user. See the <code class="literal">bpf</code>(2) man page for more information.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="bcc_is_available_as_a_technology_preview"/>BCC is available as a Technology Preview</h2></div></div></div><p>
					<code class="literal">BPF Compiler Collection (BCC)</code> is a user space tool kit for creating efficient kernel tracing and manipulation programs that is available as a Technology Preview in Red Hat Enterprise Linux 8. <code class="literal">BCC</code> provides tools for I/O analysis, networking, and monitoring of Linux operating systems using the <code class="literal">extended Berkeley Packet Filtering (eBPF)</code>.
				</p></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="booting-process_kernel"/>Booting process</h1></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="how_to_install_and_boot_custom_kernels_in_rhel_8"/>How to install and boot custom kernels in RHEL 8</h2></div></div></div><p>
					The Boot Loader Specification (BLS) defines a scheme and file format to manage bootloader configurations for each boot option in a drop-in directory. There is no need to manipulate the individual drop-in configuration files. This premise is particularly relevant in Red Hat Enterprise Linux 8 because not all architectures use the same bootloader:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							<code class="literal">x86_64</code>, <code class="literal">aarch64</code> and <code class="literal">ppc64le</code> with open firmware use <code class="literal">GRUB2</code>
						</li><li class="listitem">
							<code class="literal">ppc64le</code> with Open Power Abstraction Layer (OPAL) uses <code class="literal">Petitboot</code>
						</li><li class="listitem">
							<code class="literal">s390x</code> uses <code class="literal">zipl</code>
						</li></ul></div><p>
					Each bootloader has a different configuration file and format that has to be modified when a new kernel is installed or removed. In the previous versions of Red Hat Enterprise Linux the component that permitted this work was the <code class="literal">grubby</code> utility. However, for Red Hat Enterprise Linux 8 the bootloader configuration was standardized by implementing the BLS file format, where <code class="literal">grubby</code> works as a thin wrapper around the BLS operations.
				</p></div></div></div></body></html>
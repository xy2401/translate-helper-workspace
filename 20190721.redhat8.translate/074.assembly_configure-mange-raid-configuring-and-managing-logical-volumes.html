<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 7. Configuring RAID logical volumes</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_configure-mange-raid-configuring-and-managing-logical-volumes"/>Chapter 7. Configuring RAID logical volumes</h1></div></div></div><p>
			LVM supports RAID0/1/4/5/6/10.
		</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
				RAID logical volumes are not cluster-aware. While RAID logical volumes can be created and activated exclusively on one machine, they cannot be activated simultaneously on more than one machine.
			</p></div><p>
			To create a RAID logical volume, you specify a raid type as the <code class="literal">--type</code> argument of the <code class="literal">lvcreate</code> command. <a class="xref" href="assembly_configure-mange-raid-configuring-and-managing-logical-volumes.html#tb-raid-types" title="Table 7.1. RAID Segment Types">Table 7.1, “RAID Segment Types”</a> describes the possible RAID segment types. After you have created a RAID logical volume with LVM, you can activate, change, remove, display, and use the volume just as you would any other LVM logical volume.
		</p><div class="table"><a id="tb-raid-types"/><p class="title"><strong>Table 7.1. RAID Segment Types</strong></p><div class="table-contents"><table summary="RAID Segment Types" border="1"><colgroup><col class="col_1"/><col class="col_2"/></colgroup><thead><tr><th style="text-align: left" valign="top">Segment type</th><th style="text-align: left" valign="top">Description</th></tr></thead><tbody><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid1</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							RAID1 mirroring. This is the default value for the <code class="literal">--type</code> argument of the <code class="literal">lvcreate</code> command when you specify the <code class="literal">-m</code> but you do not specify striping.
						</p>
						 </td></tr><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid4</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							RAID4 dedicated parity disk
						</p>
						 </td></tr><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid5</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							Same as <code class="literal">raid5_ls</code>
						</p>
						 </td></tr><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid5_la</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							* RAID5 left asymmetric.
						</p>
						 <p>
							* Rotating parity 0 with data continuation
						</p>
						 </td></tr><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid5_ra</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							* RAID5 right asymmetric.
						</p>
						 <p>
							* Rotating parity N with data continuation
						</p>
						 </td></tr><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid5_ls</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							* RAID5 left symmetric.
						</p>
						 <p>
							* Rotating parity 0 with data restart
						</p>
						 </td></tr><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid5_rs</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							* RAID5 right symmetric.
						</p>
						 <p>
							* Rotating parity N with data restart
						</p>
						 </td></tr><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid6</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							Same as <code class="literal">raid6_zr</code>
						</p>
						 </td></tr><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid6_zr</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							* RAID6 zero restart
						</p>
						 <p>
							* Rotating parity zero (left-to-right) with data restart
						</p>
						 </td></tr><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid6_nr</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							* RAID6 N restart
						</p>
						 <p>
							* Rotating parity N (left-to-right) with data restart
						</p>
						 </td></tr><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid6_nc</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							* RAID6 N continue
						</p>
						 <p>
							* Rotating parity N (left-to-right) with data continuation
						</p>
						 </td></tr><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid10</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							* Striped mirrors. This is the default value for the <code class="literal">--type</code> argument of the <code class="literal">lvcreate</code> command if you specify the <code class="literal">-m</code> and you specify a number of stripes that is greater than 1.
						</p>
						 <p>
							* Striping of mirror sets
						</p>
						 </td></tr><tr><td style="text-align: left" valign="top"> <p>
							<code class="literal">raid0/raid0_meta</code>
						</p>
						 </td><td style="text-align: left" valign="top"> <p>
							Striping. RAID0 spreads logical volume data across multiple data subvolumes in units of stripe size. This is used to increase performance. Logical volume data will be lost if any of the data subvolumes fail.
						</p>
						 </td></tr></tbody></table></div></div><p>
			For most users, specifying one of the five available primary types (<code class="literal">raid1</code>, <code class="literal">raid4</code>, <code class="literal">raid5</code>, <code class="literal">raid6</code>, <code class="literal">raid10</code>) should be sufficient. For more information on the different algorithms used by RAID 5/6, see chapter four of the <span class="emphasis"><em><span class="citetitle">Common RAID Disk Data Format Specification</span></em></span> at <a class="link" href="http://www.snia.org/sites/default/files/SNIA_DDF_Technical_Position_v2.0.pdf">http://www.snia.org/sites/default/files/SNIA_DDF_Technical_Position_v2.0.pdf</a>.
		</p><p>
			When you create a RAID logical volume, LVM creates a metadata subvolume that is one extent in size for every data or parity subvolume in the array. For example, creating a 2-way RAID1 array results in two metadata subvolumes (<code class="literal">lv_rmeta_0</code> and <code class="literal">lv_rmeta_1</code>) and two data subvolumes (<code class="literal">lv_rimage_0</code> and <code class="literal">lv_rimage_1</code>). Similarly, creating a 3-way stripe (plus 1 implicit parity device) RAID4 results in 4 metadata subvolumes (<code class="literal">lv_rmeta_0</code>, <code class="literal">lv_rmeta_1</code>, <code class="literal">lv_rmeta_2</code>, and <code class="literal">lv_rmeta_3</code>) and 4 data subvolumes (<code class="literal">lv_rimage_0</code>, <code class="literal">lv_rimage_1</code>, <code class="literal">lv_rimage_2</code>, and <code class="literal">lv_rimage_3</code>).
		</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
				You can generate commands to create logical volumes on RAID storage with the LVM RAID Calculator application. This application uses the information you input about your current or planned storage to generate these commands. The LVM RAID Calculator application can be found at <a class="link" href="https://access.redhat.com/labs/lvmraidcalculator/">https://access.redhat.com/labs/lvmraidcalculator/</a>.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="ref_creating-raid-volume-configure-manage-raid"/>Creating RAID logical volumes</h1></div></div></div><p>
				This section provides example commands that create different types of RAID logical volume.
			</p><p>
				You can create RAID1 arrays with different numbers of copies according to the value you specify for the <code class="literal">-m</code> argument. Similarly, you specify the number of stripes for a RAID 4/5/6 logical volume with the <code class="literal">-i argument</code>. You can also specify the stripe size with the <code class="literal">-I</code> argument.
			</p><p>
				The following command creates a 2-way RAID1 array named <code class="literal">my_lv</code> in the volume group <code class="literal">my_vg</code> that is one gigabyte in size.
			</p><pre class="literallayout"># <code class="literal">lvcreate --type raid1 -m 1 -L 1G -n my_lv my_vg</code></pre><p>
				The following command creates a RAID5 array (3 stripes + 1 implicit parity drive) named <code class="literal">my_lv</code> in the volume group <code class="literal">my_vg</code> that is one gigabyte in size. Note that you specify the number of stripes just as you do for an LVM striped volume; the correct number of parity drives is added automatically.
			</p><pre class="literallayout"># <code class="literal">lvcreate --type raid5 -i 3 -L 1G -n my_lv my_vg</code></pre><p>
				The following command creates a RAID6 array (3 stripes + 2 implicit parity drives) named <code class="literal">my_lv</code> in the volume group <code class="literal">my_vg</code> that is one gigabyte in size.
			</p><pre class="literallayout"># <code class="literal">lvcreate --type raid6 -i 3 -L 1G -n my_lv my_vg</code></pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_creating-a-striped-logical-volume_configure-manage-raid"/>Creating a RAID0 (striped) logical volume</h1></div></div></div><p>
				A RAID0 logical volume spreads logical volume data across multiple data subvolumes in units of stripe size.
			</p><p>
				The format for the command to create a RAID0 volume is as follows.
			</p><pre class="literallayout">lvcreate --type raid0[_meta] --stripes <span class="emphasis"><em>Stripes</em></span> --stripesize <span class="emphasis"><em>StripeSize</em></span> <span class="emphasis"><em>VolumeGroup</em></span> [<span class="emphasis"><em>PhysicalVolumePath</em></span> ...]</pre><div class="table"><a id="idm140241349086656"/><p class="title"><strong>Table 7.2. RAID0 Command Creation parameters</strong></p><div class="table-contents"><table summary="RAID0 Command Creation parameters" border="1"><colgroup><col class="col_1"/><col class="col_2"/></colgroup><thead><tr><th style="text-align: left" valign="top">Parameter</th><th style="text-align: left" valign="top">Description</th></tr></thead><tbody><tr><td style="text-align: left" valign="top"> <p>
								<code class="literal">--type raid0[_meta]</code>
							</p>
							 </td><td style="text-align: left" valign="top"> <p>
								Specifying <code class="literal">raid0</code> creates a RAID0 volume without metadata volumes. Specifying <code class="literal">raid0_meta</code> creates a RAID0 volume with metadata volumes. Because RAID0 is non-resilient, it does not have to store any mirrored data blocks as RAID1/10 or calculate and store any parity blocks as RAID4/5/6 do. Hence, it does not need metadata volumes to keep state about resynchronization progress of mirrored or parity blocks. Metadata volumes become mandatory on a conversion from RAID0 to RAID4/5/6/10, however, and specifying <code class="literal">raid0_meta</code> preallocates those metadata volumes to prevent a respective allocation failure.
							</p>
							 </td></tr><tr><td style="text-align: left" valign="top"> <p>
								<code class="literal">--stripes <span class="emphasis"><em>Stripes</em></span></code>
							</p>
							 </td><td style="text-align: left" valign="top"> <p>
								Specifies the number of devices to spread the logical volume across.
							</p>
							 </td></tr><tr><td style="text-align: left" valign="top"> <p>
								<code class="literal">--stripesize <span class="emphasis"><em>StripeSize</em></span></code>
							</p>
							 </td><td style="text-align: left" valign="top"> <p>
								Specifies the size of each stripe in kilobytes. This is the amount of data that is written to one device before moving to the next device.
							</p>
							 </td></tr><tr><td style="text-align: left" valign="top"> <p>
								<code class="literal"><span class="emphasis"><em>VolumeGroup</em></span></code>
							</p>
							 </td><td style="text-align: left" valign="top"> <p>
								Specifies the volume group to use.
							</p>
							 </td></tr><tr><td style="text-align: left" valign="top"> <p>
								<code class="literal"><span class="emphasis"><em>PhysicalVolumePath</em></span></code> …​
							</p>
							 </td><td style="text-align: left" valign="top"> <p>
								Specifies the devices to use. If this is not specified, LVM will choose the number of devices specified by the <span class="emphasis"><em>Stripes</em></span> option, one for each stripe.
							</p>
							 </td></tr></tbody></table></div></div><p>
				This example procedure creates an LVM RAID0 logical volume called <code class="literal">mylv</code> that stripes data across the disks at <code class="literal">/dev/sda1</code>, <code class="literal">/dev/sdb1</code>, and <code class="literal">/dev/sdc1</code>.
			</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p class="simpara">
						Label the disks you will use in the volume group as LVM physical volumes with the <code class="literal">pvcreate</code> command.
					</p><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>
							This command destroys any data on <code class="literal">/dev/sda1</code>, <code class="literal">/dev/sdb1</code>, and <code class="literal">/dev/sdc1</code>.
						</p></div><pre class="literallayout"># <code class="literal">pvcreate /dev/sda1 /dev/sdb1 /dev/sdc1</code>
  Physical volume "/dev/sda1" successfully created
  Physical volume "/dev/sdb1" successfully created
  Physical volume "/dev/sdc1" successfully created</pre></li><li class="listitem"><p class="simpara">
						Create the volume group <code class="literal">myvg</code>. The following command creates the volume group <code class="literal">myvg</code>.
					</p><pre class="literallayout"># <code class="literal">vgcreate myvg /dev/sda1 /dev/sdb1 /dev/sdc1</code>
  Volume group "myvg" successfully created</pre><p class="simpara">
						You can use the <code class="literal">vgs</code> command to display the attributes of the new volume group.
					</p><pre class="literallayout"># <code class="literal">vgs</code>
  VG   #PV #LV #SN Attr   VSize  VFree
  myvg   3   0   0 wz--n- 51.45G 51.45G</pre></li><li class="listitem"><p class="simpara">
						Create a RAID0 logical volume from the volume group you have created. The following command creates the RAID0 volume <code class="literal">mylv</code> from the volume group <code class="literal">myvg</code>. This example creates a logical volume that is 2 gigabytes in size, with three stripes and a stripe size of 4 kilobytes.
					</p><pre class="literallayout"># <code class="literal">lvcreate --type raid0 -L 2G --stripes 3 --stripesize 4 -n mylv myvg</code>
  Rounding size 2.00 GiB (512 extents) up to stripe boundary size 2.00 GiB(513 extents).
  Logical volume "mylv" created.</pre></li><li class="listitem"><p class="simpara">
						Create a file system on the RAID0 logical volume. The following command creates an <code class="literal">ext4</code> file system on the logical volume.
					</p><pre class="literallayout"># <code class="literal">mkfs.ext4 /dev/myvg/mylv</code>
mke2fs 1.44.3 (10-July-2018)
Creating filesystem with 525312 4k blocks and 131376 inodes
Filesystem UUID: 9d4c0704-6028-450a-8b0a-8875358c0511
Superblock backups stored on blocks:
        32768, 98304, 163840, 229376, 294912

Allocating group tables: done
Writing inode tables: done
Creating journal (16384 blocks): done
Writing superblocks and filesystem accounting information: done</pre><p class="simpara">
						The following commands mount the logical volume and report the file system disk space usage.
					</p><pre class="literallayout"># <code class="literal">mount /dev/myvg/mylv /mnt</code>
# <code class="literal">df</code>
Filesystem             1K-blocks     Used  Available Use% Mounted on
/dev/mapper/myvg-mylv    2002684     6168    1875072   1% /mnt</pre></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_controlling-raid-initialization-configure-manage-raid"/>Controlling the rate at which RAID volumes are initialized</h1></div></div></div><p>
				When you create RAID10 logical volumes, the background I/O required to initialize the logical volumes with a <code class="literal">sync</code> operation can crowd out other I/O operations to LVM devices, such as updates to volume group metadata, particularly when you are creating many RAID logical volumes. This can cause the other LVM operations to slow down.
			</p><p>
				You can control the rate at which a RAID logical volume is initialized by implementing recovery throttling. You control the rate at which <code class="literal">sync</code> operations are performed by setting the minimum and maximum I/O rate for those operations with the <code class="literal">--minrecoveryrate</code> and <code class="literal">--maxrecoveryrate</code> options of the <code class="literal">lvcreate</code> command. You specify these options as follows.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p class="simpara">
						<code class="literal">--maxrecoveryrate <span class="emphasis"><em>Rate</em></span>[bBsSkKmMgG]</code>
					</p><p class="simpara">
						Sets the maximum recovery rate for a RAID logical volume so that it will not crowd out nominal I/O operations. The <span class="emphasis"><em>Rate</em></span> is specified as an amount per second for each device in the array. If no suffix is given, then kiB/sec/device is assumed. Setting the recovery rate to 0 means it will be unbounded.
					</p></li><li class="listitem"><p class="simpara">
						<code class="literal">--minrecoveryrate <span class="emphasis"><em>Rate</em></span>[bBsSkKmMgG]</code>
					</p><p class="simpara">
						Sets the minimum recovery rate for a RAID logical volume to ensure that I/O for <code class="literal">sync</code> operations achieves a minimum throughput, even when heavy nominal I/O is present. The <span class="emphasis"><em>Rate</em></span> is specified as an amount per second for each device in the array. If no suffix is given, then kiB/sec/device is assumed.
					</p></li></ul></div><p>
				The following command creates a 2-way RAID10 array with 3 stripes that is 10 gigabytes in size with a maximum recovery rate of 128 kiB/sec/device. The array is named <code class="literal">my_lv</code> and is in the volume group <code class="literal">my_vg</code>.
			</p><pre class="literallayout"># <code class="literal">lvcreate --type raid10 -i 2 -m 1 -L 10G --maxrecoveryrate 128 -n my_lv my_vg</code></pre><p>
				You can also specify minimum and maximum recovery rates for a RAID scrubbing operation.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_converting-linear-to-raid-configure-manage-raid"/>Converting a Linear device to a RAID device</h1></div></div></div><p>
				You can convert an existing linear logical volume to a RAID device by using the <code class="literal">--type</code> argument of the <code class="literal">lvconvert</code> command.
			</p><p>
				The following command converts the linear logical volume <code class="literal">my_lv</code> in volume group <code class="literal">my_vg</code> to a 2-way RAID1 array.
			</p><pre class="literallayout"># <code class="literal">lvconvert --type raid1 -m 1 my_vg/my_lv</code></pre><p>
				Since RAID logical volumes are composed of metadata and data subvolume pairs, when you convert a linear device to a RAID1 array, a new metadata subvolume is created and associated with the original logical volume on (one of) the same physical volumes that the linear volume is on. The additional images are added in metadata/data subvolume pairs. For example, if the original device is as follows:
			</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV     Copy%  Devices
  my_lv         /dev/sde1(0)</pre><p>
				After conversion to a 2-way RAID1 array the device contains the following data and metadata subvolume pairs:
			</p><pre class="literallayout"># <code class="literal">lvconvert --type raid1 -m 1 my_vg/my_lv</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            6.25   my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sde1(0)
  [my_lv_rimage_1]        /dev/sdf1(1)
  [my_lv_rmeta_0]         /dev/sde1(256)
  [my_lv_rmeta_1]         /dev/sdf1(0)</pre><p>
				If the metadata image that pairs with the original logical volume cannot be placed on the same physical volume, the <code class="literal">lvconvert</code> will fail.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc-converting-raid1-to-linear-configure-manage-raid"/>Converting an LVM RAID1 logical volume to an LVM linear logical volume</h1></div></div></div><p>
				You can convert an existing RAID1 LVM logical volume to an LVM linear logical volume with the <code class="literal">lvconvert</code> command by specifying the <code class="literal">-m0</code> argument. This removes all the RAID data subvolumes and all the RAID metadata subvolumes that make up the RAID array, leaving the top-level RAID1 image as the linear logical volume.
			</p><p>
				The following example displays an existing LVM RAID1 logical volume.
			</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sde1(1)
  [my_lv_rimage_1]        /dev/sdf1(1)
  [my_lv_rmeta_0]         /dev/sde1(0)
  [my_lv_rmeta_1]         /dev/sdf1(0)</pre><p>
				The following command converts the LVM RAID1 logical volume <code class="literal">my_vg/my_lv</code> to an LVM linear device.
			</p><pre class="literallayout"># <code class="literal">lvconvert -m0 my_vg/my_lv</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV      Copy%  Devices
  my_lv          /dev/sde1(1)</pre><p>
				When you convert an LVM RAID1 logical volume to an LVM linear volume, you can specify which physical volumes to remove. The following example shows the layout of an LVM RAID1 logical volume made up of two images: <code class="literal">/dev/sda1</code> and <code class="literal">/dev/sdb1</code>. In this example, the <code class="literal">lvconvert</code> command specifies that you want to remove <code class="literal">/dev/sda1</code>, leaving <code class="literal">/dev/sdb1</code> as the physical volume that makes up the linear device.
			</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sda1(1)
  [my_lv_rimage_1]        /dev/sdb1(1)
  [my_lv_rmeta_0]         /dev/sda1(0)
  [my_lv_rmeta_1]         /dev/sdb1(0)
# <code class="literal">lvconvert -m0 my_vg/my_lv /dev/sda1</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV    Copy%  Devices
  my_lv        /dev/sdb1(1)</pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc-converting-mirror-to-raid-configure-manage-raid"/>Converting a mirrored LVM device to a RAID1 device</h1></div></div></div><p>
				You can convert an existing mirrored LVM device with a segment type of <code class="literal">mirror</code> to a RAID1 LVM device with the <code class="literal">lvconvert</code> command by specifying the <code class="literal">--type raid1</code> argument. This renames the mirror subvolumes (<code class="literal"><span class="strong"><strong><span class="emphasis"><em>mimage</em></span></strong></span></code>) to RAID subvolumes (<code class="literal"><span class="strong"><strong><span class="emphasis"><em>rimage</em></span></strong></span></code>). In addition, the mirror log is removed and metadata subvolumes (<code class="literal"><span class="strong"><strong><span class="emphasis"><em>rmeta</em></span></strong></span></code>) are created for the data subvolumes on the same physical volumes as the corresponding data subvolumes.
			</p><p>
				The following example shows the layout of a mirrored logical volume <code class="literal">my_vg/my_lv</code>.
			</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv             15.20 my_lv_mimage_0(0),my_lv_mimage_1(0)
  [my_lv_mimage_0]        /dev/sde1(0)
  [my_lv_mimage_1]        /dev/sdf1(0)
  [my_lv_mlog]            /dev/sdd1(0)</pre><p>
				The following command converts the mirrored logical volume <code class="literal">my_vg/my_lv</code> to a RAID1 logical volume.
			</p><pre class="literallayout"># <code class="literal">lvconvert --type raid1 my_vg/my_lv</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sde1(0)
  [my_lv_rimage_1]        /dev/sdf1(0)
  [my_lv_rmeta_0]         /dev/sde1(125)
  [my_lv_rmeta_1]         /dev/sdf1(125)</pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc-resizing-raid-configure-manage-raid"/>Resizing a RAID logical volume</h1></div></div></div><p>
				You can resize a RAID logical volume in the following ways;
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						You can increase the size of a RAID logical volume of any type with the <code class="literal">lvresize</code> or <code class="literal">lvextend</code> command. This does not change the number of RAID images. For striped RAID logical volumes the same stripe rounding constraints apply as when you create a striped RAID logical volume.
					</li></ul></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						You can reduce the size of a RAID logical volume of any type with the <code class="literal">lvresize</code> or <code class="literal">lvreduce</code> command. This does not change the number of RAID images. As with the <code class="literal">lvextend</code> command, the same stripe rounding constraints apply as when you create a striped RAID logical volume.
					</li></ul></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						You can change the number of stripes on a striped RAID logical volume (<code class="literal">raid4/5/6/10</code>) with the <code class="literal">--stripes N</code> parameter of the <code class="literal">lvconvert</code> command. This increases or reduces the size of the RAID logical volume by the capacity of the stripes added or removed. Note that <code class="literal">raid10</code> volumes are capable only of adding stripes. This capability is part of the RAID <span class="emphasis"><em>reshaping</em></span> feature that allows you to change attributes of a RAID logical volume while keeping the same RAID level. For information on RAID reshaping and examples of using the <code class="literal">lvconvert</code> command to reshape a RAID logical volume, see the <code class="literal">lvmraid</code>(7) man page.
					</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc-changing-raid-imagenum-configure-manage-raid"/>Changing the number of images in an existing RAID1 device</h1></div></div></div><p>
				You can change the number of images in an existing RAID1 array just as you can change the number of images in the earlier implementation of LVM mirroring. Use the <code class="literal">lvconvert</code> command to specify the number of additional metadata/data subvolume pairs to add or remove.
			</p><p>
				When you add images to a RAID1 device with the <code class="literal">lvconvert</code> command, you can specify the total number of images for the resulting device, or you can specify how many images to add to the device. You can also optionally specify on which physical volumes the new metadata/data image pairs will reside.
			</p><p>
				Metadata subvolumes (named <code class="literal"><span class="strong"><strong><span class="emphasis"><em>rmeta</em></span></strong></span></code>) always exist on the same physical devices as their data subvolume counterparts <code class="literal"><span class="strong"><strong><span class="emphasis"><em>rimage</em></span></strong></span></code>). The metadata/data subvolume pairs will not be created on the same physical volumes as those from another metadata/data subvolume pair in the RAID array (unless you specify <code class="literal">--alloc anywhere</code>).
			</p><p>
				The format for the command to add images to a RAID1 volume is as follows:
			</p><pre class="literallayout">lvconvert -m <span class="emphasis"><em>new_absolute_count</em></span> vg/lv [<span class="emphasis"><em>removable_PVs</em></span>]
lvconvert -m +<span class="emphasis"><em>num_additional_images</em></span> vg/lv [<span class="emphasis"><em>removable_PVs</em></span>]</pre><p>
				For example, the following command displays the LVM device <code class="literal">my_vg/my_lv</code>, which is a 2-way RAID1 array:
			</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV                Copy%  Devices
  my_lv             6.25    my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]         /dev/sde1(0)
  [my_lv_rimage_1]         /dev/sdf1(1)
  [my_lv_rmeta_0]          /dev/sde1(256)
  [my_lv_rmeta_1]          /dev/sdf1(0)</pre><p>
				The following command converts the 2-way RAID1 device <code class="literal">my_vg/my_lv</code> to a 3-way RAID1 device:
			</p><pre class="literallayout"># <code class="literal">lvconvert -m 2 my_vg/my_lv</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv              6.25 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]        /dev/sde1(0)
  [my_lv_rimage_1]        /dev/sdf1(1)
  [my_lv_rimage_2]        /dev/sdg1(1)
  [my_lv_rmeta_0]         /dev/sde1(256)
  [my_lv_rmeta_1]         /dev/sdf1(0)
  [my_lv_rmeta_2]         /dev/sdg1(0)</pre><p>
				When you add an image to a RAID1 array, you can specify which physical volumes to use for the image. The following command converts the 2-way RAID1 device <code class="literal">my_vg/my_lv</code> to a 3-way RAID1 device, specifying that the physical volume <code class="literal">/dev/sdd1</code> be used for the array:
			</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv             56.00 my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sda1(1)
  [my_lv_rimage_1]        /dev/sdb1(1)
  [my_lv_rmeta_0]         /dev/sda1(0)
  [my_lv_rmeta_1]         /dev/sdb1(0)
# <code class="literal">lvconvert -m 2 my_vg/my_lv /dev/sdd1</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv             28.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]        /dev/sda1(1)
  [my_lv_rimage_1]        /dev/sdb1(1)
  [my_lv_rimage_2]        /dev/sdd1(1)
  [my_lv_rmeta_0]         /dev/sda1(0)
  [my_lv_rmeta_1]         /dev/sdb1(0)
  [my_lv_rmeta_2]         /dev/sdd1(0)</pre><p>
				To remove images from a RAID1 array, use the following command. When you remove images from a RAID1 device with the <code class="literal">lvconvert</code> command, you can specify the total number of images for the resulting device, or you can specify how many images to remove from the device. You can also optionally specify the physical volumes from which to remove the device.
			</p><pre class="literallayout">lvconvert -m <span class="emphasis"><em>new_absolute_count</em></span> <span class="emphasis"><em>vg/lv</em></span> [<span class="emphasis"><em>removable_PVs</em></span>]
lvconvert -m -<span class="emphasis"><em>num_fewer_images</em></span> <span class="emphasis"><em>vg/lv</em></span> [<span class="emphasis"><em>removable_PVs</em></span>]</pre><p>
				Additionally, when an image and its associated metadata subvolume volume are removed, any higher-numbered images will be shifted down to fill the slot. If you remove <code class="literal">lv_rimage_1</code> from a 3-way RAID1 array that consists of <code class="literal">lv_rimage_0</code>, <code class="literal">lv_rimage_1</code>, and <code class="literal">lv_rimage_2</code>, this results in a RAID1 array that consists of <code class="literal">lv_rimage_0</code> and <code class="literal">lv_rimage_1</code>. The subvolume <code class="literal">lv_rimage_2</code> will be renamed and take over the empty slot, becoming <code class="literal">lv_rimage_1</code>.
			</p><p>
				The following example shows the layout of a 3-way RAID1 logical volume <code class="literal">my_vg/my_lv</code>.
			</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]        /dev/sde1(1)
  [my_lv_rimage_1]        /dev/sdf1(1)
  [my_lv_rimage_2]        /dev/sdg1(1)
  [my_lv_rmeta_0]         /dev/sde1(0)
  [my_lv_rmeta_1]         /dev/sdf1(0)
  [my_lv_rmeta_2]         /dev/sdg1(0)</pre><p>
				The following command converts the 3-way RAID1 logical volume into a 2-way RAID1 logical volume.
			</p><pre class="literallayout"># <code class="literal">lvconvert -m1 my_vg/my_lv</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sde1(1)
  [my_lv_rimage_1]        /dev/sdf1(1)
  [my_lv_rmeta_0]         /dev/sde1(0)
  [my_lv_rmeta_1]         /dev/sdf1(0)</pre><p>
				The following command converts the 3-way RAID1 logical volume into a 2-way RAID1 logical volume, specifying the physical volume that contains the image to remove as <code class="literal">/dev/sde1</code>.
			</p><pre class="literallayout"># <code class="literal">lvconvert -m1 my_vg/my_lv /dev/sde1</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sdf1(1)
  [my_lv_rimage_1]        /dev/sdg1(1)
  [my_lv_rmeta_0]         /dev/sdf1(0)
  [my_lv_rmeta_1]         /dev/sdg1(0)</pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc-splitting-raid-image-configure-manage-raid"/>Splitting off a RAID image as a separate logical volume</h1></div></div></div><p>
				You can split off an image of a RAID logical volume to form a new logical volume.
			</p><p>
				The format of the command to split off a RAID image is as follows:
			</p><pre class="literallayout">lvconvert --splitmirrors <span class="emphasis"><em>count</em></span> -n <span class="emphasis"><em>splitname</em></span> <span class="emphasis"><em>vg/lv</em></span> [<span class="emphasis"><em>removable_PVs</em></span>]</pre><p>
				Just as when you are removing a RAID image from an existing RAID1 logical volume, when you remove a RAID data subvolume (and its associated metadata subvolume) from the middle of the device any higher numbered images will be shifted down to fill the slot. The index numbers on the logical volumes that make up a RAID array will thus be an unbroken sequence of integers.
			</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
					You cannot split off a RAID image if the RAID1 array is not yet in sync.
				</p></div><p>
				The following example splits a 2-way RAID1 logical volume, <code class="literal">my_lv</code>, into two linear logical volumes, <code class="literal">my_lv</code> and <code class="literal">new</code>.
			</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv             12.00 my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sde1(1)
  [my_lv_rimage_1]        /dev/sdf1(1)
  [my_lv_rmeta_0]         /dev/sde1(0)
  [my_lv_rmeta_1]         /dev/sdf1(0)
# <code class="literal">lvconvert --splitmirror 1 -n new my_vg/my_lv</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV      Copy%  Devices
  my_lv          /dev/sde1(1)
  new            /dev/sdf1(1)</pre><p>
				The following example splits a 3-way RAID1 logical volume, <code class="literal">my_lv</code>, into a 2-way RAID1 logical volume, <code class="literal">my_lv</code>, and a linear logical volume, <code class="literal">new</code>
			</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]        /dev/sde1(1)
  [my_lv_rimage_1]        /dev/sdf1(1)
  [my_lv_rimage_2]        /dev/sdg1(1)
  [my_lv_rmeta_0]         /dev/sde1(0)
  [my_lv_rmeta_1]         /dev/sdf1(0)
  [my_lv_rmeta_2]         /dev/sdg1(0)
# <code class="literal">lvconvert --splitmirror 1 -n new my_vg/my_lv</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV            Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sde1(1)
  [my_lv_rimage_1]        /dev/sdf1(1)
  [my_lv_rmeta_0]         /dev/sde1(0)
  [my_lv_rmeta_1]         /dev/sdf1(0)
  new                     /dev/sdg1(1)</pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc-split-merge-raid-image-configure-manage-raid"/>Splitting and Merging a RAID Image</h1></div></div></div><p>
				You can temporarily split off an image of a RAID1 array for read-only use while keeping track of any changes by using the <code class="literal">--trackchanges</code> argument in conjunction with the <code class="literal">--splitmirrors</code> argument of the <code class="literal">lvconvert</code> command. This allows you to merge the image back into the array at a later time while resyncing only those portions of the array that have changed since the image was split.
			</p><p>
				The format for the <code class="literal">lvconvert</code> command to split off a RAID image is as follows.
			</p><pre class="literallayout">lvconvert --splitmirrors <span class="emphasis"><em>count</em></span> --trackchanges <span class="emphasis"><em>vg/lv</em></span> [<span class="emphasis"><em>removable_PVs</em></span>]</pre><p>
				When you split off a RAID image with the <code class="literal">--trackchanges</code> argument, you can specify which image to split but you cannot change the name of the volume being split. In addition, the resulting volumes have the following constraints.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						The new volume you create is read-only.
					</li><li class="listitem">
						You cannot resize the new volume.
					</li><li class="listitem">
						You cannot rename the remaining array.
					</li><li class="listitem">
						You cannot resize the remaining array.
					</li><li class="listitem">
						You can activate the new volume and the remaining array independently.
					</li></ul></div><p>
				You can merge an image that was split off with the <code class="literal">--trackchanges</code> argument specified by executing a subsequent <code class="literal">lvconvert</code> command with the <code class="literal">--merge</code> argument. When you merge the image, only the portions of the array that have changed since the image was split are resynced.
			</p><p>
				The format for the <code class="literal">lvconvert</code> command to merge a RAID image is as follows.
			</p><pre class="literallayout">lvconvert --merge <span class="emphasis"><em>raid_image</em></span></pre><p>
				The following example creates a RAID1 logical volume and then splits off an image from that volume while tracking changes to the remaining array.
			</p><pre class="literallayout"># <code class="literal">lvcreate --type raid1 -m 2 -L 1G -n my_lv my_vg</code>
  Logical volume "my_lv" created
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]        /dev/sdb1(1)
  [my_lv_rimage_1]        /dev/sdc1(1)
  [my_lv_rimage_2]        /dev/sdd1(1)
  [my_lv_rmeta_0]         /dev/sdb1(0)
  [my_lv_rmeta_1]         /dev/sdc1(0)
  [my_lv_rmeta_2]         /dev/sdd1(0)
# <code class="literal">lvconvert --splitmirrors 1 --trackchanges my_vg/my_lv</code>
  my_lv_rimage_2 split from my_lv for read-only purposes.
  Use 'lvconvert --merge my_vg/my_lv_rimage_2' to merge back into my_lv
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]        /dev/sdb1(1)
  [my_lv_rimage_1]        /dev/sdc1(1)
  my_lv_rimage_2          /dev/sdd1(1)
  [my_lv_rmeta_0]         /dev/sdb1(0)
  [my_lv_rmeta_1]         /dev/sdc1(0)
  [my_lv_rmeta_2]         /dev/sdd1(0)</pre><p>
				The following example splits off an image from a RAID1 volume while tracking changes to the remaining array, then merges the volume back into the array.
			</p><pre class="literallayout"># <code class="literal">lvconvert --splitmirrors 1 --trackchanges my_vg/my_lv</code>
  lv_rimage_1 split from my_lv for read-only purposes.
  Use 'lvconvert --merge my_vg/my_lv_rimage_1' to merge back into my_lv
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sdc1(1)
  my_lv_rimage_1          /dev/sdd1(1)
  [my_lv_rmeta_0]         /dev/sdc1(0)
  [my_lv_rmeta_1]         /dev/sdd1(0)
# <code class="literal">lvconvert --merge my_vg/my_lv_rimage_1</code>
  my_vg/my_lv_rimage_1 successfully merged back into my_vg/my_lv
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sdc1(1)
  [my_lv_rimage_1]        /dev/sdd1(1)
  [my_lv_rmeta_0]         /dev/sdc1(0)
  [my_lv_rmeta_1]         /dev/sdd1(0)</pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc-setting-raid-fault-policy-configure-manage-raid"/>Setting a RAID fault policy</h1></div></div></div><p>
				LVM RAID handles device failures in an automatic fashion based on the preferences defined by the <code class="literal">raid_fault_policy</code> field in the <code class="literal">lvm.conf</code> file.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						If the <code class="literal">raid_fault_policy</code> field is set to <code class="literal">allocate</code>, the system will attempt to replace the failed device with a spare device from the volume group. If there is no available spare device, this will be reported to the system log.
					</li><li class="listitem">
						If the <code class="literal">raid_fault_policy</code> field is set to <code class="literal">warn</code>, the system will produce a warning and the log will indicate that a device has failed. This allows the user to determine the course of action to take.
					</li></ul></div><p>
				As long as there are enough devices remaining to support usability, the RAID logical volume will continue to operate.
			</p><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="raid-allocate-faultpolicy"/>The allocate RAID Fault Policy</h2></div></div></div><p>
					In the following example, the <code class="literal">raid_fault_policy</code> field has been set to <code class="literal">allocate</code> in the <code class="literal">lvm.conf</code> file. The RAID logical volume is laid out as follows.
				</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]        /dev/sde1(1)
  [my_lv_rimage_1]        /dev/sdf1(1)
  [my_lv_rimage_2]        /dev/sdg1(1)
  [my_lv_rmeta_0]         /dev/sde1(0)
  [my_lv_rmeta_1]         /dev/sdf1(0)
  [my_lv_rmeta_2]         /dev/sdg1(0)</pre><p>
					If the <code class="literal">/dev/sde</code> device fails, the system log will display error messages.
				</p><pre class="literallayout"># <code class="literal">grep lvm /var/log/messages</code>
Jan 17 15:57:18 bp-01 lvm[8599]: Device #0 of raid1 array, my_vg-my_lv, has failed.
Jan 17 15:57:18 bp-01 lvm[8599]: /dev/sde1: read failed after 0 of 2048 at
250994294784: Input/output error
Jan 17 15:57:18 bp-01 lvm[8599]: /dev/sde1: read failed after 0 of 2048 at
250994376704: Input/output error
Jan 17 15:57:18 bp-01 lvm[8599]: /dev/sde1: read failed after 0 of 2048 at 0:
Input/output error
Jan 17 15:57:18 bp-01 lvm[8599]: /dev/sde1: read failed after 0 of 2048 at
4096: Input/output error
Jan 17 15:57:19 bp-01 lvm[8599]: Couldn't find device with uuid
3lugiV-3eSP-AFAR-sdrP-H20O-wM2M-qdMANy.
Jan 17 15:57:27 bp-01 lvm[8599]: raid1 array, my_vg-my_lv, is not in-sync.
Jan 17 15:57:36 bp-01 lvm[8599]: raid1 array, my_vg-my_lv, is now in-sync.</pre><p>
					Since the <code class="literal">raid_fault_policy</code> field has been set to <code class="literal">allocate</code>, the failed device is replaced with a new device from the volume group.
				</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices vg</code>
  Couldn't find device with uuid 3lugiV-3eSP-AFAR-sdrP-H20O-wM2M-qdMANy.
  LV            Copy%  Devices
  lv            100.00 lv_rimage_0(0),lv_rimage_1(0),lv_rimage_2(0)
  [lv_rimage_0]        /dev/sdh1(1)
  [lv_rimage_1]        /dev/sdf1(1)
  [lv_rimage_2]        /dev/sdg1(1)
  [lv_rmeta_0]         /dev/sdh1(0)
  [lv_rmeta_1]         /dev/sdf1(0)
  [lv_rmeta_2]         /dev/sdg1(0)</pre><p>
					Note that even though the failed device has been replaced, the display still indicates that LVM could not find the failed device. This is because, although the failed device has been removed from the RAID logical volume, the failed device has not yet been removed from the volume group. To remove the failed device from the volume group, you can execute <code class="literal">vgreduce --removemissing <span class="emphasis"><em>VG</em></span></code>.
				</p><p>
					If the <code class="literal">raid_fault_policy</code> has been set to <code class="literal">allocate</code> but there are no spare devices, the allocation will fail, leaving the logical volume as it is. If the allocation fails, you have the option of fixing the drive, then initiating recovery of the failed device with the <code class="literal">--refresh</code> option of the <code class="literal">lvchange</code> command. Alternately, you can replace the failed device.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="raid-warn-faultpolicy"/>The warn RAID Fault Policy</h2></div></div></div><p>
					In the following example, the <code class="literal">raid_fault_policy</code> field has been set to <code class="literal">warn</code> in the <code class="literal">lvm.conf</code> file. The RAID logical volume is laid out as follows.
				</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]        /dev/sdh1(1)
  [my_lv_rimage_1]        /dev/sdf1(1)
  [my_lv_rimage_2]        /dev/sdg1(1)
  [my_lv_rmeta_0]         /dev/sdh1(0)
  [my_lv_rmeta_1]         /dev/sdf1(0)
  [my_lv_rmeta_2]         /dev/sdg1(0)</pre><p>
					If the <code class="literal">/dev/sdh</code> device fails, the system log will display error messages. In this case, however, LVM will not automatically attempt to repair the RAID device by replacing one of the images. Instead, if the device has failed you can replace the device with the <code class="literal">--repair</code> argument of the <code class="literal">lvconvert</code> command.
				</p></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_replacing-a-raid-device-configure-manage-raid"/>Replacing a RAID device in a logical volume</h1></div></div></div><p>
				You can replace a RAID device in a logical volume with the <code class="literal">lvconvert</code> command.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						If there has been no failure on a RAID device, use the <code class="literal">--replace</code> argument of the <code class="literal">lvconvert</code> command to replace the device.
					</li><li class="listitem">
						If the RAID device has failed, use the <code class="literal">--repair</code> argument of the <code class="literal">lvconvert</code> command to replace the failed device.
					</li></ul></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="proc-replacing-raid-device-with-no-failure-replacing-a-raid-device"/>Replacing a RAID device that has not failed</h2></div></div></div><p>
					To replace a RAID device in a logical volume, use the <code class="literal">--replace</code> argument of the <code class="literal">lvconvert</code> command. Note that this command will not work if the RAID device has failed.
				</p><p>
					The format for the <code class="literal">lvconvert --replace</code> command is as follows.
				</p><pre class="literallayout">lvconvert --replace <span class="emphasis"><em>dev_to_remove</em></span> <span class="emphasis"><em>vg/lv</em></span> [<span class="emphasis"><em>possible_replacements</em></span>]</pre><p>
					The following example creates a RAID1 logical volume and then replaces a device in that volume.
				</p><pre class="literallayout"># <code class="literal">lvcreate --type raid1 -m 2 -L 1G -n my_lv my_vg</code>
  Logical volume "my_lv" created
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]        /dev/sdb1(1)
  [my_lv_rimage_1]        /dev/sdb2(1)
  [my_lv_rimage_2]        /dev/sdc1(1)
  [my_lv_rmeta_0]         /dev/sdb1(0)
  [my_lv_rmeta_1]         /dev/sdb2(0)
  [my_lv_rmeta_2]         /dev/sdc1(0)
# <code class="literal">lvconvert --replace /dev/sdb2 my_vg/my_lv</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv             37.50 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]        /dev/sdb1(1)
  [my_lv_rimage_1]        /dev/sdc2(1)
  [my_lv_rimage_2]        /dev/sdc1(1)
  [my_lv_rmeta_0]         /dev/sdb1(0)
  [my_lv_rmeta_1]         /dev/sdc2(0)
  [my_lv_rmeta_2]         /dev/sdc1(0)</pre><p>
					The following example creates a RAID1 logical volume and then replaces a device in that volume, specifying which physical volume to use for the replacement.
				</p><pre class="literallayout"># <code class="literal">lvcreate --type raid1 -m 1 -L 100 -n my_lv my_vg</code>
  Logical volume "my_lv" created
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sda1(1)
  [my_lv_rimage_1]        /dev/sdb1(1)
  [my_lv_rmeta_0]         /dev/sda1(0)
  [my_lv_rmeta_1]         /dev/sdb1(0)
# <code class="literal">pvs</code>
  PV          VG       Fmt  Attr PSize    PFree
  /dev/sda1   my_vg    lvm2 a--  1020.00m  916.00m
  /dev/sdb1   my_vg    lvm2 a--  1020.00m  916.00m
  /dev/sdc1   my_vg    lvm2 a--  1020.00m 1020.00m
  /dev/sdd1   my_vg    lvm2 a--  1020.00m 1020.00m
# <code class="literal">lvconvert --replace /dev/sdb1 my_vg/my_lv /dev/sdd1</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv             28.00 my_lv_rimage_0(0),my_lv_rimage_1(0)
  [my_lv_rimage_0]        /dev/sda1(1)
  [my_lv_rimage_1]        /dev/sdd1(1)
  [my_lv_rmeta_0]         /dev/sda1(0)
  [my_lv_rmeta_1]         /dev/sdd1(0)</pre><p>
					You can replace more than one RAID device at a time by specifying multiple <code class="literal">replace</code> arguments, as in the following example.
				</p><pre class="literallayout"># <code class="literal">lvcreate --type raid1 -m 2 -L 100 -n my_lv my_vg</code>
  Logical volume "my_lv" created
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv            100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]        /dev/sda1(1)
  [my_lv_rimage_1]        /dev/sdb1(1)
  [my_lv_rimage_2]        /dev/sdc1(1)
  [my_lv_rmeta_0]         /dev/sda1(0)
  [my_lv_rmeta_1]         /dev/sdb1(0)
  [my_lv_rmeta_2]         /dev/sdc1(0)
# <code class="literal">lvconvert --replace /dev/sdb1 --replace /dev/sdc1 my_vg/my_lv</code>
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Copy%  Devices
  my_lv             60.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]        /dev/sda1(1)
  [my_lv_rimage_1]        /dev/sdd1(1)
  [my_lv_rimage_2]        /dev/sde1(1)
  [my_lv_rmeta_0]         /dev/sda1(0)
  [my_lv_rmeta_1]         /dev/sdd1(0)
  [my_lv_rmeta_2]         /dev/sde1(0)</pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="proc-replacing-failed-raid-device-replacing-a-raid-device"/>Replacing a failed RAID device in a logical volume</h2></div></div></div><p>
					RAID is not like traditional LVM mirroring. LVM mirroring required failed devices to be removed or the mirrored logical volume would hang. RAID arrays can keep on running with failed devices. In fact, for RAID types other than RAID1, removing a device would mean converting to a lower level RAID (for example, from RAID6 to RAID5, or from RAID4 or RAID5 to RAID0). Therefore, rather than removing a failed device unconditionally and potentially allocating a replacement, LVM allows you to replace a failed device in a RAID volume in a one-step solution by using the <code class="literal">--repair</code> argument of the <code class="literal">lvconvert</code> command.
				</p><p>
					In the following example, a RAID logical volume is laid out as follows.
				</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  LV               Cpy%Sync Devices
  my_lv            100.00   my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]          /dev/sde1(1)
  [my_lv_rimage_1]          /dev/sdc1(1)
  [my_lv_rimage_2]          /dev/sdd1(1)
  [my_lv_rmeta_0]           /dev/sde1(0)
  [my_lv_rmeta_1]           /dev/sdc1(0)
  [my_lv_rmeta_2]           /dev/sdd1(0)</pre><p>
					If the <code class="literal">/dev/sdc</code> device fails, the output of the <code class="literal">lvs</code> command is as follows.
				</p><pre class="literallayout"># <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  /dev/sdc: open failed: No such device or address
  Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee.
  WARNING: Couldn't find all devices for LV my_vg/my_lv_rimage_1 while checking used and assumed devices.
  WARNING: Couldn't find all devices for LV my_vg/my_lv_rmeta_1 while checking used and assumed devices.
  LV               Cpy%Sync Devices
  my_lv            100.00   my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]          /dev/sde1(1)
  [my_lv_rimage_1]          [unknown](1)
  [my_lv_rimage_2]          /dev/sdd1(1)
  [my_lv_rmeta_0]           /dev/sde1(0)
  [my_lv_rmeta_1]           [unknown](0)
  [my_lv_rmeta_2]           /dev/sdd1(0)</pre><p>
					Use the following commands to replace the failed device and display the logical volume.
				</p><pre class="literallayout"># <code class="literal">lvconvert --repair my_vg/my_lv</code>
  /dev/sdc: open failed: No such device or address
  Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee.
  WARNING: Couldn't find all devices for LV my_vg/my_lv_rimage_1 while checking used and assumed devices.
  WARNING: Couldn't find all devices for LV my_vg/my_lv_rmeta_1 while checking used and assumed devices.
Attempt to replace failed RAID images (requires full device resync)? [y/n]: y
  Faulty devices in my_vg/my_lv successfully replaced.
# <code class="literal">lvs -a -o name,copy_percent,devices my_vg</code>
  /dev/sdc: open failed: No such device or address
  /dev/sdc1: open failed: No such device or address
  Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee.
  LV               Cpy%Sync Devices
  my_lv            43.79    my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0)
  [my_lv_rimage_0]          /dev/sde1(1)
  [my_lv_rimage_1]          /dev/sdb1(1)
  [my_lv_rimage_2]          /dev/sdd1(1)
  [my_lv_rmeta_0]           /dev/sde1(0)
  [my_lv_rmeta_1]           /dev/sdb1(0)
  [my_lv_rmeta_2]           /dev/sdd1(0)</pre><p>
					Note that even though the failed device has been replaced, the display still indicates that LVM could not find the failed device. This is because, although the failed device has been removed from the RAID logical volume, the failed device has not yet been removed from the volume group. To remove the failed device from the volume group, you can execute <code class="literal">vgreduce --removemissing <span class="emphasis"><em>VG</em></span></code>.
				</p><p>
					If the device failure is a transient failure or you are able to repair the device that failed, you can initiate recovery of the failed device with the <code class="literal">--refresh</code> option of the <code class="literal">lvchange</code> command.
				</p><p>
					The following command refreshes a logical volume.
				</p><pre class="literallayout"># <code class="literal">lvchange --refresh my_vg/my_lv</code></pre></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc-scrubbing-raid-volume-configure-manage-raid"/>Checking data coherency in a RAID logical volume (RAID scrubbing)</h1></div></div></div><p>
				LVM provides scrubbing support for RAID logical volumes. RAID scrubbing is the process of reading all the data and parity blocks in an array and checking to see whether they are coherent.
			</p><p>
				You initiate a RAID scrubbing operation with the <code class="literal">--syncaction</code> option of the <code class="literal">lvchange</code> command. You specify either a <code class="literal">check</code> or <code class="literal">repair</code> operation. A <code class="literal">check</code> operation goes over the array and records the number of discrepancies in the array but does not repair them. A <code class="literal">repair</code> operation corrects the discrepancies as it finds them.
			</p><p>
				The format of the command to scrub a RAID logical volume is as follows:
			</p><pre class="literallayout">lvchange --syncaction {check|repair} <span class="emphasis"><em>vg/raid_lv</em></span></pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
					The <code class="literal">lvchange --syncaction repair <span class="emphasis"><em>vg/raid_lv</em></span></code> operation does not perform the same function as the <code class="literal">lvconvert --repair <span class="emphasis"><em>vg/raid_lv</em></span></code> operation. The <code class="literal">lvchange --syncaction repair</code> operation initiates a background synchronization operation on the array, while the <code class="literal">lvconvert --repair</code> operation is designed to repair/replace failed devices in a mirror or RAID logical volume.
				</p></div><p>
				In support of the RAID scrubbing operation, the <code class="literal">lvs</code> command supports two new printable fields: <code class="literal">raid_sync_action</code> and <code class="literal">raid_mismatch_count</code>. These fields are not printed by default. To display these fields you specify them with the <code class="literal">-o</code> parameter of the <code class="literal">lvs</code>, as follows.
			</p><pre class="literallayout">lvs -o +raid_sync_action,raid_mismatch_count <span class="emphasis"><em>vg/lv</em></span></pre><p>
				The <code class="literal">raid_sync_action</code> field displays the current synchronization operation that the raid volume is performing. It can be one of the following values:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						<code class="literal">idle</code>: All sync operations complete (doing nothing)
					</li><li class="listitem">
						<code class="literal">resync</code>: Initializing an array or recovering after a machine failure
					</li><li class="listitem">
						<code class="literal">recover</code>: Replacing a device in the array
					</li><li class="listitem">
						<code class="literal">check</code>: Looking for array inconsistencies
					</li><li class="listitem">
						<code class="literal">repair</code>: Looking for and repairing inconsistencies
					</li></ul></div><p>
				The <code class="literal">raid_mismatch_count</code> field displays the number of discrepancies found during a <code class="literal">check</code> operation.
			</p><p>
				The <code class="literal">Cpy%Sync</code> field of the <code class="literal">lvs</code> command now prints the progress of any of the <code class="literal">raid_sync_action</code> operations, including <code class="literal">check</code> and <code class="literal">repair</code>.
			</p><p>
				The <code class="literal">lv_attr</code> field of the <code class="literal">lvs</code> command output now provides additional indicators in support of the RAID scrubbing operation. Bit 9 of this field displays the health of the logical volume, and it now supports the following indicators.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						(<span class="emphasis"><em>m</em></span>)ismatches indicates that there are discrepancies in a RAID logical volume. This character is shown after a scrubbing operation has detected that portions of the RAID are not coherent.
					</li><li class="listitem">
						(<span class="emphasis"><em>r</em></span>)efresh indicates that a device in a RAID array has suffered a failure and the kernel regards it as failed, even though LVM can read the device label and considers the device to be operational. The logical volume should be (r)efreshed to notify the kernel that the device is now available, or the device should be (r)eplaced if it is suspected of having failed.
					</li></ul></div><p>
				When you perform a RAID scrubbing operation, the background I/O required by the <code class="literal">sync</code> operations can crowd out other I/O operations to LVM devices, such as updates to volume group metadata. This can cause the other LVM operations to slow down. You can control the rate at which the RAID logical volume is scrubbed by implementing recovery throttling.
			</p><p>
				You control the rate at which <code class="literal">sync</code> operations are performed by setting the minimum and maximum I/O rate for those operations with the <code class="literal">--minrecoveryrate</code> and <code class="literal">--maxrecoveryrate</code> options of the <code class="literal">lvchange</code> command. You specify these options as follows.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p class="simpara">
						<code class="literal">--maxrecoveryrate <span class="emphasis"><em>Rate</em></span>[bBsSkKmMgG]</code>
					</p><p class="simpara">
						Sets the maximum recovery rate for a RAID logical volume so that it will not crowd out nominal I/O operations. The <span class="emphasis"><em>Rate</em></span> is specified as an amount per second for each device in the array. If no suffix is given, then kiB/sec/device is assumed. Setting the recovery rate to 0 means it will be unbounded.
					</p></li><li class="listitem"><p class="simpara">
						<code class="literal">--minrecoveryrate <span class="emphasis"><em>Rate</em></span>[bBsSkKmMgG]</code>
					</p><p class="simpara">
						Sets the minimum recovery rate for a RAID logical volume to ensure that I/O for <code class="literal">sync</code> operations achieves a minimum throughput, even when heavy nominal I/O is present. The <span class="emphasis"><em>Rate</em></span> is specified as an amount per second for each device in the array. If no suffix is given, then kiB/sec/device is assumed.
					</p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc-raid-takeover-configure-manage-raid"/>Converting a RAID level (RAID takeover)</h1></div></div></div><p>
				LVM supports Raid <span class="emphasis"><em>takeover</em></span>, which means converting a RAID logical volume from one RAID level to another (such as from RAID 5 to RAID 6). Changing the RAID level is usually done to increase or decrease resilience to device failures or to restripe logical volumes. You use the <code class="literal">lvconvert</code> for RAID takeover. For information on RAID takeover and for examples of using the <code class="literal">lvconvert</code> to convert a RAID logical volume, see the <code class="literal">lvmraid</code>(7) man page.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc-raid-reshape-configure-manage-raid"/>Changing attributes of a RAID volume (RAID reshape)</h1></div></div></div><p>
				RAID <span class="emphasis"><em>reshaping</em></span> means changing attributes of a RAID logical volume while keeping the same RAID level. Some attributes you can change include RAID layout, stripe size, and number of stripes. For information on RAID reshaping and examples of using the <code class="literal">lvconvert</code> command to reshape a RAID logical volume, see the <code class="literal">lvmraid</code>(7) man page.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc-controlling-raid-io-configure-manage-raid"/>Controlling I/O Operations on a RAID1 logical volume</h1></div></div></div><p>
				You can control the I/O operations for a device in a RAID1 logical volume by using the <code class="literal">--writemostly</code> and <code class="literal">--writebehind</code> parameters of the <code class="literal">lvchange</code> command. The format for using these parameters is as follows.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p class="simpara">
						<code class="literal">--[raid]writemostly <span class="emphasis"><em>PhysicalVolume</em></span>[:{t|y|n}]</code>
					</p><p class="simpara">
						Marks a device in a RAID1 logical volume as <code class="literal">write-mostly</code>. All reads to these drives will be avoided unless necessary. Setting this parameter keeps the number of I/O operations to the drive to a minimum. By default, the <code class="literal">write-mostly</code> attribute is set to yes for the specified physical volume in the logical volume. It is possible to remove the <code class="literal">write-mostly</code> flag by appending <code class="literal">:n</code> to the physical volume or to toggle the value by specifying <code class="literal">:t</code>. The <code class="literal">--writemostly</code> argument can be specified more than one time in a single command, making it possible to toggle the write-mostly attributes for all the physical volumes in a logical volume at once.
					</p></li><li class="listitem"><p class="simpara">
						<code class="literal">--[raid]writebehind <span class="emphasis"><em>IOCount</em></span></code>
					</p><p class="simpara">
						Specifies the maximum number of outstanding writes that are allowed to devices in a RAID1 logical volume that are marked as <code class="literal">write-mostly</code>. Once this value is exceeded, writes become synchronous, causing all writes to the constituent devices to complete before the array signals the write has completed. Setting the value to zero clears the preference and allows the system to choose the value arbitrarily.
					</p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc-changing-raid-regionsize-configure-manage-raid"/>Changing the region size on a RAID logical volume</h1></div></div></div><p>
				When you create a RAID logical volume, the region size for the logical volume will be the value of the <code class="literal">raid_region_size</code> parameter in the <code class="literal">/etc/lvm/lvm.conf</code> file. You can override this default value with the <code class="literal">-R</code> option of the <code class="literal">lvcreate</code> command.
			</p><p>
				After you have created a RAID logical volume, you can change the region size of the volume with the <code class="literal">-R</code> option of the <code class="literal">lvconvert</code> command. The following example changes the region size of logical volume <code class="literal">vg/raidlv</code> to 4096K. The RAID volume must be synced in order to change the region size.
			</p><pre class="literallayout"># <code class="literal">lvconvert -R 4096K vg/raid1</code>
Do you really want to change the region_size 512.00 KiB of LV vg/raid1 to 4.00 MiB? [y/n]: <code class="literal">y</code>
  Changed region size on RAID LV vg/raid1 to 4.00 MiB.</pre></div></div></body></html>
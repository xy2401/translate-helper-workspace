<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 1. High Availability Add-On overview</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters"/>Chapter 1. High Availability Add-On overview</h1></div></div></div><p>
			The High Availability Add-On is a clustered system that provides reliability, scalability, and availability to critical production services.
		</p><p>
			A cluster is two or more computers (called <span class="emphasis"><em>nodes</em></span> or <span class="emphasis"><em>members</em></span>) that work together to perform a task. Clusters can be used to provide highly available services or resources. The redundancy of multiple machines is used to guard against failures of many types.
		</p><p>
			High availability clusters provide highly available services by eliminating single points of failure and by failing over services from one cluster node to another in case a node becomes inoperative. Typically, services in a high availability cluster read and write data (by means of read-write mounted file systems). Therefore, a high availability cluster must maintain data integrity as one cluster node takes over control of a service from another cluster node. Node failures in a high availability cluster are not visible from clients outside the cluster. (High availability clusters are sometimes referred to as failover clusters.) The High Availability Add-On provides high availability clustering through its high availability service management component, <code class="literal">Pacemaker</code>.
		</p><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_high-availability-add-on-concepts-overview-of-high-availability"/>High Availability Add-On components</h1></div></div></div><p>
				The High Availability Add-On consists of the following major components:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						Cluster infrastructure — Provides fundamental functions for nodes to work together as a cluster: configuration file management, membership management, lock management, and fencing.
					</li><li class="listitem">
						High availability service management — Provides failover of services from one cluster node to another in case a node becomes inoperative.
					</li><li class="listitem">
						Cluster administration tools — Configuration and management tools for setting up, configuring, and managing the High Availability Add-On. The tools are for use with the cluster infrastructure components, the high availability and service management components, and storage.
					</li></ul></div><p>
				You can supplement the High Availability Add-On with the following components:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						Red Hat GFS2 (Global File System 2) — Part of the Resilient Storage Add-On, this provides a cluster file system for use with the High Availability Add-On. GFS2 allows multiple nodes to share storage at a block level as if the storage were connected locally to each cluster node. GFS2 cluster file system requires a cluster infrastructure.
					</li><li class="listitem">
						LVM Locking Daemon (<code class="literal">lvmlockd</code>) — Part of the Resilient Storage Add-On, this provides volume management of cluster storage. <code class="literal">lvmlockd</code> support also requires cluster infrastructure.
					</li><li class="listitem">
						Load Balancer Add-On — Routing software that provides high availability load balancing and failover in layer 4 (TCP) and layer 7 (HTTP, HTTPS) services. The Load Balancer Add-On runs in a cluster of redundant virtual routers that uses load algorithms to distribute client requests to real servers, collectively acting as a virtual server. It is not necessary to use the Load Balancer Add-On in conjunction with Pacemaker.
					</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="s1-Pacemakeroverview-HAAO"/>Pacemaker overview</h1></div></div></div><p>
				Pacemaker is a cluster resource manager. It achieves maximum availability for your cluster services and resources by making use of the cluster infrastructure’s messaging and membership capabilities to deter and recover from node and resource-level failure.
			</p><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="s2-Pacemakerarchitecture-HAAO"/>Pacemaker architecture components</h2></div></div></div><p>
					A cluster configured with Pacemaker comprises separate component daemons that monitor cluster membership, scripts that manage the services, and resource management subsystems that monitor the disparate resources.
				</p><p>
					The following components form the Pacemaker architecture:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Cluster Information Base (CIB)</span></dt><dd>
								The Pacemaker information daemon, which uses XML internally to distribute and synchronize current configuration and status information from the Designated Coordinator (DC) — a node assigned by Pacemaker to store and distribute cluster state and actions by means of the CIB — to all other cluster nodes.
							</dd><dt><span class="term">Cluster Resource Management Daemon (CRMd)</span></dt><dd><p class="simpara">
								Pacemaker cluster resource actions are routed through this daemon. Resources managed by CRMd can be queried by client systems, moved, instantiated, and changed when needed.
							</p><p class="simpara">
								Each cluster node also includes a local resource manager daemon (LRMd) that acts as an interface between CRMd and resources. LRMd passes commands from CRMd to agents, such as starting and stopping and relaying status information.
							</p></dd><dt><span class="term">Shoot the Other Node in the Head (STONITH)</span></dt><dd>
								STONITH is the Pacemaker fencing implementation. It acts as a cluster resource in Pacemaker that processes fence requests, forcefully powering down nodes and removing them from the cluster to ensure data integrity. STONITH is configured in the CIB and can be monitored as a normal cluster resource. For a general overview of fencing, see <a class="xref" href="assembly_overview-of-high-availability-configuring-and-managing-high-availability-clusters.html#s1-fencing-HAAO" title="Fencing overview">the section called “Fencing overview”</a>.
							</dd><dt><span class="term">corosync</span></dt><dd><p class="simpara">
								<code class="literal">corosync</code> is the component - and a daemon of the same name - that serves the core membership and member-communication needs for high availability clusters. It is required for the High Availability Add-On to function.
							</p><p class="simpara">
								In addition to those membership and messaging functions, <code class="literal">corosync</code> also:
							</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
										Manages quorum rules and determination.
									</li><li class="listitem">
										Provides messaging capabilities for applications that coordinate or operate across multiple members of the cluster and thus must communicate stateful or other information between instances.
									</li><li class="listitem">
										Uses the <code class="literal">kronosnet</code> library as its network transport to provide multiple redundant links and automatic failover.
									</li></ul></div></dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="s2-Pacemakertools-HAAO"/>Configuration and management tools</h2></div></div></div><p>
					The High Availability Add-On features two configuration tools for cluster deployment, monitoring, and management.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">pcs</code></span></dt><dd><p class="simpara">
								The <code class="literal">pcs</code> command line interface controls and configures Pacemaker and the <code class="literal">corosync</code> heartbeat daemon. A command-line based program, <code class="literal">pcs</code> can perform the following cluster management tasks:
							</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
										Create and configure a Pacemaker/Corosync cluster
									</li><li class="listitem">
										Modify configuration of the cluster while it is running
									</li><li class="listitem">
										Remotely configure both Pacemaker and Corosync as well as start, stop, and display status information of the cluster
									</li></ul></div></dd><dt><span class="term"><code class="literal">pcsd</code> Web UI</span></dt><dd>
								A graphical user interface to create and configure Pacemaker/Corosync clusters.
							</dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="s1-configfileoverview-HAAR"/>The cluster and pacemaker configuration files</h2></div></div></div><p>
					The configuration files for the Red Hat High Availability Add-On are <code class="literal">corosync.conf</code> and <code class="literal">cib.xml</code>.
				</p><p>
					The <code class="literal">corosync.conf</code> file provides the cluster parameters used by <code class="literal">corosync</code>, the cluster manager that Pacemaker is built on. In general, you should not edit the <code class="literal">corosync.conf</code> directly but, instead, use the <code class="literal">pcs</code> or <code class="literal">pcsd</code> interface. However, there may be a situation where you do need to edit this file directly.
				</p><p>
					The <code class="literal">cib.xml</code> file is an XML file that represents both the cluster’s configuration and the current state of all resources in the cluster. This file is used by Pacemaker’s Cluster Information Base (CIB). The contents of the CIB are automatically kept in sync across the entire cluster. Do not edit the <code class="literal">cib.xml</code> file directly; use the <code class="literal">pcs</code> or <code class="literal">pcsd</code> interface instead.
				</p></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="s1-fencing-HAAO"/>Fencing overview</h1></div></div></div><p>
				If communication with a single node in the cluster fails, then other nodes in the cluster must be able to restrict or release access to resources that the failed cluster node may have access to. This cannot be accomplished by contacting the cluster node itself as the cluster node may not be responsive. Instead, you must provide an external method, which is called fencing with a fence agent. A fence device is an external device that can be used by the cluster to restrict access to shared resources by an errant node, or to issue a hard reboot on the cluster node.
			</p><p>
				Without a fence device configured you do not have a way to know that the resources previously used by the disconnected cluster node have been released, and this could prevent the services from running on any of the other cluster nodes. Conversely, the system may assume erroneously that the cluster node has released its resources and this can lead to data corruption and data loss. Without a fence device configured data integrity cannot be guaranteed and the cluster configuration will be unsupported.
			</p><p>
				When the fencing is in progress no other cluster operation is allowed to run. Normal operation of the cluster cannot resume until fencing has completed or the cluster node rejoins the cluster after the cluster node has been rebooted.
			</p><p>
				For more information about fencing, see <a class="link" href="https://access.redhat.com/solutions/15575"> Fencing in a Red Hat High Availability Cluster</a>.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="s1-quorumoverview-HAAO"/>Quorum overview</h1></div></div></div><p>
				In order to maintain cluster integrity and availability, cluster systems use a concept known as <span class="emphasis"><em>quorum</em></span> to prevent data corruption and loss. A cluster has quorum when more than half of the cluster nodes are online. To mitigate the chance of data corruption due to failure, Pacemaker by default stops all resources if the cluster does not have quorum.
			</p><p>
				Quorum is established using a voting system. When a cluster node does not function as it should or loses communication with the rest of the cluster, the majority working nodes can vote to isolate and, if needed, fence the node for servicing.
			</p><p>
				For example, in a 6-node cluster, quorum is established when at least 4 cluster nodes are functioning. If the majority of nodes go offline or become unavailable, the cluster no longer has quorum and Pacemaker stops clustered services.
			</p><p>
				The quorum features in Pacemaker prevent what is also known as <span class="emphasis"><em>split-brain</em></span>, a phenomenon where the cluster is separated from communication but each part continues working as separate clusters, potentially writing to the same data and possibly causing corruption or loss. For more information on what it means to be in a split-brain state, and on quorum concepts in general, see <a class="link" href="https://access.redhat.com/articles/2824071">Exploring Concepts of RHEL High Availability Clusters - Quorum</a>.
			</p><p>
				A Red Hat Enterprise Linux High Availability Add-On cluster uses the <code class="literal">votequorum</code> service, in conjunction with fencing, to avoid split brain situations. A number of votes is assigned to each system in the cluster, and cluster operations are allowed to proceed only when a majority of votes is present.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="s1-resourceoverview-HAAO"/>Resource overview</h1></div></div></div><p>
				A <span class="emphasis"><em>cluster resource</em></span> is an instance of program, data, or application to be managed by the cluster service. These resources are abstracted by <span class="emphasis"><em>agents</em></span> that provide a standard interface for managing the resource in a cluster environment.
			</p><p>
				To ensure that resources remain healthy, you can add a monitoring operation to a resource’s definition. If you do not specify a monitoring operation for a resource, one is added by default.
			</p><p>
				You can determine the behavior of a resource in a cluster by configuring <span class="emphasis"><em>constraints</em></span>. You can configure the following categories of constraints:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						location constraints — A location constraint determines which nodes a resource can run on.
					</li><li class="listitem">
						ordering constraints — An ordering constraint determines the order in which the resources run.
					</li><li class="listitem">
						colocation constraints — A colocation constraint determines where resources will be placed relative to other resources.
					</li></ul></div><p>
				One of the most common elements of a cluster is a set of resources that need to be located together, start sequentially, and stop in the reverse order. To simplify this configuration, Pacemaker supports the concept of <span class="emphasis"><em>groups</em></span>.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_HA-lvm-shared-volumes-overview-of-high-availability"/>LVM logical volumes in a Red Hat high availability cluster</h1></div></div></div><p>
				The Red Hat High Availability Add-On provides support for LVM volumes in two distinct cluster configurations:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						High availability LVM volumes (HA-LVM) in active/passive failover configurations in which only a single node of the cluster accesses the storage at any one time.
					</li><li class="listitem">
						LVM volumes that use the <code class="literal">lvmlockd</code> daemon to manage storage devices in active/active configurations in which more than one node of the cluster requires access to the storage at the same time. The <code class="literal">lvmlockd</code> daemon is part of the Resilient Storage Add-On.
					</li></ul></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="choosing_ha_lvm_or_shared_volumes"/>Choosing HA-LVM or shared volumes</h2></div></div></div><p>
					When to use HA-LVM or shared logical volumes managed by the <code class="literal">lvmlockd</code> daemon should be based on the needs of the applications or services being deployed.
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							If multiple nodes of the cluster require simultaneous read/write access to LVM volumes in an active/active system, then you must use the <code class="literal">lvmlockd</code> daemon and configure your volumes as shared volumes. The <code class="literal">lvmlockd</code> daemon provides a system for coordinating activation of and changes to LVM volumes across nodes of a cluster concurrently. The <code class="literal">lvmlockd</code> daemon’s locking service provides protection to LVM metadata as various nodes of the cluster interact with volumes and make changes to their layout. This protection is contingent upon configuring any volume group that will be activated simultaneously across multiple cluster nodes as a shared volume.
						</li><li class="listitem">
							If the high availability cluster is configured to manage shared resources in an active/passive manner with only one single member needing access to a given LVM volume at a time, then you can use HA-LVM without the <code class="literal">lvmlockd</code> locking service.
						</li></ul></div><p>
					Most applications will run better in an active/passive configuration, as they are not designed or optimized to run concurrently with other instances. Choosing to run an application that is not cluster-aware on shared logical volumes may result in degraded performance. This is because there is cluster communication overhead for the logical volumes themselves in these instances. A cluster-aware application must be able to achieve performance gains above the performance losses introduced by cluster file systems and cluster-aware logical volumes. This is achievable for some applications and workloads more easily than others. Determining what the requirements of the cluster are and whether the extra effort toward optimizing for an active/active cluster will pay dividends is the way to choose between the two LVM variants. Most users will achieve the best HA results from using HA-LVM.
				</p><p>
					HA-LVM and shared logical volumes using <code class="literal">lvmlockd</code> are similar in the fact that they prevent corruption of LVM metadata and its logical volumes, which could otherwise occur if multiple machines are allowed to make overlapping changes. HA-LVM imposes the restriction that a logical volume can only be activated exclusively; that is, active on only one machine at a time. This means that only local (non-clustered) implementations of the storage drivers are used. Avoiding the cluster coordination overhead in this way increases performance. A shared volume using <code class="literal">lvmlockd</code> does not impose these restrictions and a user is free to activate a logical volume on all machines in a cluster; this forces the use of cluster-aware storage drivers, which allow for cluster-aware file systems and applications to be put on top.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="configuring_lvm_volumes_in_a_cluster"/>Configuring LVM volumes in a cluster</h2></div></div></div><p>
					In Red Hat Enterprise Linux 8, clusters are managed through Pacemaker. Both HA-LVM and shared logical volumes are supported only in conjunction with Pacemaker clusters, and must be configured as cluster resources.
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p class="simpara">
							For examples of procedures for configuring an HA-LVM volume as part of a Pacemaker cluster, see <a class="link" href="assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters.html" title="Chapter 5. Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster">Configuring an active/passive Apache HTTP server in a Red Hat High Availability cluster</a>. and <a class="link" href="assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters.html" title="Chapter 6. Configuring an active/passive NFS server in a Red Hat High Availability cluster">Configuring an active/passive NFS server in a Red Hat High Availability cluster</a>.
						</p><p class="simpara">
							Note that these procedures include the following steps:
						</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
									Ensuring that only the cluster is capable of activating the volume group
								</li><li class="listitem">
									Configuring an LVM logical volume
								</li><li class="listitem">
									Configuring the LVM volume as a cluster resource
								</li></ul></div></li><li class="listitem">
							For a procedure for configuring shared LVM volumes that use the <code class="literal">lvmlockd</code> daemon to manage storage devices in active/active configurations, see <a class="link" href="assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters.html#proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster" title="Configuring a GFS2 file system in a cluster">Configuring a GFS2 file system in a cluster</a>
						</li></ul></div></div></div></div></body></html>
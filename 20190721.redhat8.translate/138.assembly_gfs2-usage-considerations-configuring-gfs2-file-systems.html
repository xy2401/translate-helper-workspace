<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 2. Recommendations for GFS2 usage</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_gfs2-usage-considerations-configuring-gfs2-file-systems"/>Chapter 2. Recommendations for GFS2 usage</h1></div></div></div><p>
			This section provides general recommendations about GFS2 usage.
		</p><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_atime-mount-options-gfs2-usage-considerations"/>Mount Options: noatime and nodiratime</h1></div></div></div><p>
				It is generally recommended to mount GFS2 file systems with the <code class="literal">noatime</code> and <code class="literal">nodiratime</code> arguments. This allows GFS2 to spend less time updating disk inodes for every access. For more information on the effect of these arguments on GFS2 file system performance, see <a class="link" href="assembly_gfs2-performance-creating-mounting-gfs2.html#proc_gfs2-node-locking-gfs2-performance" title="GFS2 Node Locking">GFS2 Node Locking</a>.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_configuring-atime-gfs2-usage-considerations"/>Configuring <code class="literal">atime</code> Updates</h1></div></div></div><p>
				Each file inode and directory inode has three time stamps associated with it:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						<code class="literal">ctime</code> — The last time the inode status was changed
					</li><li class="listitem">
						<code class="literal">mtime</code> — The last time the file (or directory) data was modified
					</li><li class="listitem">
						<code class="literal">atime</code> — The last time the file (or directory) data was accessed
					</li></ul></div><p>
				If <code class="literal">atime</code> updates are enabled as they are by default on GFS2 and other Linux file systems then every time a file is read, its inode needs to be updated.
			</p><p>
				Because few applications use the information provided by <code class="literal">atime</code>, those updates can require a significant amount of unnecessary write traffic and file locking traffic. That traffic can degrade performance; therefore, it may be preferable to turn off or reduce the frequency of <code class="literal">atime</code> updates.
			</p><p>
				Two methods of reducing the effects of <code class="literal">atime</code> updating are available:
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						Mount with <code class="literal">relatime</code> (relative atime), which updates the <code class="literal">atime</code> if the previous <code class="literal">atime</code> update is older than the <code class="literal">mtime</code> or <code class="literal">ctime</code> update.
					</li><li class="listitem">
						Mount with <code class="literal">noatime</code>, which disables <code class="literal">atime</code> updates on that file system.
					</li></ul></div><h3><a id="mount_with_literal_role_command_relatime_literal"/>Mount with <code class="literal">relatime</code></h3><p>
				The <code class="literal">relatime</code> (relative atime) Linux mount option can be specified when the file system is mounted. This specifies that the <code class="literal">atime</code> is updated if the previous <code class="literal">atime</code> update is older than the <code class="literal">mtime</code> or <code class="literal">ctime</code> update. Usage
			</p><pre class="literallayout">mount  <span class="emphasis"><em>BlockDevice</em></span> <span class="emphasis"><em>MountPoint</em></span> -o relatime</pre><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">BlockDevice</code></span></dt><dd>
							Specifies the block device where the GFS2 file system resides.
						</dd><dt><span class="term"><code class="literal">MountPoint</code></span></dt><dd>
							Specifies the directory where the GFS2 file system should be mounted.
						</dd></dl></div><p>
				Example In this example, the GFS2 file system resides on <code class="literal">/dev/vg01/lvol0</code> and is mounted on directory <code class="literal">/mygfs2</code>. The <code class="literal">atime</code> updates take place only if the previous <code class="literal">atime</code> update is older than the <code class="literal">mtime</code> or <code class="literal">ctime</code> update.
			</p><pre class="literallayout"># <code class="literal">mount /dev/vg01/lvol0 /mygfs2 -o relatime</code></pre><h3><a id="mount_with_literal_role_command_noatime_literal"/>Mount with <code class="literal">noatime</code></h3><p>
				The <code class="literal">noatime</code> Linux mount option can be specified when the file system is mounted, which disables <code class="literal">atime</code> updates on that file system. Usage
			</p><pre class="literallayout">mount <span class="emphasis"><em>BlockDevice</em></span> <span class="emphasis"><em>MountPoint</em></span> -o noatime</pre><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">BlockDevice</code></span></dt><dd>
							Specifies the block device where the GFS2 file system resides.
						</dd><dt><span class="term"><code class="literal">MountPoint</code></span></dt><dd>
							Specifies the directory where the GFS2 file system should be mounted.
						</dd></dl></div><p>
				Example In this example, the GFS2 file system resides on <code class="literal">/dev/vg01/lvol0</code> and is mounted on directory <code class="literal">/mygfs2</code> with <code class="literal">atime</code> updates turned off.
			</p><pre class="literallayout"># <code class="literal">mount /dev/vg01/lvol0 /mygfs2 -o noatime</code></pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_vfs-tuning-options-gfs2-usage-considerations"/>VFS tuning options: research and experiment</h1></div></div></div><p>
				Like all Linux file systems, GFS2 sits on top of a layer called the virtual file system (VFS). You can tune the VFS layer to improve underlying GFS2 performance by using the <code class="literal">sysctl</code>(8) command. For example, the values for <code class="literal">dirty_background_ratio</code> and <code class="literal">vfs_cache_pressure</code> may be adjusted depending on your situation. To fetch the current values, use the following commands:
			</p><pre class="literallayout"># <code class="literal">sysctl -n vm.dirty_background_ratio</code>
# <code class="literal">sysctl -n vm.vfs_cache_pressure</code></pre><p>
				The following commands adjust the values:
			</p><pre class="literallayout"># <code class="literal">sysctl -w vm.dirty_background_ratio=20</code>
# <code class="literal">sysctl -w vm.vfs_cache_pressure=500</code></pre><p>
				You can permanently change the values of these parameters by editing the <code class="literal">/etc/sysctl.conf</code> file.
			</p><p>
				To find the optimal values for your use cases, research the various VFS options and experiment on a test cluster before deploying into full production.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_selinux-on-gfs2-gfs2-usage-considerations"/>SELinux on GFS2</h1></div></div></div><p>
				Use of Security Enhanced Linux (SELinux) with GFS2 incurs a small performance penalty. To avoid this overhead, you may choose not to use SELinux with GFS2 even on a system with SELinux in enforcing mode. When mounting a GFS2 file system, you can ensure that SELinux will not attempt to read the <code class="literal">seclabel</code> element on each file system object by using one of the <code class="literal">context</code> options as described on the <code class="literal">mount</code>(8) man page; SELinux will assume that all content in the file system is labeled with the <code class="literal">seclabel</code> element provided in the <code class="literal">context</code> mount options. This will also speed up processing as it avoids another disk read of the extended attribute block that could contain <code class="literal">seclabel</code> elements.
			</p><p>
				For example, on a system with SELinux in enforcing mode, you can use the following <code class="literal">mount</code> command to mount the GFS2 file system if the file system is going to contain Apache content. This label will apply to the entire file system; it remains in memory and is not written to disk.
			</p><pre class="literallayout"># <code class="literal">mount -t gfs2 -o context=system_u:object_r:httpd_sys_content_t:s0 /dev/mapper/xyz/mnt/gfs2</code></pre><p>
				If you are not sure whether the file system will contain Apache content, you can use the labels <code class="literal">public_content_rw_t</code> or <code class="literal">public_content_t</code>, or you could define a new label altogether and define a policy around it.
			</p><p>
				Note that in a Pacemaker cluster you should always use Pacemaker to manage a GFS2 file system. You can specify the mount options when you create a GFS2 file system resource.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_nfs-over-gfs2-gfs2-usage-considerations"/>Setting up NFS over GFS2</h1></div></div></div><p>
				Due to the added complexity of the GFS2 locking subsystem and its clustered nature, setting up NFS over GFS2 requires taking many precautions and careful configuration. This section describes the caveats you should take into account when configuring an NFS service over a GFS2 file system.
			</p><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>
					If the GFS2 file system is NFS exported, and NFS client applications use POSIX locks, then you must mount the file system with the <code class="literal">localflocks</code> option. The effect of this is to force both POSIX locks and flocks from each server to be local: non-clustered, independent of each other. This is necessary because a number of problems exist if GFS2 attempts to implement POSIX locks from NFS across the nodes of a cluster. For applications running on NFS clients, localized POSIX locks means that two clients can hold the same lock concurrently if the two clients are mounting from different servers. For this reason, when using NFS over GFS2, it is always safest to specify the <code class="literal">-o localflocks</code> mount option so that NFS can coordinate both POSIX locks and the flocks among all clients mounting NFS.
				</p><p>
					For all other (non-NFS) GFS2 applications, do not mount your file system using <code class="literal">localflocks</code>, so that GFS2 will manage the POSIX locks and flocks between all the nodes in the cluster (on a cluster-wide basis). If you specify <code class="literal">localflocks</code> and do not use NFS, the other nodes in the cluster will not have knowledge of each other’s POSIX locks and flocks, thus making them unsafe in a clustered environment
				</p></div><p>
				In addition to the locking considerations, you should take the following into account when configuring an NFS service over a GFS2 file system.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p class="simpara">
						Red Hat supports only Red Hat High Availability Add-On configurations using NFSv3 with locking in an active/passive configuration with the following characteristics:
					</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
								The back-end file system is a GFS2 file system running on a 2 to 16 node cluster.
							</li><li class="listitem">
								An NFSv3 server is defined as a service exporting the entire GFS2 file system from a single cluster node at a time.
							</li><li class="listitem">
								The NFS server can fail over from one cluster node to another (active/passive configuration).
							</li><li class="listitem">
								No access to the GFS2 file system is allowed <span class="emphasis"><em>except</em></span> through the NFS server. This includes both local GFS2 file system access as well as access through Samba or Clustered Samba.
							</li><li class="listitem"><p class="simpara">
								There is no NFS quota support on the system.
							</p><p class="simpara">
								This configuration provides High Availability (HA) for the file system and reduces system downtime since a failed node does not result in the requirement to execute the <code class="literal">fsck</code> command when failing the NFS server from one node to another.
							</p></li></ul></div></li><li class="listitem">
						The <code class="literal">fsid=</code> NFS option is mandatory for NFS exports of GFS2.
					</li><li class="listitem">
						If problems arise with your cluster (for example, the cluster becomes inquorate and fencing is not successful), the clustered logical volumes and the GFS2 file system will be frozen and no access is possible until the cluster is quorate. You should consider this possibility when determining whether a simple failover solution such as the one defined in this procedure is the most appropriate for your system.
					</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_samba-over-gfs2-gfs2-usage-considerations"/>Samba (SMB or Windows) file serving over GFS2</h1></div></div></div><p>
				You can use Samba (SMB or Windows) file serving from a GFS2 file system with CTDB, which allows active/active configurations.
			</p><p>
				Simultaneous access to the data in the Samba share from outside of Samba is not supported. There is currently no support for GFS2 cluster leases, which slows Samba file serving.
			</p></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="con_vms-for-gfs2-gfs2-usage-considerations"/>Configuring virtual machines for GFS2</h1></div></div></div><p>
				When using a GFS2 file system with a virtual machine, it is important that your VM storage settings on each node be configured properly in order to force the cache off. For example, including these settings for <code class="literal">cache</code> and <code class="literal">io</code> in the <code class="literal">libvirt</code> domain should allow GFS2 to behave as expected.
			</p><pre class="literallayout">&lt;driver name='qemu' type='raw' cache='none' io='native'/&gt;</pre><p>
				Alternately, you can configure the <code class="literal">shareable</code> attribute within the device element. This indicates that the device is expected to be shared between domains (as long as hypervisor and OS support this). If <code class="literal">shareable</code> is used, <code class="literal">cache='no'</code> should be used for that device.
			</p></div></div></body></html>
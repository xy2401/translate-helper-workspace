<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 17. Managing cluster nodes</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="assembly_clusternode-management-configuring-and-managing-high-availability-clusters"/>Chapter 17. Managing cluster nodes</h1></div></div></div><p>
			The following sections describe the commands you use to manage cluster nodes, including commands to start and stop cluster services and to add and remove cluster nodes.
		</p><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_cluster-stop-clusternode-management"/>Stopping cluster services</h1></div></div></div><p>
				The following command stops cluster services on the specified node or nodes. As with the <code class="literal">pcs cluster start</code>, the <code class="literal">--all</code> option stops cluster services on all nodes and if you do not specify any nodes, cluster services are stopped on the local node only.
			</p><pre class="literallayout">pcs cluster stop [--all | <span class="emphasis"><em>node</em></span>] [...]</pre><p>
				You can force a stop of cluster services on the local node with the following command, which performs a <code class="literal">kill -9</code> command.
			</p><pre class="literallayout">pcs cluster kill</pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_cluster-enable-clusternode-management"/>Enabling and disabling cluster services</h1></div></div></div><p>
				Use the following command to enables the cluster services, which configures the cluster services to run on startup on the specified node or nodes. Enabling allows nodes to automatically rejoin the cluster after they have been fenced, minimizing the time the cluster is at less than full strength. If the cluster services are not enabled, an administrator can manually investigate what went wrong before starting the cluster services manually, so that, for example, a node with hardware issues in not allowed back into the cluster when it is likely to fail again.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						If you specify the <code class="literal">--all</code> option, the command enables cluster services on all nodes.
					</li><li class="listitem">
						If you do not specify any nodes, cluster services are enabled on the local node only.
					</li></ul></div><pre class="literallayout">pcs cluster enable [--all | <span class="emphasis"><em>node</em></span>] [...]</pre><p>
				Use the following command to configure the cluster services not to run on startup on the specified node or nodes.
			</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
						If you specify the <code class="literal">--all</code> option, the command disables cluster services on all nodes.
					</li><li class="listitem">
						If you do not specify any nodes, cluster services are disabled on the local node only.
					</li></ul></div><pre class="literallayout">pcs cluster disable [--all | <span class="emphasis"><em>node</em></span>] [...]</pre></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_cluster-nodeadd-clusternode-management"/>Adding cluster nodes</h1></div></div></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
					It is highly recommended that you add nodes to existing clusters only during a production maintenance window. This allows you to perform appropriate resource and deployment testing for the new node and its fencing configuration.
				</p></div><p>
				Use the following procedure to add a new node to an existing cluster. This procedure adds standard clusters nodes running <code class="literal">corosync</code>. For information on integrating non-corosync nodes into a cluster, see <a class="link" href="assembly_remote-node-management-configuring-and-managing-high-availability-clusters.html" title="Chapter 24. Integrating non-corosync nodes into a cluster: the pacemaker_remote service">Integrating non-corosync nodes into a cluster: the pacemaker_remote service</a>.
			</p><p>
				In this example, the existing cluster nodes are <code class="literal">clusternode-01.example.com</code>, <code class="literal">clusternode-02.example.com</code>, and <code class="literal">clusternode-03.example.com</code>. The new node is <code class="literal">newnode.example.com</code>.
			</p><p>
				On the new node to add to the cluster, perform the following tasks.
			</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p class="simpara">
						Install the cluster packages. If the cluster uses SBD, the Booth ticket manager, or a quorum device, you must manually install the respective packages (<code class="literal">sbd</code>, <code class="literal">booth-site</code>, <code class="literal">corosync-qdevice</code>) on the new node as well.
					</p><pre class="literallayout">[root@newnode ~]# <code class="literal">yum install -y pcs fence-agents-all</code></pre><p class="simpara">
						In addition to the cluster packages, you will also need to install and configure all of the services that you are running in the cluster, which you have installed on the existing cluster nodes. For example, if you are running an Apache HTTP server in a Red Hat high availability cluster, you will need to install the server on the node you are adding, as well as the <code class="literal">wget</code> tool that checks the status of the server.
					</p></li><li class="listitem"><p class="simpara">
						If you are running the <code class="literal">firewalld</code> daemon, execute the following commands to enable the ports that are required by the Red Hat High Availability Add-On.
					</p><pre class="literallayout"># <code class="literal">firewall-cmd --permanent --add-service=high-availability</code>
# <code class="literal">firewall-cmd --add-service=high-availability</code></pre></li><li class="listitem"><p class="simpara">
						Set a password for the user ID <code class="literal">hacluster</code>. It is recommended that you use the same password for each node in the cluster.
					</p><pre class="literallayout">[root@newnode ~]# <code class="literal">passwd hacluster</code>
Changing password for user hacluster.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.</pre></li><li class="listitem"><p class="simpara">
						Execute the following commands to start the <code class="literal">pcsd</code> service and to enable <code class="literal">pcsd</code> at system start.
					</p><pre class="literallayout"># <code class="literal">systemctl start pcsd.service</code>
# <code class="literal">systemctl enable pcsd.service</code></pre></li></ol></div><p>
				On a node in the existing cluster, perform the following tasks.
			</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p class="simpara">
						Authenticate user <code class="literal">hacluster</code> on the new cluster node.
					</p><pre class="literallayout">[root@clusternode-01 ~]# <code class="literal">pcs host auth newnode.example.com</code>
Username: hacluster
Password:
newnode.example.com: Authorized</pre></li><li class="listitem"><p class="simpara">
						Add the new node to the existing cluster. This command also syncs the cluster configuration file <code class="literal">corosync.conf</code> to all nodes in the cluster, including the new node you are adding.
					</p><pre class="literallayout">[root@clusternode-01 ~]# <code class="literal">pcs cluster node add newnode.example.com</code></pre></li></ol></div><p>
				On the new node to add to the cluster, perform the following tasks.
			</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem"><p class="simpara">
						Start and enable cluster services on the new node.
					</p><pre class="literallayout">[root@newnode ~]# <code class="literal">pcs cluster start</code>
Starting Cluster...
[root@newnode ~]# <code class="literal">pcs cluster enable</code></pre></li><li class="listitem">
						Ensure that you configure and test a fencing device for the new cluster node.
					</li></ol></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="proc_cluster-noderemove-clusternode-management"/>Removing cluster nodes</h1></div></div></div><p>
				The following command shuts down the specified node and removes it from the cluster configuration file, <code class="literal">corosync.conf</code>, on all of the other nodes in the cluster.
			</p><pre class="literallayout">pcs cluster node remove <span class="emphasis"><em>node</em></span></pre></div></div></body></html>
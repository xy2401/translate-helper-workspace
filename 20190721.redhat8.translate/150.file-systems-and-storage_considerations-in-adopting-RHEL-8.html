<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 12. File systems and storage</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta name="generator" content="DocBook XSL-NS Stylesheets V1.78.1"/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="file-systems-and-storage_considerations-in-adopting-RHEL-8"/>Chapter 12. File systems and storage</h1></div></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="file-systems_file-systems-and-storage"/>File systems</h1></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="btrfs-has-been-removed_file-systems-and-storage"/>Btrfs has been removed</h2></div></div></div><p>
					The Btrfs file system has been removed in Red Hat Enterprise Linux 8. This includes the following components:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							The <code class="literal">btrfs.ko</code> kernel module
						</li><li class="listitem">
							The <code class="literal">btrfs-progs</code> package
						</li><li class="listitem">
							The <code class="literal">snapper</code> package
						</li></ul></div><p>
					You can no longer create, mount, or install on Btrfs file systems in Red Hat Enterprise Linux 8. The Anaconda installer and the Kickstart commands no longer support Btrfs.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="xfs-now-supports-shared-copy-on-write-data-extents_file-systems-and-storage"/>XFS now supports shared copy-on-write data extents</h2></div></div></div><p>
					The XFS file system supports shared copy-on-write data extent functionality. This feature enables two or more files to share a common set of data blocks. When either of the files sharing common blocks changes, XFS breaks the link to common blocks and creates a new file. This is similar to the copy-on-write (COW) functionality found in other file systems.
				</p><p>
					Shared copy-on-write data extents are:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Fast</span></dt><dd>
								Creating shared copies does not utilize disk I/O.
							</dd><dt><span class="term">Space-efficient</span></dt><dd>
								Shared blocks do not consume additional disk space.
							</dd><dt><span class="term">Transparent</span></dt><dd>
								Files sharing common blocks act like regular files.
							</dd></dl></div><p>
					Userspace utilities can use shared copy-on-write data extents for:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							Efficient file cloning, such as with the <code class="literal">cp --reflink</code> command
						</li><li class="listitem">
							Per-file snapshots
						</li></ul></div><p>
					This functionality is also used by kernel subsystems such as Overlayfs and NFS for more efficient operation.
				</p><p>
					Shared copy-on-write data extents are now enabled by default when creating an XFS file system, starting with the <code class="literal">xfsprogs</code> package version <code class="literal">4.17.0-2.el8</code>.
				</p><p>
					Note that Direct Access (DAX) devices currently do not support XFS with shared copy-on-write data extents. To create an XFS file system without this feature, use the following command:
				</p><pre class="screen"># mkfs.xfs -m reflink=0 <span class="emphasis"><em>block-device</em></span></pre><p>
					Red Hat Enterprise Linux 7 can mount XFS file systems with shared copy-on-write data extents only in the read-only mode.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="the-ext4-file-system-now-supports-metadata-checksums_file-systems-and-storage"/>The ext4 file system now supports metadata checksums</h2></div></div></div><p>
					With this update, ext4 metadata is protected by checksums. This enables the file system to recognize the corrupt metadata, which avoids damage and increases the file system resilience.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="the-etc-sysconfig-nfs-file-and-legacy-nfs-service-names-are-no-longer-available_file-systems-and-storage"/>The <code class="literal">/etc/sysconfig/nfs</code> file and legacy NFS service names are no longer available</h2></div></div></div><p>
					In Red Hat Enterprise Linux 8.0, the NFS configuration has moved from the <code class="literal">/etc/sysconfig/nfs</code> configuration file, which was used in Red Hat Enterprise Linux 7, to <code class="literal">/etc/nfs.conf</code>.
				</p><p>
					The <code class="literal">/etc/nfs.conf</code> file uses a different syntax. Red Hat Enterprise Linux 8 attempts to automatically convert all options from <code class="literal">/etc/sysconfig/nfs</code> to <code class="literal">/etc/nfs.conf</code> when upgrading from Red Hat Enterprise Linux 7.
				</p><p>
					Both configuration files are supported in Red Hat Enterprise Linux 7. Red Hat recommends that you use the new <code class="literal">/etc/nfs.conf</code> file to make NFS configuration in all versions of Red Hat Enterprise Linux compatible with automated configuration systems.
				</p><p>
					Additionally, the following NFS service aliases have been removed and replaced by their upstream names:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							<code class="literal">nfs.service</code>, replaced by <code class="literal">nfs-server.service</code>
						</li><li class="listitem">
							<code class="literal">nfs-secure.service</code>, replaced by <code class="literal">rpc-gssd.service</code>
						</li><li class="listitem">
							<code class="literal">rpcgssd.service</code>, replaced by <code class="literal">rpc-gssd.service</code>
						</li><li class="listitem">
							<code class="literal">nfs-idmap.service</code>, replaced by <code class="literal">nfs-idmapd.service</code>
						</li><li class="listitem">
							<code class="literal">rpcidmapd.service</code>, replaced by <code class="literal">nfs-idmapd.service</code>
						</li><li class="listitem">
							<code class="literal">nfs-lock.service</code>, replaced by <code class="literal">rpc-statd.service</code>
						</li><li class="listitem">
							<code class="literal">nfslock.service</code>, replaced by <code class="literal">rpc-statd.service</code>
						</li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="storage_file-systems-and-storage"/>Storage</h1></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="the-boom-boot-manager-simplifies-the-process-of-creating-boot-entries_file-systems-and-storage"/>The BOOM boot manager simplifies the process of creating boot entries</h2></div></div></div><p>
					BOOM is a boot manager for Linux systems that use boot loaders supporting the BootLoader Specification for boot entry configuration. It enables flexible boot configuration and simplifies the creation of new or modified boot entries: for example, to boot snapshot images of the system created using LVM.
				</p><p>
					BOOM does not modify the existing boot loader configuration, and only inserts additional entries. The existing configuration is maintained, and any distribution integration, such as kernel installation and update scripts, continue to function as before.
				</p><p>
					BOOM has a simplified command-line interface (CLI) and API that ease the task of creating boot entries.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="stratis-is-available_file-systems-and-storage"/>Stratis is now available</h2></div></div></div><p>
					Stratis is a new local storage manager. It provides managed file systems on top of pools of storage with additional features to the user.
				</p><p>
					Stratis enables you to more easily perform storage tasks such as:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							Manage snapshots and thin provisioning
						</li><li class="listitem">
							Automatically grow file system sizes as needed
						</li><li class="listitem">
							Maintain file systems
						</li></ul></div><p>
					To administer Stratis storage, use the <code class="literal">stratis</code> utility, which communicates with the <code class="literal">stratisd</code> background service.
				</p><p>
					Stratis is provided as a Technology Preview.
				</p><p>
					For more information, see the Stratis documentation: <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_file_systems/managing-layered-local-storage-with-stratis_managing-file-systems">Managing layered local storage with Stratis</a>.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="luks2-is-now-the-default-format-for-encrypting-volumes_file-systems-and-storage"/>LUKS2 is now the default format for encrypting volumes</h2></div></div></div><p>
					In RHEL 8, the LUKS version 2 (LUKS2) format replaces the legacy LUKS (LUKS1) format. The <code class="literal">dm-crypt</code> subsystem and the <code class="literal">cryptsetup</code> tool now uses LUKS2 as the default format for encrypted volumes. LUKS2 provides encrypted volumes with metadata redundancy and auto-recovery in case of a partial metadata corruption event.
				</p><p>
					Due to the internal flexible layout, LUKS2 is also an enabler of future features. It supports auto-unlocking through the generic kernel-keyring token built in <code class="literal">libcryptsetup</code> that allow users unlocking of LUKS2 volumes using a passphrase stored in the kernel-keyring retention service.
				</p><p>
					Other notable enhancements include:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							The protected key setup using the wrapped key cipher scheme.
						</li><li class="listitem">
							Easier integration with Policy-Based Decryption (Clevis).
						</li><li class="listitem">
							Up to 32 key slots - LUKS1 provides only 8 key slots.
						</li></ul></div><p>
					For more details, see the <code class="literal">cryptsetup(8)</code> and <code class="literal">cryptsetup-reencrypt(8)</code> man pages.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="multiqueue-scheduling-on-block-devices_file-systems-and-storage"/>Multiqueue scheduling on block devices</h2></div></div></div><p>
					Block devices now use multiqueue scheduling in Red Hat Enterprise Linux 8. This enables the block layer performance to scale well with fast solid-state drives (SSDs) and multi-core systems.
				</p><p>
					The SCSI Multiqueue (<code class="literal">scsi-mq</code>) driver is now enabled by default, and the kernel boots with the <code class="literal">scsi_mod.use_blk_mq=Y</code> option. This change is consistent with the upstream Linux kernel.
				</p><p>
					Device Mapper Multipath (DM Multipath) requires the <code class="literal">scsi-mq</code> driver to be active.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="vdo-now-supports-all-architectures_file-systems-and-storage"/>VDO now supports all architectures</h2></div></div></div><p>
					Virtual Data Optimizer (VDO) is now available on all of the architectures supported by RHEL 8.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="vdo-no-longer-supports-read-cache_file-systems-and-storage"/>VDO no longer supports read cache</h2></div></div></div><p>
					The read cache functionality has been removed from Virtual Data Optimizer (VDO). The read cache is always disabled on VDO volumes, and you can no longer enable it using the <code class="literal">--readCache</code> option of the <code class="literal">vdo</code> utility.
				</p><p>
					Red Hat might reintroduce the VDO read cache in a later Red Hat Enterprise Linux release, using a different implementation.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="the-dmraid-package-has-been-removed_file-systems-and-storage"/>The <code class="literal">dmraid</code> package has been removed</h2></div></div></div><p>
					The <code class="literal">dmraid</code> package has been removed from Red Hat Enterprise Linux 8. Users requiring support for combined hardware and software RAID host bus adapters (HBA) should use the <code class="literal">mdadm</code> utility, which supports native MD software RAID, the SNIA RAID Common Disk Data Format (DDF), and the Intel® Matrix Storage Manager (IMSM) formats.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="software-fcoe-and-fibre-channel-no-longer-support-the-target-mode_file-systems-and-storage"/>Software FCoE and Fibre Channel no longer support the target mode</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							Software FCoE: NIC Software FCoE target functionality is removed in Red Hat Enterprise Linux 8.0.
						</li><li class="listitem">
							Fibre Channel no longer supports the target mode. Target mode is disabled for the <code class="literal">qla2xxx</code> QLogic Fibre Channel driver in Red Hat Enterprise Linux 8.0.
						</li></ul></div><p>
					For more information, see <a class="xref" href="hardware-enablement_considerations-in-adopting-RHEL-8.html#fcoe-sw-removal_hardware-enablement" title="FCoE software removal">the section called “FCoE software removal”</a>.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="the-detection-of-marginal-paths-in-dm-multipath-has-been-improved_file-systems-and-storage"/>The detection of marginal paths in DM Multipath has been improved</h2></div></div></div><p>
					The <code class="literal">multipathd</code> service now supports improved detection of marginal paths. This helps multipath devices avoid paths that are likely to fail repeatedly, and improves performance. Marginal paths are paths with persistent but intermittent I/O errors.
				</p><p>
					The following options in the <code class="literal">/etc/multipath.conf</code> file control marginal paths behavior:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							<code class="literal">marginal_path_double_failed_time</code>
						</li><li class="listitem">
							<code class="literal">marginal_path_err_sample_time</code>
						</li><li class="listitem">
							<code class="literal">marginal_path_err_rate_threshold</code>
						</li><li class="listitem">
							<code class="literal">marginal_path_err_recheck_gap_time</code>
						</li></ul></div><p>
					DM Multipath disables a path and tests it with repeated I/O for the configured sample time if:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							the listed <code class="literal">multipath.conf</code> options are set,
						</li><li class="listitem">
							a path fails twice in the configured time, and
						</li><li class="listitem">
							other paths are available.
						</li></ul></div><p>
					If the path has more than the configured err rate during this testing, DM Multipath ignores it for the configured gap time, and then retests it to see if it is working well enough to be reinstated.
				</p><p>
					For more information, see the <code class="literal">multipath.conf</code> man page.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="new-overrides-section-of-the-dm-multipath-configuration-file_file-systems-and-storage"/>New <code class="literal">overrides</code> section of the DM Multipath configuration file</h2></div></div></div><p>
					The <code class="literal">/etc/multipath.conf</code> file now includes an <code class="literal">overrides</code> section that allows you to set a configuration value for all of your devices. These attributes are used by DM Multipath for all devices unless they are overwritten by the attributes specified in the <code class="literal">multipaths</code> section of the <code class="literal">/etc/multipath.conf</code> file for paths that contain the device. This functionality replaces the <code class="literal">all_devs</code> parameter of the <code class="literal">devices</code> section of the configuration file, which is no longer supported.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="nvme-fc-is-fully-supported-on-broadcom-emulex-and-marvell-qlogic-fibre-channel-adapters_file-systems-and-storage"/>NVMe/FC is fully supported on Broadcom Emulex and Marvell Qlogic Fibre Channel adapters</h2></div></div></div><p>
					The NVMe over Fibre Channel (NVMe/FC) transport type is now fully supported in Initiator mode when used with Broadcom Emulex and Marvell Qlogic Fibre Channel 32Gbit adapters that feature NVMe support.
				</p><p>
					NVMe over Fibre Channel is an additional fabric transport type for the Nonvolatile Memory Express (NVMe) protocol, in addition to the Remote Direct Memory Access (RDMA) protocol that was previously introduced in Red Hat Enterprise Linux.
				</p><p>
					Enabling NVMe/FC:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p class="simpara">
							To enable NVMe/FC in the <code class="literal">lpfc</code> driver, edit the <code class="literal">/etc/modprobe.d/lpfc.conf</code> file and add the following option:
						</p><pre class="screen">lpfc_enable_fc4_type=3</pre></li><li class="listitem"><p class="simpara">
							To enable NVMe/FC in the <code class="literal">qla2xxx</code> driver, edit the <code class="literal">/etc/modprobe.d/qla2xxx.conf</code> file and add the following option:
						</p><pre class="screen">qla2xxx.ql2xnvmeenable=1</pre></li></ul></div><p>
					Additional restrictions:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							Multipath is not supported with NVMe/FC.
						</li><li class="listitem">
							NVMe clustering is not supported with NVMe/FC.
						</li><li class="listitem">
							With Marvell Qlogic adapters, Red Hat Enterprise Linux does not support using NVMe/FC and SCSI/FC on an initiator port at the same time.
						</li><li class="listitem">
							<code class="literal">kdump</code> is not supported with NVMe/FC.
						</li><li class="listitem">
							Booting from Storage Area Network (SAN) NVMe/FC is not supported.
						</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="support-for-data-integrity-field-data-integrity-extension-dif-dix_file-systems-and-storage"/>Support for Data Integrity Field/Data Integrity Extension (DIF/DIX)</h2></div></div></div><p>
					DIF/DIX is an addition to the SCSI Standard. It remains in Technology Preview for all HBAs and storage arrays, except for those specifically listed as supported.
				</p><p>
					DIF/DIX increases the size of the commonly used 512 byte disk block from 512 to 520 bytes, adding the Data Integrity Field (DIF). The DIF stores a checksum value for the data block that is calculated by the Host Bus Adapter (HBA) when a write occurs. The storage device then confirms the checksum on receipt, and stores both the data and the checksum. Conversely, when a read occurs, the checksum can be verified by the storage device, and by the receiving HBA.
				</p></div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="lvm_file-systems-and-storage"/>LVM</h1></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="removal-of-clvmd-for-managing-shared-storage-devices_file-systems-and-storage"/>Removal of <code class="literal">clvmd</code> for managing shared storage devices</h2></div></div></div><p>
					LVM no longer uses <code class="literal">clvmd</code> (cluster lvm daemon) for managing shared storage devices. Instead, LVM now uses <code class="literal">lvmlockd</code> (lvm lock daemon).
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							For details about using <code class="literal">lvmlockd</code>, see the <code class="literal">lvmlockd(8)</code> man page. For details about using shared storage in general, see the <code class="literal">lvmsystemid(7)</code> man page.
						</li><li class="listitem">
							For information on using LVM in a Pacemaker cluster, see the help screen for the <code class="literal">LVM-activate</code> resource agent.
						</li><li class="listitem">
							For an example of a procedure to configure a shared logical volume in a Red Hat High Availability cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_high_availability_clusters/#assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters">Configuring a GFS2 file system in a cluster</a>.
						</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="removal-of-lvmetad-daemon_file-systems-and-storage"/>Removal of <code class="literal">lvmetad</code> daemon</h2></div></div></div><p>
					LVM no longer uses the <code class="literal">lvmetad</code> daemon for caching metadata, and will always read metadata from disk. LVM disk reading has been reduced, which reduces the benefits of caching.
				</p><p>
					Previously, autoactivation of logical volumes was indirectly tied to the <code class="literal">use_lvmetad</code> setting in the <code class="literal">lvm.conf</code> configuration file. The correct way to disable autoactivation continues to be setting <code class="literal">auto_activation_volume_list</code> in the <code class="literal">lvm.conf</code> file.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="lvm-can-no-longer-manage-devices-formatted-with-the-gfs-pool-volume-manager-or-the-lvm1-metadata-format_file-systems-and-storage"/>LVM can no longer manage devices formatted with the GFS pool volume manager or the <code class="literal">lvm1</code> metadata format.</h2></div></div></div><p>
					LVM can no longer manage devices formatted with the GFS pool volume manager or the`lvm1` metadata format. if you created your logical volume before Red Hat Enterprise Linux 4 was introduced, then this may affect you. Volume groups using the <code class="literal">lvm1</code> format should be converted to the <code class="literal">lvm2</code> format using the <code class="literal">vgconvert</code> command.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="lvm-libraries-and-lvm-python-bindings-have-been-removed_file-systems-and-storage"/>LVM libraries and LVM Python bindings have been removed</h2></div></div></div><p>
					The <code class="literal">lvm2app</code> library and LVM Python bindings, which were provided by the <code class="literal">lvm2-python-libs</code> package, have been removed. Red Hat recommends the following solutions instead:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							The LVM D-Bus API in combination with the <code class="literal">lvm2-dbusd</code> service. This requires using Python version 3.
						</li><li class="listitem">
							The LVM command-line utilities with JSON formatting; this formatting has been available since the <code class="literal">lvm2</code> package version 2.02.158.
						</li><li class="listitem">
							The <code class="literal">libblockdev</code> library, included in AppStream, for C/C++
						</li></ul></div><p>
					You must port any applications using the removed libraries and bindings to the D-Bus API before upgrading to Red Hat Enterprise Linux 8.
				</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="the-ability-to-mirror-the-log-for-lvm-mirrors-has-been-removed_file-systems-and-storage"/>The ability to mirror the log for LVM mirrors has been removed</h2></div></div></div><p>
					The mirrored log feature of mirrored LVM volumes has been removed. Red Hat Enterprise Linux (RHEL) 8 no longer supports creating or activating LVM volumes with a mirrored mirror log.
				</p><p>
					The recommended replacements are:
				</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
							RAID1 LVM volumes. The main advantage of RAID1 volumes is their ability to work even in degraded mode and to recover after a transient failure.
						</li><li class="listitem">
							Disk mirror log. To convert a mirrored mirror log to disk mirror log, use the following command: <code class="literal">lvconvert --mirrorlog disk my_vg/my_lv</code>.
						</li></ul></div></div></div></div></body></html>
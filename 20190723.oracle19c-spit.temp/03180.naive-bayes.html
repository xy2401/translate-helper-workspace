<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xml:lang="en-us" lang="en-us">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <meta name="abstract" content="Learn how to use Naive Bayes Classification algorithm that the Oracle Data Mining supports.">
      <meta name="description" content="Learn how to use Naive Bayes Classification algorithm that the Oracle Data Mining supports.">
      <title>Naive Bayes</title>
      <meta property="og:site_name" content="Oracle Help Center">
      <meta property="og:title" content="Concepts">
      <meta property="og:description" content="Learn how to use Naive Bayes Classification algorithm that the Oracle Data Mining supports.">
      <link rel="stylesheet" href="/sp_common/book-template/ohc-book-template/css/book.css">
      <link rel="shortcut icon" href="/sp_common/book-template/ohc-common/img/favicon.ico">
      <meta name="application-name" content="Concepts">
      <meta name="generator" content="DITA Open Toolkit version 1.8.5 (Mode = doc)">
      <meta name="plugin" content="SP_docbuilder HTML plugin release 18.2.2">
      <link rel="alternate" href="data-mining-concepts.pdf" title="PDF File" type="application/pdf">
      <meta name="robots" content="all">
      <link rel="schema.dcterms" href="http://purl.org/dc/terms/">
      <meta name="dcterms.created" content="2019-01-10T04:37:09-08:00">
      
      <meta name="dcterms.dateCopyrighted" content="2005, 2019">
      <meta name="dcterms.category" content="database">
      <meta name="dcterms.identifier" content="E97867-01">
      
      <meta name="dcterms.product" content="en/database/oracle/oracle-database/19">
      
      <link rel="prev" href="minimum-description-length.html" title="Previous" type="text/html">
      <link rel="next" href="neural-network.html" title="Next" type="text/html">
      <script>
        document.write('<style type="text/css">');
        document.write('body > .noscript, body > .noscript ~ * { visibility: hidden; }');
        document.write('</style>');
     </script>
      <script data-main="/sp_common/book-template/ohc-book-template/js/book-config" src="/sp_common/book-template/requirejs/require.js"></script>
      <script>
            if (window.require === undefined) {
                document.write('<script data-main="sp_common/book-template/ohc-book-template/js/book-config" src="sp_common/book-template/requirejs/require.js"><\/script>');
                document.write('<link href="sp_common/book-template/ohc-book-template/css/book.css" rel="stylesheet"/>');
            }
        </script>
      <script type="application/json" id="ssot-metadata">{"primary":{"category":{"short_name":"database","element_name":"Database","display_in_url":true},"suite":{"short_name":"oracle","element_name":"Oracle","display_in_url":true},"product_group":{"short_name":"not-applicable","element_name":"Not applicable","display_in_url":false},"product":{"short_name":"oracle-database","element_name":"Oracle Database","display_in_url":true},"release":{"short_name":"19","element_name":"Release 19","display_in_url":true}}}</script>
      
    <meta name="dcterms.title" content="Data Mining Concepts">
    <meta name="dcterms.isVersionOf" content="DMCON">
    <meta name="dcterms.release" content="Release 19">
  </head>
   <body>
      <div class="noscript alert alert-danger text-center" role="alert">
         <a href="minimum-description-length.html" class="pull-left"><span class="glyphicon glyphicon-chevron-left" aria-hidden="true"></span>Previous</a>
         <a href="neural-network.html" class="pull-right">Next<span class="glyphicon glyphicon-chevron-right" aria-hidden="true"></span></a>
         <span class="fa fa-exclamation-triangle" aria-hidden="true"></span> JavaScript must be enabled to correctly display this content
        
      </div>
      <article>
         <header>
            <ol class="breadcrumb" vocab="http://schema.org/" typeof="BreadcrumbList">
               <li property="itemListElement" typeof="ListItem"><a href="index.html" property="item" typeof="WebPage"><span property="name">Concepts</span></a></li>
               <li property="itemListElement" typeof="ListItem"><a href="algorithms.html" property="item" typeof="WebPage"><span property="name"> Algorithms</span></a></li>
               <li class="active" property="itemListElement" typeof="ListItem"> Naive Bayes</li>
            </ol>
            <a id="GUID-BB77D68D-3E07-4522-ACB6-FD6723BDA92A" name="GUID-BB77D68D-3E07-4522-ACB6-FD6723BDA92A"></a><a id="DMCON018"></a>
            
            <h2 id="DMCON-GUID-BB77D68D-3E07-4522-ACB6-FD6723BDA92A" class="sect2"><span class="enumeration_chapter">20 </span> Naive Bayes
            </h2>
         </header>
         <div class="ind">
            <div>
               <p>Learn how to use Naive Bayes Classification algorithm that the Oracle Data Mining supports.</p>
               <p><a id="d22542e19" class="indexterm-anchor"></a><a id="d22542e21" class="indexterm-anchor"></a><a id="d22542e25" class="indexterm-anchor"></a></p>
               <ul style="list-style-type: disc;">
                  <li>
                     <p><a href="naive-bayes.html#GUID-DA3E0198-C041-4A85-BD63-9C03C6ABC167" title="Learn about Naive Bayes algorithm.">About Naive Bayes</a></p>
                  </li>
                  <li>
                     <p><a href="naive-bayes.html#GUID-11AC5A10-CF1D-4CD9-8CB2-33801F75FD21" title="Introduces about probability calculation of pairwise occurrences and percentage of singleton occurrences.">Tuning a Naive Bayes Model</a></p>
                  </li>
                  <li>
                     <p><a href="naive-bayes.html#GUID-FD929CF9-6513-42DE-B04D-FD57310C3A04" title="Learn about preparing the data for Naive Bayes.">Data Preparation for Naive Bayes</a></p>
                  </li>
               </ul>
            </div>
            <div>
               <div class="relinfo">
                  <p><strong>Related Topics</strong></p>
                  <ul>
                     <li><a href="classification.html#GUID-3D51EC47-E686-4468-8F49-A27B5F8E8FE4" title="Learn how to predict a categorical target through Classification - the supervised mining function.">Classification</a></li>
                  </ul>
               </div>
            </div><a id="DMCON344"></a><a id="DMCON345"></a><a id="DMCON343"></a><div class="sect2"><a id="GUID-DA3E0198-C041-4A85-BD63-9C03C6ABC167" name="GUID-DA3E0198-C041-4A85-BD63-9C03C6ABC167"></a><h3 id="DMCON-GUID-DA3E0198-C041-4A85-BD63-9C03C6ABC167" class="sect3"><span class="enumeration_section">20.1 </span>About Naive Bayes
               </h3>
               <div>
                  <p>Learn about Naive Bayes algorithm.</p>
                  <p>The Naive Bayes algorithm is based on conditional probabilities. It uses Bayes' theorem,<a id="d22542e87" class="indexterm-anchor"></a> a formula that calculates a probability by counting the frequency of values and combinations of values in the historical data. 
                  </p>
                  <p>Bayes' theorem finds the probability of an event occurring given the probability of another event that has already occurred. If <code class="codeph">B</code> represents the dependent event and <code class="codeph">A</code> represents the prior event, Bayes' theorem can be stated as follows. 
                  </p>
                  <div class="infoboxnote" id="GUID-DA3E0198-C041-4A85-BD63-9C03C6ABC167__GUID-451F598E-1F59-4BC6-841A-DB90633B0BE4">
                     <p class="notep1">Note:</p>
                     <p>Prob(B given A) = Prob(A and B)/Prob(A)</p>
                  </div>
                  <p>To calculate the probability of <code class="codeph">B</code> given <code class="codeph">A</code>, the algorithm counts the number of cases where <code class="codeph">A</code> and <code class="codeph">B</code> occur together and divides it by the number of cases where <code class="codeph">A</code> occurs alone.
                  </p>
                  <div class="example" id="GUID-DA3E0198-C041-4A85-BD63-9C03C6ABC167__BGBEEFIA">
                     <p class="titleinexample">Example 20-1 Use Bayes' Theorem to Predict an Increase in Spending</p>
                     <p>Suppose you want to determine the likelihood that a customer under 21 increases spending. In this case, the prior condition (<code class="codeph">A</code>) is "under 21," and the dependent condition (<code class="codeph">B</code>) is "increase spending." 
                     </p>
                     <p>If there are 100 customers in the training data and 25 of them are customers under 21 who have increased spending, then:</p>
                     <p>Prob(A and B) = 25% </p>
                     <p>If 75 of the 100 customers are under 21, then:</p>
                     <p>Prob(A) = 75%</p>
                     <p>Bayes' theorem predicts that 33% of customers under 21 are likely to increase spending (25/75). </p>
                  </div>
                  <!-- class="example" -->
                  <p>The cases where both conditions occur together are referred to as <strong class="term">pairwise</strong>. In <a href="naive-bayes.html#GUID-DA3E0198-C041-4A85-BD63-9C03C6ABC167__BGBEEFIA">Example 20-1</a>, 25% of all cases are pairwise.
                  </p>
                  <p>The cases where only the prior event occurs are referred to as <strong class="term">singleton</strong>. In <a href="naive-bayes.html#GUID-DA3E0198-C041-4A85-BD63-9C03C6ABC167__BGBEEFIA">Example 20-1</a>, 75% of all cases are singleton.
                  </p>
                  <p>A visual representation of the conditional relationships used in Bayes' theorem is shown in the following figure.</p>
                  <div class="figure" id="GUID-DA3E0198-C041-4A85-BD63-9C03C6ABC167__BGBCJGDA">
                     <p class="titleinfigure">Figure 20-1 Conditional Probabilities in Bayes' Theorem</p><img src="img/bayes.gif" alt="Description of Figure 20-1 follows" title="Description of Figure 20-1 follows" longdesc="img_text/bayes.html"><br><a href="img_text/bayes.html">Description of "Figure 20-1 Conditional Probabilities in Bayes' Theorem"</a></div>
                  <!-- class="figure" -->
                  <p>For purposes of illustration, <a href="naive-bayes.html#GUID-DA3E0198-C041-4A85-BD63-9C03C6ABC167__BGBEEFIA">Example 20-1</a> and <a href="naive-bayes.html#GUID-DA3E0198-C041-4A85-BD63-9C03C6ABC167__BGBCJGDA">Figure 20-1</a> show a dependent event based on a single independent event. In reality, the Naive Bayes algorithm must usually take many independent events into account. In <a href="naive-bayes.html#GUID-DA3E0198-C041-4A85-BD63-9C03C6ABC167__BGBEEFIA">Example 20-1</a>, factors such as income, education, gender, and store location might be considered in addition to age. 
                  </p>
                  <p>Naive Bayes makes the assumption that each predictor is conditionally independent of the others. For a given target value, the distribution of each predictor is independent of the other predictors. In practice, this assumption of independence, even when violated, does not degrade the model's predictive accuracy significantly, and makes the difference between a fast, computationally feasible algorithm and an intractable one. </p>
                  <p>Sometimes the distribution of a given predictor is clearly not representative of the larger population. For example, there might be only a few customers under 21 in the training data, but in fact there are many customers in this age group in the wider customer base. To compensate for this, you can specify <strong class="term">prior probabilities</strong> when training the model. 
                  </p>
               </div>
               <div>
                  <div class="relinfo">
                     <p><strong>Related Topics</strong></p>
                     <ul>
                        <li><a href="classification.html#GUID-590DD2C5-1BA5-40A3-9E3E-92AA2AE1D0EC" title="Learn about Priors and Class Weights in a Classification model to produce a useful result.">Priors and Class Weights</a></li>
                     </ul>
                  </div>
               </div><a id="DMCON346"></a><div class="props_rev_3"><a id="GUID-831ED761-1F51-420C-B9A8-41D1C1FABF0E" name="GUID-831ED761-1F51-420C-B9A8-41D1C1FABF0E"></a><h4 id="DMCON-GUID-831ED761-1F51-420C-B9A8-41D1C1FABF0E" class="sect4"><span class="enumeration_section">20.1.1 </span>Advantages of Naive Bayes
                  </h4>
                  <div>
                     <p>Learn about the advantages of Naive Bayes.</p>
                     <p>The Naive Bayes algorithm affords fast, highly scalable model building and scoring. It scales linearly with the number of predictors and rows.</p>
                     <p>The build process for Naive Bayes supports <a id="d22542e208" class="indexterm-anchor"></a>parallel execution. (Scoring supports parallel execution irrespective of the algorithm.)
                     </p>
                     <p>Naive Bayes can be used for both binary and multiclass classification problems.</p>
                  </div>
                  <div>
                     <div class="relinfo">
                        <p><strong>Related Topics</strong></p>
                        <ul>
                           <li><a href="../vldbg/using-parallel.html#VLDBG010" target="_blank"><span><cite>Oracle Database VLDB and Partitioning Guide</cite></span></a></li>
                        </ul>
                     </div>
                  </div>
               </div>
            </div><a id="DMCON347"></a><div class="props_rev_3"><a id="GUID-11AC5A10-CF1D-4CD9-8CB2-33801F75FD21" name="GUID-11AC5A10-CF1D-4CD9-8CB2-33801F75FD21"></a><h3 id="DMCON-GUID-11AC5A10-CF1D-4CD9-8CB2-33801F75FD21" class="sect3"><span class="enumeration_section">20.2 </span>Tuning a Naive Bayes Model
               </h3>
               <div>
                  <p>Introduces about probability calculation of pairwise occurrences and percentage of singleton occurrences.</p>
                  <p>Naive Bayes calculates a probability by dividing the percentage of pairwise occurrences by the percentage of singleton occurrences. If these percentages are very small for a given predictor, they probably do not contribute to the effectiveness of the model. Occurrences below a certain threshold can usually be ignored. </p>
                  <p>The following build settings are available for adjusting the probability thresholds. You can specify:</p>
                  <ul style="list-style-type: disc;">
                     <li>
                        <p>The minimum percentage of pairwise occurrences required for including a predictor in the model.</p>
                     </li>
                     <li>
                        <p>The minimum percentage of singleton occurrences required for including a predictor in the model .</p>
                     </li>
                  </ul>
                  <p>The default thresholds work well for most models, so you need not adjust these settings.</p>
               </div>
               <div>
                  <div class="relinfo">
                     <p><strong>Related Topics</strong></p>
                     <ul>
                        <li><a href="../arpls/DBMS_DATA_MINING.html#ARPLS-GUID-A04C5F4E-1303-44DC-A7DA-185C969330C8" target="_blank"><span><cite>Oracle Database PL/SQL Packages and Types Reference</cite></span></a></li>
                     </ul>
                  </div>
               </div>
            </div><a id="DMCON348"></a><div class="props_rev_3"><a id="GUID-FD929CF9-6513-42DE-B04D-FD57310C3A04" name="GUID-FD929CF9-6513-42DE-B04D-FD57310C3A04"></a><h3 id="DMCON-GUID-FD929CF9-6513-42DE-B04D-FD57310C3A04" class="sect3"><span class="enumeration_section">20.3 </span>Data Preparation for Naive Bayes
               </h3>
               <div>
                  <p>Learn about preparing the data for Naive Bayes.</p>
                  <p><a id="d22542e282" class="indexterm-anchor"></a>Automatic Data Preparation performs supervised <a id="d22542e287" class="indexterm-anchor"></a>binning for Naive Bayes. Supervised binning uses decision trees to create the optimal bin boundaries. Both categorical and numeric attributes are binned.
                  </p>
                  <p>Naive Bayes handles missing values naturally as missing at random. The algorithm replaces sparse numerical data with zeros and sparse categorical data with zero vectors. Missing values in nested columns are interpreted as sparse. Missing values in columns with simple data types are interpreted as missing at random.</p>
                  <p>If you choose to manage your own data preparation, keep in mind that Naive Bayes usually requires binning. Naive Bayes relies on counting techniques to calculate probabilities. Columns must be binned to reduce the cardinality as appropriate. Numerical data can be binned into ranges of values (for example, low, medium, and high), and categorical data can be binned into meta-classes (for example, regions instead of cities). Equi-width binning is not recommended, since outliers cause most of the data to concentrate in a few bins, sometimes a single bin. As a result, the discriminating power of the algorithms is significantly reduced</p>
               </div>
               <div>
                  <div class="relinfo">
                     <p><strong>Related Topics</strong></p>
                     <ul>
                        <li><a href="../dmprg/preparing-data.html#DMPRG-GUID-E1AB599C-1921-4BD7-B06B-FC466180A460" target="_blank">Preparing the Data</a></li>
                        <li><a href="../dmprg/transforming-data.html#DMPRG-GUID-C3FDDEC7-8CC9-4AC1-A6C3-75D91E26B703" target="_blank">Transforming the Data</a></li>
                     </ul>
                  </div>
               </div>
            </div>
         </div>
      </article>
   </body>
</html>